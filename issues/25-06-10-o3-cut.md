---
id: MjAyNS0w
title: 'Reasoning Price War 2: Mistral Magistral + o3''s 80% price cut + o3-pro'
date: '2025-06-10T05:44:39.731046Z'
description: >-
  **OpenAI** announced an **80% price cut** for its **o3** model, making it
  competitively priced with **GPT-4.1** and rivaling **Anthropic's Claude 4
  Sonnet** and **Google's Gemini 2.5 Pro**. Alongside, **o3-pro** was released
  as a more powerful and reliable variant, though early benchmarks showed mixed
  performance relative to cost. **Mistral AI** launched its **Magistral**
  reasoning models, including an open-source **24B parameter** version optimized
  for efficient deployment on consumer GPUs. The price reduction and new model
  releases signal intensified competition in reasoning-focused large language
  models, with notable improvements in token efficiency and cost-effectiveness.
companies:
  - openai
  - anthropic
  - google-deepmind
  - mistral-ai
  - perplexity-ai
models:
  - o3
  - o3-pro
  - gpt-4.1
  - claude-4-sonnet
  - gemini-2.5-pro
  - magistral-small
  - magistral-medium
  - mistral-small-3.1
topics:
  - reasoning
  - token-efficiency
  - price-cut
  - benchmarking
  - open-source
  - model-releases
  - context-windows
  - gpu-optimization
people:
  - swyx
  - sama
  - scaling01
  - polynoamial
  - nrehiew_
  - kevinweil
  - gdb
  - flavioad
  - stevenheidel
  - aravsrinivas
---


**Reasoning too cheap to meter.**

> AI News for 6/9/2025-6/10/2025. We checked 9 subreddits, 449 Twitters and 29 Discords (218 channels, and 9374 messages) for you. Estimated reading time saved (at 200wpm): 715 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!
> 

Every 3-4 months we get a big leg down in the cost of the frontier LLM ([March 2024](https://x.com/swyx/status/1772799201023557697), [Aug 2024](https://news.smol.ai/issues/24-08-08-ainews-too-cheap-to-meter-ai-prices-cut-50-70percent-in-last-30-days), [Jan 2025)](https://news.smol.ai/issues/25-01-22-ainews-bespoke-stratos-sky-t1-the-vicunaalpaca-moment-for-reasoning), and today we got confirmation of the [80% price cut for o3](https://x.com/sama/status/1932434606558462459), making it nominally the same cost as GPT 4.1, the non-reasoning model. (you can be forgiven for suspecting the price cut is due to distillation, but this is categorically [denied](https://x.com/TheRealAdamG/status/1932534244774957121)). Of course, the real cost come in the reasoning token efficiency, and fortunately o3 is [notably](https://x.com/ArtificialAnlys/status/1932489580592435301) better than Gemini and Deepseek in that department:

[](https://resend-attachments.s3.amazonaws.com/BzCdr8FpQfdFU9j)

Alongside of this o3 price cut, [o3 pro was released](https://www.latent.space/p/o3-pro), which if the o1/o1-pro relationship holds is more or less 10 o3's in a trenchcoat (and is priced that way).

This news is released conveniently on the same day as [Mistral's Magistral](https://news.ycombinator.com/item?id=44236997) reasoning model - a 24B open source version and a Medium closed version - that would've otherwise taken today's headline. We're REALLY glad though that Mistral is continuing to release good open source models but unfortunately the o3 price cut is more likely to be the relevant story for the majority of AI engineers today.

---

# AI Twitter Recap

**Large Language Models (LLMs) & AI Model Releases**

- **OpenAI's o3 and o3-pro model updates and pricing changes:** OpenAI announced significant price drops for its **o3 input tokens**, reducing them by **80%** to **$2.00 per million tokens**, making **o3 cheaper than GPT-4o** and competing with **Anthropic's Claude 4 Sonnet** and **Google's Gemini 2.5 Pro** in pricing, leading some to declare "price wars" [@scaling01](https://twitter.com/scaling01/status/1932437241592152161), [@scaling01](https://twitter.com/scaling01/status/1932488441100468438), [@polynoamial](https://twitter.com/polynoamial/status/1932463115867914741), [@nrehiew_/](https://twitter.com/nrehiew_/status/1932504409834979823). They also released **o3-pro**, a more intelligent and reliable version of o3 designed to "think longer," priced at **$20 Input and $80 Output per million tokens** [@scaling01](https://twitter.com/scaling01/status/1932530425609552334), [@kevinweil](https://twitter.com/kevinweil/status/1932503887040151556), [@polynoamial](https://twitter.com/polynoamial/status/1932532770594857350). Early testers reported **o3-pro** to be "much stronger" than **o3** [@gdb](https://twitter.com/gdb/status/1932561536268329463) and "extremely cheaper, faster, and way more precise than o1-pro" for coding and reasoning tasks [@flavioAd](https://twitter.com/OpenAIDevs/status/1932538094168801492). However, initial **ARC-AGI-1** and **ARC-AGI-2** benchmark results showed **o3-pro (high)** not outperforming **o3-high** despite being **8 to 9 times more expensive** [@scaling01](https://twitter.com/scaling01/status/1932539254703321399), [@scaling01](https://twitter.com/scaling01/status/1932539573432684779). OpenAI also experienced "elevated error rates and latency" across **ChatGPT** and the **API** during these releases, which were later fixed [@OpenAI](https://twitter.com/OpenAI/status/1932443370452037804). The API was restored to 100% functionality and rate limits for **o3** for **Plus users** were doubled [@stevenheidel](https://twitter.com/stevenheidel/status/1932532920482476217), [@kevinweil](https://twitter.com/kevinweil/status/1932565467736027597). **Perplexity AI** quickly integrated **o3** for its **Pro users** on both web and mobile apps [@perplexity_ai](https://twitter.com/perplexity_ai/status/1932522350727344151), [@AravSrinivas](https://twitter.com/AravSrinivas/status/1932523892771361117).
- **Mistral AI's Magistral reasoning models:** Mistral AI released **Magistral-Small** and **Magistral-Medium**, their first reasoning models, designed for "domain-specific, transparent, and multilingual reasoning" [@MistralAI](https://twitter.com/MistralAI/status/1932445269611688315). **Magistral Small** is an open-source **24B parameter model** based on **Mistral Small 3.1**, capable of running on a single **RTX 4090** with **128K context** (40k effective) [@scaling01](https://twitter.com/scaling01/status/1932445360380612712), [@reach_vb](https://twitter.com/reach_vb/status/1932449015657836730). It supports **MLX**, **llama.cpp**, **transformers**, and **vLLM** [@reach_vb](https://twitter.com/reach_vb/status/1932449015657836730). The underlying methodology, **GRPO**, involves modifications like removing KL Divergence and normalizing by total length [@danielhanchen](https://twitter.com/danielhanchen/status/1932451325398413518). Initial evaluations showed **Magistral Small** being outperformed by **Qwen3-32B** and **Qwen3-30B-A3B** [@scaling01](https://twitter.com/scaling01/status/1932445360380612712), though some noted its impressive speed on platforms like **Le Chat** [@qtnx_](https://twitter.com/qtnx_/status/1932442022574723407).
- **Other notable LLM/AI Model updates and releases:**
    - **MiniCPM4**, an ultra-efficient family of LLMs explicitly designed for end devices, was released on Hugging Face [@OpenBMB](https://twitter.com/OpenBMB/status/1932473479099789605).
    - **Google DeepMind** showcased **Veo 3 Fast** for **Gemini App** and **Flow**, touted as **2x faster** with improved visual quality and consistency [@demishassabis](https://twitter.com/demishassabis/status/1932140707499880728), [@demishassabis](https://twitter.com/demishassabis/status/1932491435439227108).
    - **Vui**, a new **open-source dialogue generation model** with **100M parameters** trained on **40k hours of audio**, was released as an alternative to **NotebookLM** [@_akhaliq](https://twitter.com/freddy_alfonso_/status/1932149790747525396), [@kylebrussell](https://twitter.com/Tu7uruu/status/1932416857165693101).
    - **Gemma 3n**, a desktop-optimized model (2B and 4B), is now available for **Mac/Windows/Linux** via the **LiteRT-LM library** [@demishassabis](https://twitter.com/osanseviero/status/1932607299178148184).
    - **Krea AI** introduced its first image model, **Krea 1**, promising "superior aesthetic control and image quality" [@_akhaliq](https://twitter.com/krea_ai/status/1932479466300670401).
    - **MeiGen-MultiTalk** released its code and checkpoints for an audio model [@TomLikesRobots](https://twitter.com/Norris29973102/status/1932451429454578019).
    - **DatologyAI** released two **state-of-the-art CLIP ViT-B/32** variants, optimized for classification and retrieval, achieved through **data curation alone** [@code_star](https://twitter.com/code_star/status/1932438873399033943), [@sarahcat21](https://twitter.com/sarahcat21/status/1932447722659082577).

**AI Infrastructure & Tools**

- **Agentic Frameworks and Development:**
    - **LangGraph** rolled out updates including **node/task caching** and built-in **provider tools** for more efficient and configurable workflows [@LangChainAI](https://twitter.com/LangChainAI/status/1932156503072674109). It has been successfully used by **Uber** to build **AI developer agents** that generate "thousands of daily code fixes" saving **21,000+ hours** for **5,000 developers** [@LangChainAI](https://twitter.com/LangChainAI/status/1932493346498543898). **Box's CTO Ben Kus** also detailed how they rebuilt their AI with agentic architecture using **LangGraph** to power their AI agent workforce [@LangChainAI](https://twitter.com/LangChainAI/status/1932165368375841255).
    - **DSPy** is highlighted for its foresight in treating prompts as "compiled outputs" rather than "ephemeral artifacts," with the expectation that many platforms will embody "The Spirit of DSPy" by **2026** [@lateinteraction](https://twitter.com/lateinteraction/status/1932551576100667416).
    - The **Agents & MCP Hackathon** saw over **400 submissions** leveraging tools like **Claude API**, **Gradio**, and **Modal** [@_akhaliq](https://twitter.com/_akhaliq/status/1932125696140435546), with **LangChain's GPT Researcher** now integrating **Model Context Protocol (MCP)** adapters for intelligent tool selection [@LangChainAI](https://twitter.com/LangChainAI/status/1932452695912116437).
- **Compute & Optimization:**
    - **SkyPilot** is now featured in **AWS SageMaker HyperPod** tutorials, combining HyperPod's availability/node recovery with SkyPilot's ease of AI execution [@skypilot_org](https://twitter.com/skypilot_org/status/1932142760754393179).
    - **vLLM** announced support for **Magistral** with **vLLM 0.9.1rc1** [@vllm_project](https://twitter.com/vllm_project/status/1932447031542608118) and showcased a new **AMD MI355X system** at **Berkeley Sky** for open-source support [@vllm_project](https://twitter.com/vllm_project/status/1932487956159475896).
    - **Modular** demonstrated "industry leading performance" on **AMD MI300/325** (up to **50% faster than vLLM 0.9**) and previewed **Blackwell** support for compute portability [@clattner_llvm](https://twitter.com/clattner_llvm/status/1932531652540183028), [@clattner_llvm](https://twitter.com/clattner_llvm/status/1932525600218251439). They are also partnering with **NVIDIA** for a **GPU Prize Pool** at their **Hack Weekend** [@clattner_llvm](https://twitter.com/clattner_llvm/status/1932210827098210330).
- **Data & Evaluation:**
    - The importance of **data curation** for model improvements is emphasized, with **DatologyAI** demonstrating state-of-the-art **CLIP model performance** through data curation alone [@sarahcat21](https://twitter.com/sarahcat21/status/1932447722659082577), [@code_star](https://twitter.com/code_star/status/1932438873399033943).
    - **AI Evals for Engineers & PMs** course by **@sh_reya** and **@HamelHusain** is proving "tremendously helpful" for data scientists and engineers in building and debugging AI applications, including multi-turn conversation traces [@HamelHusain](https://twitter.com/HamelHusain/status/1932126527694700896), [@HamelHusain](https://twitter.com/HamelHusain/status/1932143339312382229), [@HamelHusain](https://twitter.com/HamelHusain/status/1932204210994704625).
    - A new large-scale dataset, **MIRIAD**, with **5,821,948 medical question-answer pairs**, was released to improve **RAG** in medicine [@lateinteraction](https://twitter.com/Michael_D_Moor/status/1932159654068674880).
    - **NVIDIA** released **Nemotron-Personas**, an open-source dataset of **100k synthetically-generated personas**, and **PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams**, a **3TB synthetic driving dataset** on Hugging Face [@_akhaliq](https://twitter.com/HuggingPapers/status/1932198253417861222), [@_akhaliq](https://twitter.com/HuggingPapers/status/1932584984113787181).
- **Editor & IDE Integrations:**
    - **Claude Code** now integrates more deeply with **VS Code** and **JetBrains IDEs**, providing access to open files and **LSP diagnostics** [@_sholtodouglas](https://twitter.com/_catwu/status/1932204495951520129).
    - **Cursor AI** integrated the **o3 price drop**, making **o3** a viable "daily driver" for users [@cursor_ai](https://twitter.com/cursor_ai/status/1932484008816050492), and mentioned an interview with **Anthropic** about **Cursor** [@AnthropicAI](https://twitter.com/AnthropicAI/status/1932461901235007589).
    - **Zed editor** has improved its **Git UI** and **agentic editor sidebar**, offering faster performance compared to **2-5ms latency** of other editors [@vikhyatk](https://twitter.com/vikhyatk/status/1932390454731100289).

**AI Applications & Use Cases**

- **AI Agents in Enterprise & Workflows:**
    - **LlamaIndex** is enabling the building of "practical document agents in production" for use cases like **form filling** [@jerryjliu0](https://twitter.com/jerryjliu0/status/1932128211045122244). They demonstrated how to turn any **LlamaIndex agent** into an **MCP server** for "custom FidelityFundEngine" [@jerryjliu0](https://twitter.com/jerryjliu0/status/1932473411152064742) and how to build **Knowledge Agents** to automate workflows at **Databricks Data + AI Summit** [@jerryjliu0](https://twitter.com/jerryjliu0/status/1932504716455653648).
    - **Jerry Liu** also explained how **LlamaCloud** can be used to set up **parsing and extraction agents** for company filings and integrate with **LlamaIndex workflows** to generate reports, bridging the gap between AI and business value [@jerryjliu0](https://twitter.com/jerryjliu0/status/1932208932879212723).
    - **Scouts** launched as "always-on AI agents that monitor the web" for specific user interests [@krandiash](https://twitter.com/abhshkdz/status/1932471532871438685).
    - **Weaviate Agents** are highlighted for enabling "autonomous AI-driven workflows" [@bobvanluijt](https://twitter.com/bobvanluijt/status/1932387361914020007).
    - The concept of **AI agents** forcing a rethinking of "professional, commercial, or personal" interactions where time-wasting for information was once beneficial was put forth as a "VERY GOOD THING" [@francoisfleuret](https://twitter.com/francoisfleuret/status/1932514452026368342).
- **Creative AI & Content Generation:**
    - **Kling AI** introduced its **Pengfei Wan**, Head of **Kling Video Generation Models**, presenting "An Introduction to Kling and Our Research towards More Powerful Video Generation Models" at **CVPR 2025** [@Kling_ai](https://twitter.com/Kling_ai/status/1932464913018147291). Kling is also noted for its ability to automatically create "video-matched audio and ambient sounds" [@Kling_ai](https://twitter.com/onofumi_AI/status/1932366793747886313).
    - **Google's Veo 3** is achieving "consistent characters + mood" in video generation, which was previously a challenge in text-to-image prompting [@demishassabis](https://twitter.com/juliewdesign_/status/1932608957945950407).
    - **Higgsfield AI** announced "hyper-real vocals from **Suno Music**" for its upcoming "Rap Icon era" of AI superstars [@_akhaliq](https://twitter.com/higgsfield_ai/status/1932178722393903237).
    - **Runway ML** is developing "new products that bring a completely new experience," aiming to make creation "as natural and easy as possible" and "feel like your creative partner" [@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1932600586123227219).
- **Other Applications:**
    - **Sakana AI** partnered with **Hokkoku Bank** in Japan to develop "bank-specific AI-powered tools" and contribute to regional issues, following a comprehensive partnership with **Mitsubishi UFJ Bank** [@SakanaAILabs](https://twitter.com/SakanaAILabs/status/1932359607122628809), [@hardmaru](https://twitter.com/hardmaru/status/1932370496483697105).
    - **Google DeepMind's CEO Demis Hassabis** discussed AI's potential in **mathematics** at a workshop at **the IAS** [@GoogleDeepMind](https://twitter.com/GoogleDeepMind/status/1932135617640726996).
    - **Perplexity AI** updated its Discover articles to default to "Summary" mode for lighter reading, with a toggle for "Report" mode depth [@AravSrinivas](https://twitter.com/AravSrinivas/status/1932299234797052197).
    - [**You.com**](http://you.com/) partnered with **TIME** to offer free **Pro subscriptions** to their digital subscribers [@RichardSocher](https://twitter.com/RichardSocher/status/1932165821805326830).

**AI Industry & Market Dynamics**

- **Apple's WWDC Announcements and AI Strategy:**
    - Apple's **WWDC** announcements, particularly concerning **Apple Intelligence** and the new **iOS UI (Liquid Glass)**, generated significant discussion. Critics dubbed the new design a "Windows Vista moment" [@zacharynado](https://twitter.com/zacharynado/status/1932259455368102098) and "soulless UI update" [@scaling01](https://twitter.com/scaling01/status/1932222559610659063), comparing it to a "junior designer discovered the gradient tool" [@dzhng](https://twitter.com/dzhng/status/1932135452569714863). Some expressed disappointment, finding no "magic or delight" compared to past Apple products like the **iPod mini** [@raizamrtn](https://twitter.com/raizamrtn/status/1932172447857659985).
    - The "Liquid Glass" design is also criticized for potential usability issues, with **John Carmack** noting that "translucent UI is usually a bad idea" and that "Windows and Mac have both been down this road before" [@ID_AA_Carmack](https://twitter.com/ID_AA_Carmack/status/1932521605340483607).
    - Despite the critique, some suggested potential, arguing that monochrome UI could lead to "less addictive habits" [@zachtratar](https://twitter.com/zachtratar/status/1932175553903333476).
    - Apple introduced **MLX**, their machine learning framework, with new webpage and sessions for **Python** and **Swift** developers at **WWDC 2025** [@ClementDelangue](https://twitter.com/rudrankriyam/status/1932189026695688382), [@awnihannun](https://twitter.com/shshnkp/status/1932225029707833472).
    - **Safari 26** will gain **WebGPU** support [@jeremyphoward](https://twitter.com/wesbos/status/1932266119316078735), and macOS 26 will get "native support for Linux containers" [@jeremyphoward](https://twitter.com/MarkVillacampa/status/1932268008598343721).
- **AI Talent & Investment:**
    - **Meta** is reportedly offering **$2M+/yr** for **AI talent** but is still losing them to **OpenAI** and **Anthropic** [@slashML](https://twitter.com/deedydas/status/1932441521049006586). There are questions about **Meta's strategy** with **Scale AI** and the new "Superintelligence Lab" [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1932499937578955098).
    - Discussion continues on the **UK's AI and Biosciences industry**, with concerns that **US firms' "garden leave" policies** hinder local talent flow and benefit US acquirers, despite government announcements like **OpenBind** [@NandoDF](https://twitter.com/NandoDF/status/1932549812785754524), [@NandoDF](https://twitter.com/NandoDF/status/1932554598822117537).
    - **ZyphraAI** is expanding its team in **Palo Alto** with roles open across multimodal foundation models and RL [@QuentinAnthon15](https://twitter.com/QuentinAnthon15/status/1932128395594510598).
- **AI Ecosystem & Growth:**
    - **Stripe's macro figures** (e.g., payment volume) appear to be influenced by AI, suggesting growing adoption [@BorisMPower](https://twitter.com/patrickc/status/1932428519839343102).
    - The **AI Engineer World's Fair** highlighted that "the startup playbook is being rewritten in real-time" and emphasized publishing takeaways from events [@swyx](https://twitter.com/swyx/status/1932233921863020715), [@swyx](https://twitter.com/swyx/status/1932154530860519669).
    - The **Common Crawl Foundation**, **IBM**, the **AI Alliance**, and **BrightQuery** are hosting an **"UN Conference"** at **IBM's NYC HQ** on **June 20** to discuss AI, policy, and responsible data [@CommonCrawl](https://twitter.com/CommonCrawl/status/1932247624847151376).
    - [**DeepLearning.AI**](http://deeplearning.ai/) released a new course on **Data Storytelling** as part of its **Data Analytics Professional Certificate**, stressing its importance for business performance and revenue generation [@DeepLearningAI](https://twitter.com/DeepLearningAI/status/1932507879266820343).

**AI Research & Philosophy**

- **AGI and AI Capabilities:**
    - **Finbarr Timbers** proposed that **RL + GPT-style LLMs** could "lead to AGI" [@finbarrtimbers](https://twitter.com/finbarrtimbers/status/1932134065584714232).
    - **Ilya Sutskever's U of T honorary degree speech** was described as "the wisest words you could hear," with some interpreting his insights as indicating an impending, rapid acceleration of **code-LLMs** that could lead to mass job displacement, even for AI builders [@NandoDF](https://twitter.com/NandoDF/status/1932347615829508407), [@sbmaruf](https://twitter.com/sbmaruf/status/1932327556684120513).
    - **Sam Altman** stated that "Intelligence too cheap to meter is well within grasp" and that "We do not know how far beyond human-level intelligence we can go, but we are about to find out" [@sama](https://twitter.com/sama/status/1932550566036804087), [@scaling01](https://twitter.com/scaling01/status/1932551669134377357).
    - **FranÃ§ois Chollet** highlighted **metacognitive sensitivity** as crucial for learning rate, enabling introspection and critique of mental models [@fchollet](https://twitter.com/fchollet/status/1932332984935625197).
- **Architectures & Optimizations:**
    - The paper "Cartridges" explores scaling cache-time compute as an alternative to **ICL** for scenarios where many user messages reference the same large text corpus, aiming for **38.6x less memory** [@simran_s_arora](https://twitter.com/simran_s_arora/status/1932150476524667141), [@simran_s_arora](https://twitter.com/gm8xx8/status/1932304114857509305). This and similar work suggests **KV caches have significant room for compression** [@gallabytes](https://twitter.com/gallabytes/status/1932475390293156275).
    - Research into **Hierarchical Masked Auto-Regressive Image Generation (HMAR)** focuses on hardware-efficient reformulation for autoregressive image generation to leverage **tensor cores** [@realDanFu](https://twitter.com/realDanFu/status/1932163091174981978).
    - **Reinforcement Pre-Training (RPT)** reframes next-token prediction as a reasoning task using **RLVR** [@kylebrussell](https://twitter.com/qx_dong/status/1932454923322450186).
    - **Grafting** is introduced as a new method to distill pretrained diffusion transformers into new architectures, enabling swap attention for new primitives at **2% pretraining cost** [@realDanFu](https://twitter.com/realDanFu/status/1932494049061318821).
- **Societal Impact & Ethics:**
    - The "AI as Normal Technology" paper by **@random_walker** and **@sayashk** emphasizes the need for serious engagement with superintelligence and existential risk ideas, moving beyond social media mud-wrestling to productive debate and acknowledging different worldviews [@random_walker](https://twitter.com/random_walker/status/1932410929112572384).
    - Concerns were raised about **AI personas**, especially multimodal and real-time ones, potentially becoming addictive and "seem better than humans" [@sirbayes](https://twitter.com/sirbayes/status/1932155427703431647).
    - A position paper by **SEAL** and **Red Team at Scale AI** outlined lessons learned from red teaming LLMs, focusing on what matters for model safety within broader system safety and monitoring [@summeryue0](https://twitter.com/summeryue0/status/1932487755034210475).
    - The debate on **AI's energy consumption** highlights a sharp increase in electricity use by data centers (potentially doubling by **2030**), but also AI's potential to reduce emissions by optimizing energy systems five times more than expected data center production [@DeepLearningAI](https://twitter.com/DeepLearningAI/status/1932271502017064960).

**Humor, Memes & General Observations**

- **Reactions to Apple's WWDC and UI:** Many found humor in Apple's **WWDC** announcements, with comments like "lmfao Apple models sound so 2010ish" [@cto_junior](https://twitter.com/cto_junior/status/1932128352036605962) and comparing the new UI to "Windows Vista" [@skirano](https://twitter.com/skirano/status/1932145646963704199), and even quipping that Apple put "the last in the group chat to get the joke" in an ad [@swyx](https://twitter.com/swyx/status/1932137205268688983). The term "Liquid Glass" for the new design also became a point of mockery [@fabianstelzer](https://twitter.com/fabianstelzer/status/1932454556656382187).
- **Observations on AI Progress and Hype:** Discussions included humorous takes on the rapid pace of AI development, such as "AI hedonic treadmill," where new tools quickly make old ones feel "broken" [@rishdotblog](https://twitter.com/rishdotblog/status/1932383644020269464), and the observation that "research and product development move way faster than most people can keep up with" [@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1932203777462849916).
- **General Commentary & Satire:** Jokes about the nature of intelligence ("my brain is special and conscious because it is made of meat" [@vikhyatk](https://twitter.com/vikhyatk/status/1932316124596895923) and Terry Bisson's "They're Made Out of Meat" short story [@vikhyatk](https://twitter.com/vikhyatk/status/1932316563757223974)), observations on the tech industry ("members of technical staff' is fitting because there's a lot of dicks in ai" [@typedfemale](https://twitter.com/typedfemale/status/1932203221973659903)), and self-deprecating humor about coding and AI usage ("trying to explain to my wife which ChatGPT model to use ðŸ˜…" [@finbarrtimbers](https://twitter.com/finbarrtimbers/status/1932282319580311806)) were prevalent.
- **Non-Technical/Political:** A significant portion of the tweets from @SerranoAcademy focused on **international protests** and **political events** related to **Gaza**, **Greta Thunberg's arrests**, and **European Parliament members' kidnapping** [@SerranoAcademy](https://twitter.com/SerranoAcademy/status/1932288281107890188), [@SerranoAcademy](https://twitter.com/SerranoAcademy/status/1932288352373604584), [@SerranoAcademy](https://twitter.com/SerranoAcademy/status/1932287668076859883). These tweets are summarized here for completeness but are outside the core technical focus of the summary.

---

# AI Reddit Recap

## /r/LocalLlama Recap

### 1. Mistral Magistral Reasoning Model Releases and Discussion

- [**mistralai/Magistral-Small-2506**](https://huggingface.co/mistralai/Magistral-Small-2506) ([Score: 389, Comments: 118](https://www.reddit.com/r/LocalLLaMA/comments/1l7zvph/mistralaimagistralsmall2506/)): [**Magistral-Small-2506](https://huggingface.co/mistralai/Magistral-Small-2506) is a 24B parameter LLM derived from Mistral Small 3.1 (2503), with enhanced reasoning via SFT from Magistral Medium traces and RL, targeting efficient local deployment (fits in RTX 4090 or 32GB RAM MacBook when quantized). It offers strong multilingual (40+ languages) capabilities, a 128k context window (optimal <40k tokens), and is licensed Apache 2.0. Benchmarks show Magistral-Small achieves** `70.68%` **AIME24,** `62.76%` **AIME25,** `68.18%` **GPQA Diamond, and** `55.84%` **Livecodebench, slightly below Magistral-Medium. Quantized GGUF models and deployment guides are available (llama.cpp, lmstudio, ollama, unsloth), with best inference using temperature=0.7, top_p=0.95, and** `-jinja` **in llama.cpp. See Mistral's [blog](https://mistral.ai/news/magistral/) for further details.** Commentary highlights excitement for Magistral-Small's benchmark position relative to larger models (e.g., Qwen3 32B), and notes the model's permissive Apache 2.0 license. Technical users recommend specific inference parameters, raising potential improvements in performance with increased Ollama context length. Community fine-tuning and conversion (GGUF) support via Unsloth is praised for deployment flexibility.
    - danielhanchen provides direct usage instructions for running Magistral-Small-2506 GGUFs, specifying critical inference parameters: `temperature=0.7`, `top_p=0.95`, and emphasizes the importance of using the `-jinja` flag in llama.cpp for proper operation. They include command-line examples for both llama.cpp and Ollama, and recommend increasing Ollama's context length to at least 8K (`OLLAMA_CONTEXT_LENGTH=8192`) to optimize performance. Detailed deployment and usage guidance is available in the linked documentation: https://docs.unsloth.ai/basics/magistral
    - Only-Letterhead-3411 expresses interest in benchmarking Magistral-Small-2506 against Qwen3 32B, suggesting its perceived relevance as a competitor at the 30B+ model scale. This implies community interest in comparative performance and capability testing between Magistral and other leading large models.
    - AppearanceHeavy6724 raises concerns regarding Magistral-Small-2506's general-purpose abilities, speculating that its performance may be significantly subpar for non-coding tasks. This highlights open questions about the model's domain generalization and applicability outside programming contexts.
- [**New open-weight reasoning model from Mistral**](https://www.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/) ([Score: 303, Comments: 59](https://www.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/)): **Mistral has released Magistral, an open-weight reasoning model, with technical details available in their [news post](https://mistral.ai/news/magistral) and [official paper](https://mistral.ai/static/research/magistral.pdf). Notably, a GGUF quantized version of Magistral-Small-2506 is already available on [Hugging Face](https://huggingface.co/unsloth/Magistral-Small-2506-GGUF) and collaborates smoothly with downstream tools. The 24B parameter model size shows impressive performance benchmarks, especially in reasoning, with speculation on public release or comparison with competitor models such as Qwen. Performance on Cerebras hardware for applications like Le Chat shows significant inference speedups (reportedly up to 1000 tok/s for Flash Answers mode), highlighting hardware/model synergy.** Community discussion centers on the competitive performance of the 24B model, interest in comparative real-world benchmarks vs. Qwen, and future prospects for larger model releases. Users positively note the utility of fast inference modes for reasoning-centric applications, especially leveraging Cerebras hardware.
    - Discussion highlights collaborative GGUF quantization efforts with Mistral, ensuring optimized compatibility and fast deployment of the Magistral-Small-2506 model on different hardware via [UnsLoTh's Hugging Face repository](https://huggingface.co/unsloth/Magistral-Small-2506-GGUF).
    - Technical comparisons are requested between the new Mistral reasoning models and alternatives, specifically Qwen (with interest in performance on real-world tasks), and [MistralThinker-v1.1](https://huggingface.co/Undi95/MistralThinker-v1.1), which distills DeepSeek-style reasoning into Mistral-small architecture.
    - Users are observing impressive benchmark results for Mistral Medium, but note a lack of published comparative benchmarks for smaller variants (like Mistral Small) versus Qwen 3 32B, indicating a gap in publicly available performance data and possible benchmarking avoidance.
- [**Magistral â€” the first reasoning model by Mistral AI**](https://www.reddit.com/r/LocalLLaMA/comments/1l807c0/magistral_the_first_reasoning_model_by_mistral_ai/) ([Score: 114, Comments: 10](https://www.reddit.com/r/LocalLLaMA/comments/1l807c0/magistral_the_first_reasoning_model_by_mistral_ai/)): **Mistral AI has announced 'Magistral', their first reasoning-focused language model, as shown in the linked preview image. One user benchmarked Magistral's summarization capability and found it competitive with Qwen-32B, but noted it exhibited infinite thinking loops on two occasions; no details are provided on model size, architecture, or training data. There is no public confirmation regarding open model weights as of this announcement.** Top debate questions the availability of open weights, and a technical comment notes Magistral's summarization quality as comparable to Qwen32B but highlights specific failure modes (infinite loops), suggesting the need for further evaluation of deployment safety and robustness.
    - One user reports that while testing the model, it entered infinite thinking loops on two occasions, raising concerns about potential inference bugs or weaknesses in the control logic. However, aside from this, its summarization performance was observed to be on par with Qwen-32B, a known strong model in this domain.

### 2. Qwen3 0.6B Embedding Model Semantic Search Demos

- [**Semantic Search Demo Using Qwen3 0.6B Embedding (w/o reranker) in-browser Using transformers.js**](https://v.redd.it/y6ht8zacj06f1) ([Score: 116, Comments: 6](https://www.reddit.com/r/LocalLLaMA/comments/1l7odzw/semantic_search_demo_using_qwen3_06b_embedding_wo/)): **The post describes a semantic search demo leveraging the newly released Qwen3 0.6B embedding model for in-browser retrieval using [transformers.js](https://xenova.github.io/transformers.js/). The implementation uses ONNX quantized weights for the embedding model and ranks query results by basic cosine similarity, as the Qwen3 reranker model was not available in ONNX quantized form. The visualization maps up to three connections per node in a user-editable "memory bank" based on embedding similarity; the system currently scales to 20-100 entries given local inference. Source is available on [GitHub](https://github.com/callbacked/qwen3-semantic-search) with a live demo on [HF Spaces](https://huggingface.co/spaces/callbacked/qwen3-semantic-search).** A technical follow-up inquires about the ONNX quantized model file size, indicating interest in deployment specifics and hardware requirements.
    - One commenter inquires about the size of the quantized ONNX model file being used with Qwen3 0.6B for semantic search in-browser, suggesting interest in feasibility and storage requirements for client-side deployment. This is key for applications where bandwidth and local resource constraints matter for running transformer models directly in browsers.
- [**Google Diffusion told me its system prompt**](https://www.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/) ([Score: 146, Comments: 30](https://www.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/)): **A user claims to have received the full system prompt of "Gemini Diffusion," an experimental Google text diffusion language model advertised as non-autoregressive and explicitly tailored for generating code and web assets with fine-grained design constraints. The prompt details highly specific HTML/CSS/JS generation requirements (notably Tailwind CSS for web, custom CSS for games), icon handling, layout performance (e.g., CLS prevention), and strong emphasis on accurate instruction-following, modern aesthetics, and code self-containment. Prompt fidelity discussions are relevant, as the prompt includes constraints distinguishing it from autoregressive LLMs and reveals internal operational guidelines and security boundaries (e.g., no external file access, a Dec. 2023 knowledge cutoff, and strict handling of user requests).** Top comments raise skepticism about the prompt's authenticity, highlighting the possibility of hallucinations in LLM output and suggesting cross-verification via repositories like https://github.com/guy915/LLM-System-Prompts. A comment provides a screenshot as possible evidence, but the absence of direct confirmation from Google is noted.
    - A commenter questions the authenticity of outputs that claim to reveal a model's system prompt, raising the issue of hallucinationâ€”where a model might fabricate plausible but incorrect informationâ€”and asks how to verify whether such text truly reflects the underlying system prompt rather than generated content.
    - Another user emphasizes the uncertainty involved by stating that we cannot be sure if the provided text is actually the system prompt or just output generated in response to a user's prompt, highlighting the challenge in reliably extracting system or meta-prompts from language models like Google's Gemini Diffusion.

### 3. Cutting-Edge AI Architectures: Apple Parallel-Track MoE and Meta Superintelligence Initiatives

- [**Apple is using a "Parallel-Track" MoE architecture in their edge models. Background information.**](https://machinelearning.apple.com/research/apple-foundation-models-2025-updates) ([Score: 132, Comments: 19](https://www.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/)): **Apple's 2025 foundation model stack features two main innovations: (1) an efficient ~3B parameter on-device LLM using a Mixture-of-Experts (MoE) architecture with split-layer KV cache-sharing, enabling fast inference with reduced memory and latency on Apple silicon, and (2) a novel server-side Parallel-Track MoE (PT-MoE) architecture designed to scale out with minimal synchronization (relying on parallel processing and Limited/Distributed communication). Notably, the server models are also compressed using ASTC (Adaptive Scalable Texture Compression)â€”a GPU texture compression standardâ€”enabling direct hardware-level weight decoding with no extra compute overhead. The model pipeline integrates a custom ViT-based encoder with a 'Register-Window' mechanism for efficient vision-language tasks, trained on filtered web-scale and synthetic multimodal data. Full technical details are in the original Apple AI blog post.** Commentary highlights the clever reuse of Apple's ASTC GPU decompression hardware for LLM weight loading, and some debate about the practical capabilities of the edge modelsâ€”suggested to be basic (summarization, generic responses) versus more interactive tasks. There is strong technical interest in the split between local, private inference and hierarchical cloud fallback.
    - One user highlights Apple's use of block-based texture compression (specifically, Adaptive Scalable Texture Compressionâ€”ASTC) for compressing ML model weights, leveraging dedicated ASTC decompression hardware in Apple GPUs for efficient on-device inference without additional compute overhead. This represents an innovative repurposing of existing GPU hardware intended for graphics, now benefitting edge AI workloads.
    - A technical breakdown proposes that Apple's edge/local models are likely in the ~3B parameter range (comparable to models like Qwen 2.5 3B) and augmented with LoRAs for task specialization (few-shot or prompt tuning). Local models handle lightweight summarization and generic response tasks, while heavier requests are offloaded to more powerful server-side LLMs (possibly Qwen 3-235B-A22B scale), with a final fallback to ChatGPT for tasks outside Apple's scope.
- [**Mark Zuckerberg Personally Hiring to Create New â€œSuperintelligenceâ€ AI Team**](https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTUzOTk2NCwiZXhwIjoxNzUwMTQ0NzY0LCJhcnRpY2xlSWQiOiJTWE1KNFlEV1JHRzAwMCIsImJjb25uZWN0SWQiOiJCQjA1NkM3NzlFMTg0MjU0OUQ3OTdCQjg1MUZBODNBMCJ9.oQD8-YVuo3p13zoYHc4VDnMz-MTkSU1vpwO3bBypUBY) ([Score: 265, Comments: 122](https://www.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/)): **Mark Zuckerberg is personally overseeing the creation of a new 'superintelligence' AI team at Meta, targeting AGI development after internal dissatisfaction with Llama 4's performance and delays in the larger 'Behemoth' model. The strategy involves hiring ~50 elite AI researchers and integrating top talent from partners like Scale AI (recently valued at $28B), aiming for a fundamental overhaul of Meta's AI stack and product integration. This direct intervention reflects urgency at Meta to keep pace with global AI leaders and comes amid heightened antitrust scrutiny due to Meta's aggressive expansion in foundational AI infrastructure. [Bloomberg article](https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta)** Top comments highlight skepticism regarding the effectiveness of assembling elite teams, referencing prior failures due to intra-team politics and design divergence, and question Meta's readiness to pursue AGI given their lag behind Chinese LLMs. There is also speculation about whether existing teams (e.g., Llama) lacked capability, and doubt about Meta's ability to deliver breakthrough results under the current leadership structure.
    - One commenter describes a technical management pitfall where 'elite' teams assembled from top performers led to fragmented design processes and political issues; shifting requirements and incompatible component interfaces resulted in substantial project delays and eventual failure, highlighting risks in Meta's similar approach to 'superintelligence' team formation.
    - Skepticism is raised about Meta's technical positioning, with a suggestion that they need to "catch up to Chinese models" before seriously discussing superintelligence. The implication is that Meta's LLMs, like Llama, lag behind leading Chinese efforts in benchmarks or capabilities.
- [**Get Claude at Home - New UI generation model for Components and Tailwind with 32B, 14B, 8B, 4B**](https://v.redd.it/y74jt9x2y36f1) ([Score: 151, Comments: 45](https://www.reddit.com/r/LocalLLaMA/comments/1l808xc/get_claude_at_home_new_ui_generation_model_for/)): **Tesslate has released a suite of UI and front-end code generation models (UIGEN-T3) inspired by Claude, available in 32B, 14B, 8B, and 4B parameter versions [on Hugging Face](https://huggingface.co/collections/Tesslate/uigen-t3-hybrid-ui-model-6843bcb9bb72cc33e18d857c). The models target fine-grained component and full website code synthesis (Tailwind CSS, React syntax) and are finetuned from Qwen3 (14B, 4B [GGUF models available](https://huggingface.co/Tesslate/UIGEN-T3-14B-Preview-Q8_0-GGUF)). Notably, Tesslate uses its TframeX agent for training data cleaning and the UIGENEVAL Benchmark for evaluation. The devs caution that standard quantization harms reasoning integrityâ€”recommending BF16 or FP8â€”and are seeking collaboration for better INT8 support in vLLM. Licensing allows free research and personal use, commercial use by permission.** Expert commenters confirm significant output quality improvement versus standard Qwen3 finetunes, particularly for UI tasks. There is discussion around quantization trade-offs, especially the susceptibility to reasoning degradation at lower precision.
    - The Tesslate team highlights technical details of their new model for UI/front-end code generation, emphasizing a pre-and-post-training reasoning engine, training data cleaned using proprietary TframeX agents, and benchmarking with their UIGENEVAL framework. They note that standard quantization adversely impacts reasoning chains, recommending use of BF16 or FP8 for optimal results, and mention ongoing development of a more robust INT8 implementation for vLLM. The model is under a custom license permitting free research and non-commercial use, with commercial licensing available on request.
    - A commenter identifies that the model is a fine-tune of Qwen3 14B, providing GGUF links for both 14B and 4B versions. They report improved UI generation qualityâ€”producing more accurate, visually appealing results compared to the base Qwen3 14Bâ€”when evaluated on a Google-style thread rendering, offering anecdotal benchmarking via example output.
    - There is a technical query regarding image input: a user asks if the model can process UI design screenshots and generate corresponding code, noting that the previous 4B version did not support images. They express intent to test new model versions, indicating interest in multi-modal (image-to-code) capabilities, which is currently a limitation in the provided 4B variant.
- [**Vibe-coding without the 14-hour debug spirals**](https://www.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/) ([Score: 251, Comments: 102](https://www.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/)): **The post details strategies to avoid excessive debugging loops when using AI-assisted coding, emphasizing rules such as the "3-strike rule" (restart after three failed AI fix attempts), frequent context resets to address LLM context window limitations (restart every 8-10 messages), simplifying problem statements (ELI5 test), granular version control (commit after every working feature), and rewriting broken components instead of persisting on debugging when deep issues arise. The author benchmarks these practices as yielding a ~70% reduction in debugging time. Explicit workflow examples are provided for reproducibility. Relevant LLM limitations, like context window truncation and codebase drift, are exposed and mitigated with these techniques.** Top commenters stress that fundamental programming knowledge is essential to steer LLMs effectively, and note that granular, descriptive commits should be a universal practice irrespective of AI usage. Others advocate for code modularity and small-scope, single-responsibility functions to further ease both human and LLM-driven development, improving both debugging and maintainability.
    - Multiple commenters stress that AI-assisted coding is significantly more effective for users with existing coding knowledge, as LLMs require clear direction and oversight to deliver accurate results. Understanding code structure and function allows for proper prompt engineering and validation of model output, reducing debugging time.
    - Best practices in software engineering, such as committing to version control after every working feature and writing descriptive commit messages, remain essential even with AI assistance. These practices help with traceability and collaboration, especially when code changes are frequent or AI-generated code is iteratively refined.
    - Effective use of LLMs for coding aligns with established software design principles: breaking problems into single-responsibility functions, developing incrementally, and using modularity aid both human and AI contributors. This approach minimizes context switching errors and streamlines debugging, testing, and feature additions.

## Other AI Subreddit Recap

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo
> 

### 1. OpenAI o3 and o3-pro: Price Cuts, Model Release & Community Reactions

- [**03 80% less expensive !!**](https://i.redd.it/qz3wwzsow36f1.jpeg) ([Score: 146, Comments: 42](https://www.reddit.com/r/ChatGPTCoding/comments/1l7zkwy/03_80_less_expensive/)): **The attached image is a social media announcement by Sam Altman revealing that the cost of 'o3' (presumably an OpenAI model such as GPT-3.5 or GPT-4o) has dropped by 80%, with new prices at $2/1M tokens for input and $8/1M tokens for output, down from $10 and $40 respectively. The post compares old vs new pricing and signals competitive intent by mentioning confidence in the performance-based pricing of the 'o3-pro' tier. This price shift is likely to significantly lower operational costs for firms integrating OpenAI APIs, potentially influencing market dynamics and broader AI accessibility.** Some commenters speculate that the price drop could coincide with a reduction in model performance to incentivize 'pro' tier uptake, while others note possible instability ('is that why it's down?'), raising questions about reliability versus cost.
    - There's skepticism that the 80% price reduction might coincide with reduced performance, potentially positioning the current offering as less capable and making the pro or more expensive versions appear significantly better by comparison.
    - A key technical question is posed about how the 'o3' model compares to other leading models such as Gemini and Claude Power, with a request for direct user experiences regarding real-world quality and performance differences.
    - One commenter implies this pricing move is a direct response to competition from models like Gemini, suggesting the company had considerable pricing flexibility previously, which may hint at high margins or earlier price 'gouging.'
- [**o3 price reduced by 80%**](https://i.redd.it/j6b3lumiu36f1.jpeg) ([Score: 1482, Comments: 249](https://www.reddit.com/r/singularity/comments/1l7z9qe/o3_price_reduced_by_80/)): **The image confirms an 80% price reduction for the O3 product, correlating with discussion of increased competitive pressure in the LLM API market, as Gemini 2.5 Pro now offers similar output performance for $10 per million tokens. The post and comments highlight pricing dynamics as a driver for broader adoption and signal shifting value propositions among leading model providers.** Technically-minded comments speculate that the reduction is a competitive maneuver rather than a reaction to over-provisioned capacity, emphasizing the beneficial effects of such price wars for users.
    - Several commenters note that OAI's **O3 price cut by 80%** brings its token pricing **below both Gemini 2.5 Pro and GPT-4o**, raising questions about how it's now cheaper than both 4o and competitive Google offerings. This shift significantly alters cost-performance dynamics among premium LLMs.
    - A technical point is made that **Gemini 2.5 Pro currently offers comparable performance to O3 at $10 per million output tokens**, implying that the price drop is likely intended to increase O3's usage in direct response to this competition.
    - There's speculation that aggressive price cuts could precede a potential 'nerf' (model downgrade or throttling), a practice sometimes seen when models are made much cheaper or widely available, possibly impacting inference quality or availability.
- [**Let the price wars begin**](https://i.redd.it/ql1so3ex646f1.jpeg) ([Score: 235, Comments: 71](https://www.reddit.com/r/OpenAI/comments/1l80z5u/let_the_price_wars_begin/)): **Sam Altman publicly announced an 80% price reduction for 'o3,' likely referencing the GPT-3.5 or GPT-4o API (commonly referred to as 'o'), and expressed particular satisfaction with the performance-per-dollar ratio of the 'o3-pro' tier. The post implies a major shift in API pricing strategy by OpenAI, potentially making large-scale deployment significantly more accessible.** Commenters speculate that the substantial price cut could be offset by reduced model quality or stricter usage limits. Multiple users question whether the change will result in increased API rate limits, especially for Plus subscribers, signalling a technical concern around current usage caps.
    - Several users express confusion over differences in model versions, model names, and performance, highlighting that clear guidance or benchmarks are lacking on when to use models like o3 versus others. This is seen as a barrier for users with Plus subscriptions trying to optimize their workflow.
    - Technical discussion questions whether a significant price drop is meaningful if the new model (e.g., o3) requires substantially more tokens (e.g., 1.8x more) to achieve comparable output quality, raising the issue of effective cost per result rather than just per-token pricing.
    - Multiple comments request increases to rate limits for paid (Plus) users, indicating current limits (e.g., 100/week) are insufficient for advanced users and possibly outpaced by usage needs given model price and utility changes.
- [**it looks like we will see a big price reduction for o3**](https://www.reddit.com/gallery/1l7p02k) ([Score: 335, Comments: 40](https://www.reddit.com/r/singularity/comments/1l7p02k/it_looks_like_we_will_see_a_big_price_reduction/)): **The post discusses indications of a significant upcoming price reduction for OpenAI's GPT-4o (O3) API, referencing hints from the official OpenAI Developers Twitter account and wider industry rumors. Technical comment highlights include speculation that input token costs are a small fraction of overall LLM serving costs (with output tokens comprising '>90%'), so input price cuts may not drastically impact total API pricing; and mention of competitive pressure from open-weight models like DeepSeek and rival offerings from Google Gemini and Anthropic Claude.** Commenters debate the true impact of any price reduction, noting that unless output token prices fall, the effect on developers may be limited. There is also mention of growing preference toward open-weight models due to flexibility and cost advantages.
    - Discussion highlights the operational cost structure of reasoning models, with one user noting that output tokens account for ">90% of costs," making input cost reductions less impactful on overall pricing. This is particularly relevant for price comparisons and cloud inference optimization.
    - Usage statistics from OpenRouter are referenced, showing o3 has notably low adoptionâ€”outside the top 20â€”while models like 2.5 Pro and Sonnet rank in the top five despite being premium, which suggests significant market preference trends and may pressure pricing or product focus. See [OpenRouter rankings](https://openrouter.ai/rankings?view=month).
    - One commenter asserts that if recent price reductions for o3 are real, it remains a competitive state-of-the-art (SOTA) model. They also identify qualitative differences, such as o3's capability being more consistent or impressive compared to models like Gemini, which, despite being less 'lazy,' often misinterprets tasks.
- [**I bet o3 is now a quantized model**](https://i.redd.it/sanv7g7mn46f1.png) ([Score: 148, Comments: 55](https://www.reddit.com/r/OpenAI/comments/1l83em6/i_bet_o3_is_now_a_quantized_model/)): **The image presents a table benchmarking the performance of the OpenAI o3 model, with notable improvements in tokens per second (tps) after an 80% price reduction, suggesting backend changes such as quantization. The speculation is that such a drastic speed increase ('multiples of anything I've seen before') implies a switch to a quantized version, as quantization can significantly boost inference speed and reduce model size. However, a top comment notes OpenAI does not typically switch models on an API slug without a name change, so the improvements may also be due to other backend optimizations rather than actual quantization.** Commenters clarify terminology (quantization refers to reducing parameter precision, often from FP32 to int8/16 to increase speed and efficiency) and question the likelihood of a model swap without a new slug, with some suggesting possible 'lossless backend optimization' instead.
    - One user notes that in the OpenAI API, model upgrades or significant architectural changes always result in a new model slug (name), so if the model slug for o3 hasn't changed, it likely hasn't received major updates or quantization, and any backend improvements would be lossless and not affect inference results.
    - There's a technical prompt to benchmark the model, highlighting that any claims about quantization or changes (such as the transition from o3 to 4o, or comparisons to Blackwell) should be substantiated with actual benchmarking to determine performance or output differences.
- [**OpenAI announce o3-pro**](https://i.redd.it/xnekemuat46f1.jpeg) ([Score: 534, Comments: 129](https://www.reddit.com/r/OpenAI/comments/1l846x1/openai_announce_o3pro/)): **OpenAI has officially announced the release of "o3-pro" via a social media post, as shown in the image. The announcement lacks accompanying details about the capabilities, model sizes, or intended use cases of o3-pro, leading to confusion in the comments over the naming convention (e.g., references to 'o3-pro-medium-mini'). Recent OpenAI model announcements have followed less transparent naming schemes, which appears to complicate understanding for the technical community.** Several commenters express frustration and confusion regarding OpenAI's naming strategy for its models, noting it is difficult to decipher product differences or improvements (e.g., "Their naming scheme is garbage, I have no idea what this even means"). No substantive technical details or benchmarks are debated or provided.
    - Several users express confusion and frustration with OpenAI's model naming conventions, particularly with terms like "o3-pro-medium-mini" which lack intuitiveness or publicly documented meaning. This has led to technical ambiguity about model capabilities, position in the product lineup, and intended use cases, especially compared to more transparent schemes from other AI companies.
- [**OpenAI announce o3-pro release today**](https://i.redd.it/osiu8wugs46f1.png) ([Score: 453, Comments: 87](https://www.reddit.com/r/singularity/comments/1l842hz/openai_announce_o3pro_release_today/)): **OpenAI announced the release of o3-pro (presumably a new LLM tier) in a social post, as depicted by the announcement image and engagement stats (1,896 views, 44 reposts, 293 likes, timestamped June 10, 2025). Technical discussion in the comments centers on ongoing LLM hallucination issues, with users reporting persistent inaccuracies (especially in medical queries) despite custom prompts demanding citations and direct quotes, and fabricated sources/URLs, suggesting skepticism that o3-pro adequately addresses these critical reliability concerns.** Commenters question whether o3-pro represents a meaningful improvement over previous models with respect to hallucination control, reporting that even explicit instructions for citation do not prevent the generation of inaccurate or fabricated information. There is also criticism about the lack of a live launch event and reported service instability coinciding with the release.
    - A user points out that despite configuring ChatGPT's custom instructions to require source citation and direct quotes, **GPT-4o (and by extension, O3 and potentially O3-Pro)** continue to hallucinate, especially in technical/medical queries where fabricated statistics and non-existent references (like 404 URLs allegedly pointing to official data) are frequently encountered. This aligns with broader concerns over LLM 'trustworthiness' for domain-accurate queries and the limitations of current mitigation strategies (such as requiring citations) against model hallucinations.

### 2. ChatGPT Outage: User Experiences, Memes & Sub Reactions

- [**Typical Response to ChatGPT Being Down**](https://i.redd.it/dx4nwjezf36f1.png) ([Score: 347, Comments: 41](https://www.reddit.com/r/OpenAI/comments/1l7xhf7/typical_response_to_chatgpt_being_down/)): **The image is a meme, illustrating the common user experience and community reaction when ChatGPT is unavailable. It humorously depicts users turning to Reddit to confirm outages, as seen in the comic sequence.** No substantive technical discussion or debate is present in the comments; reactions are mainly lighthearted acknowledgments of using Reddit when ChatGPT is down.
    - One user recommends checking the official OpenAI status page (https://status.openai.com/) for real-time updates on ChatGPT's operational status, highlighting the importance of monitoring public service dashboards during outages.
- [**ChatGPT is deadâ˜ ï¸â˜ ï¸â˜ ï¸**](https://i.redd.it/9yhkdqwoz26f1.png) ([Score: 6378, Comments: 1167](https://www.reddit.com/r/ChatGPT/comments/1l7vris/chatgpt_is_dead/)): **The image shows an error interface from ChatGPT with a red banner and the message: "Hmm...something seems to have gone wrong." along with a Retry button, indicating the ChatGPT web app is experiencing downtime or service disruption. The post and comments confirm this is a widespread issue, suggesting a possible outage or backend failure rather than an isolated user or device problem.** Commenters corroborate the technical issue, with some initially suspecting client-side problems before realizing it's a platform-wide outage.
    - A reference to the OpenAI status page is made, indicating users are experiencing a service outage or degraded performance with ChatGPT and are advised to check [status.openai.com](http://status.openai.com/) for real-time updates. This links user front-end issues to infrastructure or availability challenges potentially being tracked by OpenAI's own monitoring systems.
- [**Millions forced to use brain as OpenAIâ€™s ChatGPT takes morning off**](https://www.reddit.com/r/ChatGPT/comments/1l87mvc/millions_forced_to_use_brain_as_openais_chatgpt/) ([Score: 2538, Comments: 253](https://www.reddit.com/r/ChatGPT/comments/1l87mvc/millions_forced_to_use_brain_as_openais_chatgpt/)): **A recent ChatGPT outage, humorously covered by The Register as 'Millions forced to use brain as OpenAIâ€™s ChatGPT takes morning off', highlights the platform's central role in daily digital workflows (coding, content generation, planning). The post probes fallback strategies, either using alternative LLMs (Claude, Gemini, Perplexity, Grok) or reverting to traditional research/creativity methods, amidst widespread user inconvenience and meme creation.** Technical discussion was limited; comment threads mostly contributed humor and memes rather than substantive alternatives or workflow adaptations.
    - A commenter reports that not only was the ChatGPT web interface down, but the API was also non-functional. This outage directly impacted downstream products relying on OpenAI's infrastructure, with one user recounting how their GPT-4.1-based application failed during a critical investor presentation. This highlights the operational risks and single-point-of-failure concern when building products on top of third-party LLM APIs like OpenAI.
- [**Itâ€™s downâ€¦**](https://i.redd.it/yfrw7aipc36f1.jpeg) ([Score: 711, Comments: 57](https://www.reddit.com/r/ChatGPT/comments/1l7x44q/its_down/)): **The post features a meme image (not technical) depicting confusion and chaos, referencing the ChatGPT service outage and user reactions. There are no technical benchmarks, model details, or implementation notes presented in the image, and the content is humorous rather than technical.** Comments reflect users' frustration and dependency on ChatGPT for tasks like email and discussion, but do not provide substantive technical debate.
    - A user notes that email services and even basic Reddit functions were failing to respond, suggesting a broader service outage potentially linked to the same infrastructure supporting ChatGPT.
    - No explicit technical analysis or deep debate is present in the comments; most are about user experience, but there is mention of widespread accessibility or connectivity issues, possibly indicating a significant multi-service outage.
- [**Problem**](https://i.redd.it/hzajrk0ua26f1.png) ([Score: 634, Comments: 275](https://www.reddit.com/r/ChatGPT/comments/1l7tmqz/problem/)): **The attached image shows an error message from an unidentified online system, reading 'Hmm...something seems to have gone wrong,' with an option to 'Retry.' This indicates a service outage or disruption affecting users' ability to interact with the platform. The comments and post link ([OpenAI Status Page](https://status.openai.com/)) suggest that this is a widespread, real-time outage, likely impacting OpenAI's services, with users advised to monitor the status page for updates.** Commenters confirm this is a widespread outage, not a user-specific problem. The main technical advice is to monitor OpenAI's status page for resolution.
    - A user reports that file upload functionality became unavailable roughly 3 hours prior to their comment, while text input/output remained functional until 10 minutes before postingâ€”indicating a staggered impact on various services within the OpenAI platform. This granular timeline may help correlate specific failures with back-end outages or deployment issues.
    - A reference is made to the official OpenAI status page (https://status.openai.com/), emphasizing that the issue is widespread and suggesting technical users track real-time uptime, incident reports, and investigation progress for updates on service restoration.
- [**ChatGPT HQ right now.**](https://i.redd.it/badqzzeeu36f1.jpeg) ([Score: 2229, Comments: 75](https://www.reddit.com/r/ChatGPT/comments/1l7z95k/chatgpt_hq_right_now/)): **The image is a humorous meme, showing a person inspecting a server cabinet in a technical setting, meant to represent 'ChatGPT HQ.' The context from the title and comments frames this as a lighthearted take on the troubleshooting and operational challenges faced by AI service providers like OpenAI. There is no actual technical discussion, benchmark, or model insight provided in the post or comments.** Commenters joke about troubleshooting strategies for ChatGPT, including consulting ChatGPT itself, switching to competitors, and the classic IT solution of power cycling, highlighting general expectations and frustrations with AI service uptime and reliability.
- [**I called off my work today - My brother (gpt) is down**](https://www.reddit.com/r/OpenAI/comments/1l7z1oo/i_called_off_my_work_today_my_brother_gpt_is_down/) ([Score: 335, Comments: 70](https://www.reddit.com/r/OpenAI/comments/1l7z1oo/i_called_off_my_work_today_my_brother_gpt_is_down/)): **The post describes an end user's significant reliance on GPT (OpenAI's large language model) for project work, highlighting a service outage affecting their workflow and deadline. The author expresses acute stress due to the inability to access GPT for an extended period (2+ hours), referencing its role as an essential productivity tool.** A top technical comment proposes using alternative LLMs such as Deepseek or Claude to mitigate productivity disruption during GPT outages.
    - One commenter suggests using alternatives like DeepSeek or Claude, referencing other competitive AI language models that users might consider when GPT services are down. This points toward the increasing diversity of available LLMs and user awareness of viable failovers for productivity or research.

### 3. Breakthroughs in Video Generation: Self-Forcing Model Discussions

- [**Real time video generation is finally real**](https://v.redd.it/1ylqghw2c46f1) ([Score: 410, Comments: 87](https://www.reddit.com/r/StableDiffusion/comments/1l81pwc/real_time_video_generation_is_finally_real/)): **The Self-Forcing paradigm introduces a novel approach to training autoregressive diffusion models for real-time video generation by simulating inference during training through unrolled transformers with key-value (KV) caching. Source code and model checkpoints are available ([project page](https://self-forcing.github.io/), [GitHub](https://github.com/guandeh17/Self-Forcing)), with empirical evidence showing practical generation speeds: on consumer hardware (4070Ti 12GB VRAM), it generates 81 frames (832x480, 8 steps) in 45 seconds, demonstrating both feasibility and emerging quality. Visual results and further discussion can be referenced [here](https://imgur.com/a/Z8Oww4o).** Top technical commentary acknowledges current quality limitations but highlights substantial advances and real-time feasibility, especially on mid-range GPUs. The method is regarded as a foundational step towards compelling real-time AI video interactions.
    - A user reports successful generation of 81 frames at 832x480 resolution in 45 seconds (about 18 FPS) on a consumer-grade NVIDIA RTX 4070TI (12GB VRAM) using 8 inference steps. Quality is noted as decent for an early implementation, suggesting real-time or near-real-time video generation is feasible on mid-tier hardware ([example output](https://imgur.com/a/Z8Oww4o)).
    - Another commenter compares VACE and CausVid backends, stating that enabling VACE does not yield significant improvements in render times over CausVid in this workflow, suggesting similar efficiency for both pipelines ([example output](https://i.redd.it/zlo903qdh56f1.gif)).
- [**Self Forcing: The new Holy Grail for video generation?**](https://www.reddit.com/r/StableDiffusion/comments/1l7sxh3/self_forcing_the_new_holy_grail_for_video/) ([Score: 299, Comments: 86](https://www.reddit.com/r/StableDiffusion/comments/1l7sxh3/self_forcing_the_new_holy_grail_for_video/)): **The Self Forcing model (see official [project page](https://self-forcing.github.io/)) is a 1.3B parameter text-to-video (T2V) model that achieves high-quality 480P video generation with a latency of ~0.8 seconds and real-time streaming frame rates of ~16 FPS on an H100 GPU (**`~10 FPS on a 4090`**). It is reported to be** `150â€“400Ã— faster` **than previous SoTA (Wan, SkyReels, MAGI) while providing comparable or better visual quality, and it operates at similar speed but with less artifact and more realistic motion compared to CausVid. [Models are available on Hugging Face](https://huggingface.co/gdhe17/Self-Forcing/tree/main/checkpoints) and are usable within ComfyUI or via a wrapper, typically using the LCM sampler, and require relatively low VRAM (**`~6GB` **for 49 frames at 512x512, 5 steps, simple LCM, 1CFG).** Commenters highlight ease of integration with ComfyUI, support for the Vace module, and strong performance with the dmd model, while noting hardware-specific FPS benchmarks and calling for a larger 14B model for enhanced capability.
    - Several commenters detail Self-Forcing T2V's technical deployment: the model is only 1.3B parameters, works with native Comfy or wrappers, and supports the Vace module for additional input types. The recommended checkpoint ('dmd') is highlighted for performance and only one model file is necessary. Users emphasize compatibility with **LCM Sampler**, which is required for proper function ([HuggingFace model link](https://huggingface.co/gdhe17/Self-Forcing/tree/main/checkpoints)).
    - Benchmarking on different hardware is shared: **H100 GPUs achieve 16 FPS, RTX 4090 achieves 10 FPS, RTX 3090 achieves 5 FPS**, and performance drops further on lower or midrange cards. A detailed example notes **49 frames at 16 FPS, 512x512 resolution, 5 steps, LCM simple, 6GB VRAM, 1 CFG, generated in 20 seconds**. This points to relatively low compute requirements for moderate frame rate video compared to alternative models like Causvid LoRA on similar scale networks.
    - There is demand and anticipation for larger versions (e.g., a 14B parameter model), with users suggesting that such a scale-up could further improve fidelity or performance. There is also technical curiosity about extending the method to real-time vid2vid (video-to-video) applications leveraging streaming camera input, implying potential for low-latency inference.

---

# AI Discord Recap

> A summary of Summaries of Summaries by Gemini 2.5 Pro Exp
> 

**Theme 1: The AI Model Arms Race: New Releases and Fierce Competition**

- **OpenAI's o3 Slashes Prices, Sparks "Nerf" Paranoia!**: OpenAI dramatically cut **o3** input token prices by **80%** from **$10 to $2 per million tokens**, a move confirmed by [Sam Altman on Twitter](https://x.com/sama/status/1932434606558462459), while output tokens remain **$40/M**. This sparked debate about potential model "nerfing" to push users towards the pricier **o3 Pro** (now available to all Pro users in ChatGPT and API), though some dismiss this as *survivorship bias*.
- **Mistral Unleashes Magistral, The "Reasoning" Renegade!**: Mistral AI launched **Magistral**, its first reasoning model, with the **Magistral Small (24B parameters)** version available open-source on [HuggingFace](https://huggingface.co/mistralai/Magistral-Small-2506) and detailed in [their Magistral research paper](https://mistral.ai/static/research/magistral.pdf), while the enterprise **Magistral Medium** is accessible via API. Despite claims of *transparent reasoning*, some users noted a *looping and token spamming problem* and questioned if its reasoning truly aligns with human thought.
- **Gemini Gets Grilled as o3 and Kingfall Flex Muscles!**: Users across multiple Discords heavily criticized **Google's Gemini**, with one user calling *it is shit* and another stating that *Gemini's Benchmarks are rigged*, preferring **OpenAI's o3** which is seen as smarter, more capable, and now significantly cheaper. Meanwhile, the new **Kingfall** model created buzz, with some testers in LMArena claiming it *edges o3 pro a bit* and is the *smartest model they've ever used*, though others found it a more modest improvement over **2.5 Pro** or **o3-0605**.

**Theme 2: Powering AI: Innovations in Tooling, Frameworks, and Platforms**

- **LlamaIndex Serves Up MCPs and Custom Memory!**: LlamaIndex showcased turning an agent into an **MCP server** for complex data extraction (like from Fidelity Fund PDFs, demoed [on X (formerly Twitter)](https://twitter.com/llama_index/status/1932472507577299040)) and introduced examples for building custom **multi-turn memory implementations** ideal for agentic workflows, detailed in [this X post](https://twitter.com/llama_index/status/1932173369858003304). These tools aim to enhance interoperability and control in agentic systems.
- **OpenRouter Rolls Out Model Pages and Welcomes Magistral!**: **OpenRouter** launched new **model pages** for a streamlined user experience ([as announced on X](https://x.com/OpenRouterAI/status/1932452397911310722)) and added **Mistral's Magistral** reasoning model to its platform, showcased in [this Magistral thinking video](https://cdn.discordapp.com/attachments/1092729520181739581/1382011511274344528/magistral.mp4?ex=68499a04&is=68484884&hm=cfb02d962c559f8e60e298a0063e8f4aaad15b7696285352af1df80599bb8aed&). These updates expand model accessibility and provide developers with more detailed information.
- **Modular and AMD Team Up to Supercharge Mojo on GPUs!**: **Modular** announced a collaboration with **AMD** to *unleash AI performance on AMD GPUs* with Mojo, detailed in their [Modular x AMD blog post](https://www.modular.com/blog/modular-x-amd-unleashing-ai-performance-on-amd-gpus). They also showcased **Python interoperability** with Mojo ([demo video at 14:03](https://www.youtube.com/watch?v=TrBXHPGRlnQ&t=843s)) and its official [Mojo Python integration documentation](https://docs.modular.com/mojo/manual/python/).

**Theme 3: Engineering AI: Deep Dives into Model Mechanics and Optimization**

- **Torch Compile Delivers Ludicrous Speed Boosts!**: Engineers using *torch compile* reported dramatic speedups, with one instance accelerating model forwarding from **45 seconds to 1.2 seconds**, highlighting that [ARM CPUs can excel at FP32 over FP16 as per PyTorch docs](https://pytorch.org/docs/stable/generated/torch.compile.html) even with CPU instructions. This underscores the significant performance gains achievable through optimized compilation methods for PyTorch models.
- **KV Cache Compression and Dynamic Token Limits Take Center Stage!**: Researchers are exploring new **KV compression** methods, detailed in [the KV-Zip paper on arXiv](https://arxiv.org/pdf/2505.23416), to manage growing context sizes efficiently. Concurrently, **Anthropic's Claude** was praised in Nous Research AI for its unique dynamic token limit implementation for **Chain of Thought (CoT)**, a challenge Nous aims to tackle in **Hermes 4** by teaching user-controlled token limits.
- **Triton and ROCm Users Wrestle with Precision and Profiling!**: Developers using **Triton** discussed **fp16 exp and sqrt functions** (similar to [CUDA's half2 functions](https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH____HALF2__FUNCTIONS.html#group__cuda__math____half2__functions_1gacb711f53daef957df8d3c4a673ea60ea)) and the impact of `num_warps` configuration, while also tackling precision issues in custom kernels (see [this](https://cdn.discordapp.com/attachments/1189607595451895918/1381932221161668659/matmul.py?ex=6849f8ec&is=6848a76c&hm=dd62e45721fd43e5b7c3637734242e4535962651f5d15dbfce1ce183c7a5fa99&) [matmul.py](http://matmul.py/) [example](https://cdn.discordapp.com/attachments/1189607595451895918/1381932221161668659/matmul.py?ex=6849f8ec&is=6848a76c&hm=dd62e45721fd43e5b7c3637734242e4535962651f5d15dbfce1ce183c7a5fa99&)). Meanwhile, **ROCm** users shared methods for collecting **SQTT traces** using `rocprofv2` for analysis in **Radeon GPU Analyzer (RGA)** and troubleshooted `Memory access fault` errors with CUDA graphs on newer PyTorch releases.

**Theme 4: Navigating the AI Frontier: User Experiences, Bugs, and Workarounds**

- **OpenAI's Platforms Suffer from Bug Swarm and Outages!**: Multiple users across OpenAI and Perplexity Discords reported that **ChatGPT** is buggy, with some experiencing **100% message failure rates** and reasoning models getting stuck in loops. This led some to consult [OpenAI's status page](https://status.openai.com/) and consider canceling subscriptions or switching to alternatives like **Claude Pro**.
- **Fine-Tuning Frustrations Flare for Gemma 3 and DeepSeek!**: Users fine-tuning **Gemma 3** models with Unsloth AI reported high losses on text data using `Gemma3ForConditionalGeneration`, suggesting version mismatches with `transformers` (possibly needing **transformers 4.51.3**). Separately, **DeepSeek R1 (0528)**, while promising in aider benchmarks, suffered from slow case times, with fireworks' version reportedly getting cut off mid-thinking due to token limits.
- **Platform-Specific Quirks Plague Users from Cursor to LM Studio!**: Cursor users lamented the continued lack of **local model support** and Windows users celebrated an upcoming fix for non-functional **background agents**. Over in LM Studio, Linux users reported a missing **developer mode toggle** (a feature *not yet in the Linux version*), and it was clarified that the platform *is only for inference* and doesn't support image generation models.

**Theme 5: AI in Action: Showcases, Use Cases, and Community Collaborations**

- **Agentic Coding Workflows Get Real with Aider and Windsurf!**: An aider user shared their **agentic embedded coding workflow** using **PlatformIO, Cline, and a FREE DeepSeek OpenRouter API** in a [blog post with video about agentic embedded development](https://www.circusscientist.com/2025/06/10/agentic-embedded-development/). Simultaneously, **Windsurf** launched **Planning Mode** ([Windsurf Wave 10 blog post](https://windsurf.com/blog/windsurf-wave-10-planning-mode)), enabling its AI agent to manage complex tasks via a live markdown plan.
- **Deep Research Tools Go Local with spy-search!**: The open-source tool **spy-search** gained attention in the LlamaIndex community, offering **Ollama** compatibility for extensive local research and generating reports exceeding **1000 words**. This tool provides an alternative to research platforms with limited output, emphasizing local processing power.
- **Mixedbread Hunts for Growth Guru to Hit $10M ARR!**: **Mixedbread**, a team of ex-Google Search engineers backed by prominent AI investors (from OpenAI, Vercel, Perplexity, Deepmind, and Scale AI), announced they are seeking a **founding growth person**. Their AI search infrastructure tech boasts **50M+ HuggingFace downloads** and claims to outperform OpenAI on MTEB benchmarks, signaling significant technical traction.



---

# Discord: High level Discord summaries




## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Stewie's Sexuality Splits Opinions**: Members debated the sexuality of Stewie from *Family Guy*, with some asserting he is gay, while others cited creators' confirmations that [Stewie is not gay](https://link.to.creators-confirmation).
   - Further comments revolved around whether a baby could be classified as gay, leading to more general statements about the fluid nature of characters and plotlines in *Family Guy*.
- **O3's Price Plummets, Performance Soars**: The price of **O3** has been dramatically reduced (*80% cheaper!*), prompting suggestions that Perplexity will now implement **O3** and it will replace **Deepsearch**, as **O3** is now cheaper than **2.5 Pro**.
   - However, members noted the models still have context windows limits, so there are still tradeoffs to consider.
- **Gemini Gets Grounded for Poor Performance**: Users heavily criticized **Gemini**, calling *it is shit* and *the worst* and stating that **Gemini's Benchmarks are rigged**.
   - The members stated that using **O3** over **Gemini** is prefered.
- **PPLX API Config Exposed in Screenshot**: A user requested and another user shared their **PPLX API configuration** including **base URL**, **model name**, and **response mode** in a screenshot.
   - A follow-up suggestion was made to change the **Completion mode** parameter to resolve an **Error 400**.
- **Social Media API Integration Explored**: A user asked if anyone had experience integrating **social media APIs** into an app to pull account analytics data.
   - Another user suggested using **Claude** to generate the necessary code for this task.



---



## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **User Preference Metric Debated**: Members debated whether **user preference** is the #1 metric for evaluating models, with some arguing that it matters because *it predicts who gets the users.*
   - Others argued that real-world **STEM tasks** and other factors matter more, citing **Meta's** release of a model that performed well in user preference but didn't gain many users due to factors like **accessibility, marketing, and pricing**.
- **OpenAI's o3 Dominates Competition**: Members discussed the capabilities and pricing of **OpenAI's o3** compared to **Google's Gemini**, with one stating that *OpenAI was already winning the pareto frontier with o4mini* and now they are *crushing the competition with o3 being almost 50% of gemini 2.5 pro*.
   - While some argued that **Gemini** has more overt marketing and superior image generation, others countered that **o3** is smarter, more capable, and cheaper, giving Google *zero argument or pull*.
- **Kingfall Hype: Smartest Model Yet?**: A member hyped **Kingfall** as the smartest model they've ever used, while others expressed more tempered excitement, saying it wasn't *that* much better, relatively, compared to **2.5 Pro** or **0605**.
   - One member stated that they *think kingfall edges o3 pro a bit* but another emphasized that Kingfall might be better, but not BETTER, with some describing it as having **ultra vibes** and others thinking the reverse, and calling it not a huge lift for **o3 Pro**.



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **o3-pro Hits OpenAI Pro Tiers**: **OpenAI** has rolled out **o3-pro** to all **Pro** users in **ChatGPT** and via the **API**, expanding access to enhanced features.
   - Pro users can now utilize **o3-pro** across both **ChatGPT** and the **API** platforms for improved performance and capabilities.
- **GPT-4 Teamed Up**: A student used **GPT-4** as a *co-author* to complete a theory paper, exploring its ability to *cross into deep theoretical reasoning*.
   - A solo researcher is conducting similar research into *ethical and truth alignment* in advanced **LLM** systems.
- **OpenAI Plagued by Bugs**: Multiple users reported that **ChatGPT** is buggy and failing to respond, with one user reporting **100%** message failure rates.
   - Some members cited [OpenAI's status page](https://status.openai.com/) and said they're canceling their subscriptions; others are going with **Claude Pro**.
- **Gemini 2.5 Impresses with Token Capacity**: A member noted that **Gemini 2.5** handles **100k tokens** per message well, favoring it for coding, with [Gemini 2.5 Pro](https://ai.google.dev/) offering a **1 million** context window.
   - Another user said that **Gemini 2.5** is better at writing and **Pro** mode is better at thinking.
- **Reasoning Models Go Bonkers**: Users reported that **reasoning models** are stuck in **loops**, repeating thoughts and failing to respond.
   - One user humorously described the contents of a custom GPT as a 'whole drawer full of little computer things' including *LICENSE.txt*, *privacy-policy*, and a *Java-WebSocket* inside a jar.



---



## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **Magistral Arrives, Starts Reasoning**: **Mistral's** first reasoning model, **Magistral**, is now available on **OpenRouter**, according to [this announcement](https://x.com/OpenRouterAI/status/1932452397911310722).
   - A video showcases the model thinking *very* hard (at 4x speed), and is available [here](https://cdn.discordapp.com/attachments/1092729520181739581/1382011511274344528/magistral.mp4?ex=68499a04&is=68484884&hm=cfb02d962c559f8e60e298a0063e8f4aaad15b7696285352af1df80599bb8aed&).
- **OpenRouter Opens Model Pages**: **OpenRouter** has launched model pages, as announced [here](https://x.com/OpenRouterAI/status/1932452397911310722).
   - This introduction of model pages aims to streamline user experience by providing detailed information and resources for each model.
- **Testers Jam on Jamflow**: A member is looking for testers for **Jamflow** and attached a [video](https://cdn.discordapp.com/attachments/1092850552192368710/1381808490712141926/Jamflow3.mp4?ex=684a2e70&is=6848dcf0&hm=ab2ad2599bfb945128280d5f9606c20c37e8423fc06d5d90539bce45522b3487&).
   - Other members joked about being too busy writing a book to immediately participate in testing.
- **OpenAI Slashes o3 Input Prices by 80%**: **OpenAI** has reduced **o3** input token prices by **80%**, dropping from **$10** to **$2** per million tokens, a price cut confirmed by [Sam Altman on Twitter](https://x.com/sama/status/1932434606558462459).
   - Despite the input price reduction, the output token price remains at **$40**, leading some to suggest this could be a strategy to push users toward **o3 Pro**.
- **Rumors Swirl Around OpenAI Model Nerfing**: Concerns are being raised about **OpenAI** potentially *nerfing* the **o3 model** after the price cut, with some users claiming to have observed a degradation in performance.
   - Some suggest this could be a tactic to push users to **o3 Pro**, while others dismiss such claims as *survivorship bias*.



---



## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Local Models Still Missing in Cursor**: A user inquired about integrating **local models** with Cursor, but was informed that **local models are not currently supported**.
   - This limitation may affect users who prefer or require local processing for privacy or performance reasons.
- **Community Shares Custom Cursor Rules**: Members shared resources for **Cursor rules**, including a link to the [Cursor Directory](https://cursor.directory/) and a [Pastebin link](https://pastebin.com/qZzgFjkC) with custom rules.
   - The consensus is that starting with a small project is best to determine which rules are most beneficial, as individual needs vary greatly.
- **Token Overflow Resolved with Context Reset**: Users experiencing **token overflow** were advised to use the **/Reset Context** command or prompt the AI to break the code into smaller parts.
   - An alternative suggestion involved using terminal commands to resolve the issue, providing a practical workaround.
- **Claude 4 Briefly Vanishes, Reappears**: A user reported that **Claude 4** disappeared from their Cursor setup, but they were able to manually re-add it in the settings.
   - This issue was confirmed by another user, suggesting it may be a temporary bug that will be addressed in a future update.
- **Windows Users Celebrate Background Agent Fix**: A user inquired about background agents not working, leading a developer to confirm a fix was coming soon and that the issue was specific to **Windows**.
   - The fix will address issues that have been preventing Windows users from fully utilizing background agents.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **Userbots Invade Eleuther**: Members observed more **userbots** on the server, prompting a request for self-identification from automated accounts.
   - Moderators are manually deleting bots, requesting members to react with <:delet:824412305906204692> or <:lurkmoar:800507348535214140> to aid in filtering.
- **GPTs Agents Hit Knowledge Ceiling**: **GPTs agents** cannot learn from additional information provided after initial training.
   - Uploaded files are saved as "knowledge" files for reference, but *do not continually modify the agent's base knowledge*, as detailed in [OpenAI documentation](https://link.to/openai-docs).
- **O3 Pro's Price Provokes Outrage**: The new **O3 Pro** model is priced at **$20 / 1M tokens** for input and **$80 / 1M tokens** for output.
   - One member quipped it "*better be able to solve the riemann hypothesis with that kind of pricing wtf*".
- **GaTO's Ghost: No Follow-Up Found**: Members questioned the absence of follow-up research to [Google/DM's GaTO paper from 2022](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdeepmind.google%2Fresearch%2Fpublications%2Fgeneral-purpose-agents-via-discriminator-guided-optimization%2F&psig=AOvVaw1m4q0jGwj9E0-mEym2H-eK&ust=1718216966479000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCNDF6cKex4GFAAAAAAdAAAAABAD), speculating that it either didn't scale well or was too successful to share.
   - The consensus was that without cross-task transfer, training a generalist agent becomes a compute-intensive exercise.



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Gemma 3 Text Woes**: Members reported high losses when fine-tuning **Gemma 3** models on text data using `Gemma3ForConditionalGeneration`, suggesting a version mismatch.
   - A member suggested trying **transformers 4.51.3** for the 4B+ variants, as they are working on the model with the latest transformers.
- **Unsloth's Multi-GPU Mirage**: Despite **Unsloth** not officially supporting **multi-GPU** configurations, over 50 people have confirmed it works.
   - The team is actively working on multi-GPU support with Nvidia, but vLLM might require some manual building.
- **Magistral Reasoning Questioned**: The release of new **Mistral** models, called **Magistral**, claimed *transparent reasoning* and *interpretability* [on Twitter](https://x.com/UnslothAI/status/1932441885618147402).
   - Skepticism arose regarding the models' actual reasoning capabilities.
- **DeepSeek Qwen3's Tooling Triumph**: A recent fix in **DeepSeek Qwen3** dramatically increased **tool calling accuracy** which users can redownload from [HuggingFace](https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF).
   - The update includes native tool calling using `--jinja` in llama.cpp, chat template bug fixes, UTF-8 encoding fixes, and fixes for Ollama memory usage.
- **Orpheus Sings a VRAM Requiem**: A member shared their **Orpheus (3B)-TTS GRPO** notebook, emphasizing that it requires at least **20GB of VRAM** and provided a [link to their notebook](https://github.com/Etherll/notebooks/blob/Orpheus-TTS-GRPO/nb/Orpheus_%283B%29-TTS_GRPO.ipynb).
   - Based on user reports, one can generate impressive results from an enhanced **reward function**.



---



## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **Torch Compile Boosts Model Speeds**: A member accelerated model forwarding from **45 seconds** to **1.2 seconds** using *torch compile*, highlighting that [ARM CPUs excel at FP32 over FP16](https://pytorch.org/docs/stable/generated/torch.compile.html), even with CPU instructions.
   - The speaker noted that the performance gains underscore the significance of optimized compilation methods.
- **Lightweight LLMs Power RAG and Finetuning**: Members recommended **Mistral Small 3.1** for its quality and image understanding, and **Qwen 32B** for text-focused tasks, citing them as [lightweight, local, and fine-tunable LLMs](https://huggingface.co/mistralai/Mistral-7B-v0.1) suited for consumer hardware.
   - The discussion underscored their suitability for RAG research assistants, emphasizing the need for behavior fine-tuning.
- **KVMM arrives: Timm Ports Over to Keras 3!**: A member introduced **KVMM (Keras Vision Models)**, a comprehensive library of vision models with pre-trained weights, built entirely in **Keras 3**, and compatible with segmentation and classification tasks.
   - The new library has over **25 backbone architectures**, offers multiple weight variants, and enables flexible building of segmentation models with custom backbones, according to its [GitHub repository](https://github.com/IMvision12/keras-vision-models).
- **Doubts Cast on Truth Engine's Quantum Claims**: Skepticism arose around claims of *quantum-resistant truth persistence*, with members pointing out that terms such as **Meta-Epistemic Equilibrium** lack a basis in computer science and that dependencies like `quantum_resistant` and `zkp_proofs` are nonexistent in Python.
   - While one member reported a positive response from running the code, others dismissed it as *sycophancy*.
- **Langgraph Eyes Smolagents Spot**: A course participant is trying to implement an agent using **Langgraph** and **Langchain** instead of **Smolagents** for data analysis, requiring the agent to write and execute code.
   - Another user suggested enhancing the agent with tools for reading Excel files, performing math, or executing code, stressing the importance of detailed instructions on tool usage.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Linux Lacks LM Studio Developer Mode**: A Linux user reported the **developer mode toggle** is missing in the LM Studio GUI, despite using the latest version (0.3.16), and was told *this feature is not yet in the Linux version*.
   - The user seeks alternative activation methods, as of now there is no known workaround.
- **LM Studio's Image Dream Dashed**: Users inquired about LM Studio's ability to **generate images like ChatGPT** with local models, noting their usage of **ComfyUI** for Stable Diffusion but members clarified that *LM Studio is only for inference* and **doesn't support image generation models**.
   - The user was advised to use ComfyUI separately for image generation.
- **ROCm Runs Rad on Windows**: A user successfully ran **ROCm/HIP PyTorch** preview on Windows, calling it *an abomination* that surprisingly works well and has a positive experience compared to previous attempts with **ZLUDA**.
   - The user noted that while some modules may not fully support this setup and optimizations are not remembered across relaunches.
- **Speculative Decoding Sparks GPU Debate**: Members discussed speculative decoding and the potential to offload the draft model to a different GPU or CPU, like an **RX 9070 XT** paired with a **GTX 1060**.
   - It was clarified that offloading to the CPU is a different method than changing runtimes, and while offloading to another GPU on the same runtime should be technically possible, it's complicated by each GPU typically having its own runtime.
- **Digits Degraded by Bandwidth Bottleneck**: The question arose of how **Nvidia's Project Digits** would compare to a **5090 + 3090** setup for AI tasks within 56GB VRAM, with the consensus being that **Digits** is slower due to bandwidth constraints.
   - LLM inference is typically bound by memory bandwidth, and **Digits** is expected to have less bandwidth than an **M3 Max**, which is already slower than dual **3090s** for VRAM-fitting tasks.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Gemini 2.5 Pro Doesn't Grok Lib Updates**: **Gemini 2.5 Pro** struggles with understanding library updates and following instructions, showing only **50%** effectiveness, versus **Claude Sonnet (80%+)** and **Opus (95%)**.
   - Explicit rules in **aider** and related tools are more effective with **Claude** models.
- **DeepSeek R1 Benchmarks Show Promise, Pending Speed Boost**: **DeepSeek R1 (0528)** has shown promise in aider benchmarks, but suffers from slow case times due to resource constraints, with one user suggesting a potential **7x speedup** with more resources.
   - Members noted that this model iteration has a much lower tendency to get stuck in COT loops.
- **Uninstalling Aider Requires Manual Cleanup**: Uninstalling **aider** on Linux involves using `pip uninstall aider-chat`, but leaves behind the `aider` binary, indexes, and cache files.
   - These lingering files must be manually deleted for a complete uninstall.
- **OpenAI O3 Price Cut Won't Drop KYC**: **OpenAI** has announced **O3** pricing at **$2 input, $8 output**, but using it via OpenRouter still requires bringing your own key and **KYC** verification.
   - The community is still debating the benefits, with some even suggesting re-benching it given that it might be a smaller model.
- **Agentic Embedded Coding Workflow Goes Live**: A member is building an **agentic embedded coding workflow** using **PlatformIO**, **Cline**, and a **FREE DeepSeek OpenRouter API**, and shared [a blog post with video](https://www.circusscientist.com/2025/06/10/agentic-embedded-development/).
   - He is also seeking collaborators with experience in **microcontrollers** and **IOT**.



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Mistral's Magistral Model Sparks Debate**: Mistral released **Magistral**, benchmarking against the old **R1-0125** instead of the new **R1-0528**, including the release of a [paper](https://mistral.ai/static/research/magistral.pdf) and a distilled version on [HuggingFace](https://huggingface.co/mistralai/Magistral-Small-2506).
   - The model exhibits a **looping and token spamming problem**, despite the addition of a length penalty to GRPO.
- **Anthropic Pioneers Dynamic Token Limits**: Members pointed out that **Anthropic's Claude** stands out for its unique dynamic token limit implementation for **Chain of Thought (CoT)**, a problem not yet solved by many others.
   - Nous is working on **Hermes 4** to feature user-controlled token limits by teaching the model word, character, and sentence limits during SFT and token limits during RL.
- **Control Tokens Explored for Model Reasoning**: The discussion explored the potential of injecting control tokens, such as **progress markers (00%, 25%, 50%, 75%)**, during reasoning to help models dynamically adjust and compress outputs.
   - The goal is to improve the model's ability to **split reasoning into search-consolidate-answer phases**.
- **ProRL Paper Under Scrutiny**: The discussion examined the **ProRL (Prolonged RL) paper**, with some members finding its conclusions unconvincing, especially regarding its applicability to larger models, while noting issues with entropy collapse and reduced sample diversity for shorter **CoT**.
   - The async overlapped training technique used by Mistral, similar to the PipelineRL approach, was also highlighted ([tweet](https://x.com/dmayhem93/status/1916189027881062888), [paper](https://arxiv.org/abs/2505.24864)).
- **KV Compression Method Surfaces**: A new method for **KV compression** was shared, detailed in [this paper](https://arxiv.org/pdf/2505.23416) and [this tweet](https://x.com/qx_dong/status/1932268949238067482?s=46).
   - It was also mentioned that **GRPO** (Generalized Reweighted Policy Optimization) can be used to improve **TTS LLMs** (Text-to-Speech Large Language Models), detailed in [this paper](https://arxiv.org/abs/2502.04128).



---



## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **Mistral's Magistral Model: Open Source or Open Washing?**: **Mistral AI** launched [Magistral](https://mistral.ai/news/magistral), its first reasoning model, with community members calling out that it *represents a significant contribution by Mistral AI to the open source community*, though only **Magistral Small** is open-weight under the Apache 2.0 license.
   - A user expressed disappointment that **Mistral** is not open sourcing its larger models, stating *they became Google level of open weighting*, while another quoted the paper stating they open-sourced **Magistral Small** which includes cold-start data from **Magistral Medium**.
- **Diffusion Models Generate Order from Noise**: Members discussed the counterintuitive nature of **diffusion models** to generate structure from noise, describing it as a *directed hallucination model*.
   - One member linked this to broader themes of order from chaos, referencing a [YouTube video](https://youtu.be/10cVVHKCRWw?si=xNf-PpR9N-GTvP-X) and [paper](https://arxiv.org/abs/1412.1875) on **nonequilibrium thermodynamics** and the spontaneous emergence of life.
- **Hardware Failure Prediction: DL Underperforms**: Discussion revolved around approaches to **hardware failure prediction**, with the insight that traditional methods like **Gaussian Processes** or **boosted trees** often surpass deep learning for time series analysis.
   - A member emphasized the need for guaranteed failure detection rather than probabilistic correctness due to insurance requirements in industrial settings, highlighting the narrow scope and high-stakes nature of this field.
- **Reservoir Computing: Linear Regression in Disguise?**: **Reservoir Computing** was described as *mumbo jumbo* that obscures its core mechanism: linear regression on a fixed Ordinary Differential Equation (ODE).
   - It was argued that modern architectures like **State Space Models (SSMs)** are more expressive, powerful, and efficient due to their ability to parallelize and incorporate nonlinear dynamics and linked to a [paper](https://arxiv.org/abs/2506.02475) about current SOTA.
- **OpenAI Teases Unexpected Announcement**: Users pointed out [Sam Altman teased on Twitter](https://x.com/sama/status/1932573231199707168) that **OpenAI** has an *unexpected thing* coming.
   - Users speculate that *its a diffusion model*.



---



## [Notebook LM](https://discord.com/channels/1124402182171672732) Discord

- **Google Chat Convos Coming?**: A user inquired about the possibility of connecting **Gmail and Google Chat conversations** with NotebookLM, and whether there are plans for this feature in the near future.
   - The query was directed towards any Google employees present in the server.
- **Drive File Downloads Derailing?**: A user encountered an error when trying to access a Drive file, indicating that the **file owner has disabled copy/download permissions** for the Drive file.
   - The screenshot of the error is located [here](https://cdn.discordapp.com/attachments/1124403655819415592/1382004322572959844/PXL_20250610_143128479.MP.jpg).
- **NotebookLM's Intro Stuns Game Designer**: A user was impressed by the quality of the intro generated by NotebookLM for their tabletop RPG, **The Gemini System**, using the podcast feature.
   - The user found NotebookLM's ability to analyze and provide **audio deep dives** incredibly helpful for translating mechanics and enhancing their design and writing process.
- **Iceland Workshop Attendees Encounter Access Issues**: During a **NotebookLM** workshop for 50 teachers in Iceland, 3 teachers using private Gmail accounts encountered a *"You do not have access to this service"* error.
   - It was suggested that geographic restrictions or incomplete age verification could be the cause, with a user in the UK reporting similar issues with **Brave** browser, resolved by switching to **Firefox**.
- **Sharing Notebooks Spurs Sharing Headaches**: A user reported that when sharing notebooks, added emails and *"Anyone with link"* settings revert to restricted after sending.
   - This issue appears to be persistent for the user.



---



## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **Deepwiki Digests GitHub Details**: A member recommended [deepwiki](https://deepwiki.com) as a tool to summarize **GitHub repos**, enabling chatting and structure viewing directly from a **GitHub link**.
   - Another member was developing a parallel **GPU grouping/clustering algorithm** in **GLSL** and **Vulkano** using **Rust**, seeking collaborators to work with **Vulkano** for macOS.
- **Triton's Configuration Confabulations**: Discussions covered the availability of **fp16 exp and sqrt functions** in Triton, similar to those in [CUDA](https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH____HALF2__FUNCTIONS.html#group__cuda__math____half2__functions_1gacb711f53daef957df8d3c4a673ea60ea), and the role of `num_warps` in `Triton.Config`.
   - A user asked about the role of `num_warps` in `Triton.Config` seeking insight into its **impact on performance and resource utilization**, and whether Triton adheres to the same shared memory allocation limits as CUDA.
- **Torch and Triton Tackle Precision Pitfalls**: A user faced precision issues in a matmul kernel written for LeetGPU challenges, and shared their [matmul.py file](https://cdn.discordapp.com/attachments/1189607595451895918/1381932221161668659/matmul.py?ex=6849f8ec&is=6848a76c&hm=dd62e45721fd43e5b7c3637734242e4535962651f5d15dbfce1ce183c7a5fa99&) to find out the **cause of the failure** and whether their 2D grid implementation already incorporates swizzling.
   - A separate issue reported that while `libdevice.round` is defined in Triton ROCm, it throws an error when used in a kernel, as reported on [GitHub](https://github.com/triton-lang/triton/issues/7135).
- **Profiling Puzzles in ROCm**: A user encountered a `Memory access fault by GPU node-2` error when using **CUDA graphs** with the `rocm/pytorch:rocm6.4_ubuntu22.04_py3.10_pytorch_release_2.6.0` image and **torch 2.8.0.dev20250609+rocm6.4**, but they hadn't experienced it with previous versions.
   - Another user detailed the steps to collect **SQTT traces** for analysis in **Radeon GPU Analyzer (RGA)** using `rocprofv2` noting that the correct **DISPATCH_RANGE** can be determined by first running `rocprofv2 --kernel-trace`.
- **Modular team links with AMD for Mojo**: Modular announced a collaboration with AMD to *unleash AI performance on AMD GPUs*, according to their [blog post](https://www.modular.com/blog/modular-x-amd-unleashing-ai-performance-on-amd-gpus).
   - The team shared a demo showcasing **Python interoperability** with Mojo, viewable at the [843-second mark of this YouTube video](https://www.youtube.com/watch?v=TrBXHPGRlnQ&t=843s), as well as its [Python integration](https://docs.modular.com/mojo/manual/python/).



---



## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Fireworks AI Sparks RFT Beta**: Lin Qiao unveiled the beta launch of **Reinforcement Fine-Tuning (RFT)** on [Fireworks AI](https://xcancel.com/lqiao/status/1932103761872474450), enabling training of expert open models akin to **GPT-4o mini** and **Gemini flash**.
   - The service features a web IDE, an open-source reward-kit, SOTA model support, and is free for two weeks for models up to **10B** parameters.
- **OpenAI o3 Pricing Leaks?**: Gabriel Chua hinted at a potential cost of **$2 per 1M input tokens** for **OpenAI o3**, citing an [OpenAI Developers tweet](https://xcancel.com/gabrielchua_/status/1932251026943533156) offering **200** developers free API credits worth **1M** input tokens.
   - The discussion referenced [Sam Altman's tweet](https://x.com/sama/status/1932434606558462459) and insights from [scaling01](https://x.com/scaling01/status/1932530425609552334?s=46).
- **Mistral's Magistral Reasoning Model Arrives**: Mistral AI introduced **Magistral**, its new reasoning model for domain-specific, transparent, and multilingual reasoning, with two variants: open-source **Magistral Small (24B parameters)** on [Hugging Face](https://xcancel.com/mistralai/status/1932441507262259564?s=46) and enterprise **Magistral Medium** via chat.mistral.ai or API.
   - The model is also available on platforms like [OpenRouter](https://x.com/slashML/status/1932444552352203252), with users sharing instructions for local deployment.
- **Meta Scales Up Scale AI Stake, Eyes Alex Wang**: Meta Platforms is considering acquiring a **49% stake** in Scale AI for nearly **$15 billion**, potentially bringing Scale AI's CEO, Alex Wang, into a senior role at Meta ([Source](https://xcancel.com/amir/status/1932455464203329651?s=46&t=b7l37rB6wtbyAh6ah1NpZQ)).
   - This move could reshape Meta's AI strategy and executive leadership.
- **Windsurf Plans to Launch 'Plan Mode'**: Kevin Hou launched Windsurf's new **'Plan Mode'** feature, enabling the AI agent to perform complex tasks by creating and maintaining a planning document ([Source](https://xcancel.com/kevinhou22/status/1932516093333266538?s=46)).
   - Users can activate **'Plan Mode'** to allow Windsurf to manage notes, task lists, and goals, enhancing its ability to handle longer, more involved changes, available for free on [Windsurf.com](https://xcancel.com/kevinhou22/status/1932516093333266538?s=46).



---



## [Modular (Mojo ðŸ”¥)](https://discord.com/channels/1087530497313357884) Discord

- **Modular Debuts Compute Portability Talk**: Modular kicked off a livestream focused on the **future of compute portability**, accessible on the [Modular website](https://www.modular.com/) and [LinkedIn](https://www.linkedin.com/events/thefutureofcomputeportability7335404851276734464/theater/).
   - The event promised insights into the latest advancements and discussions in **compute portability**.
- **Mojo Parameterization boundaries questioned**: Community member presentations and glimpses into the **standard library code** have sparked questions about the boundaries of parameterization in Mojo, particularly regarding its use for **comptime purposes**.
   - One member expressed concern that the exploitation of parameterization for **comptime purposes** seems to create code they *just do not want to read in a lot of cases*.
- **Mojo Meta-Programming Favored Over Rust Macros**: A member argued that reading **meta-programming** in Mojo is *100000000000% better than reading macro code in rust*, while acknowledging that Mojo can't do everything Rust can yet.
   - Another member thinks it's the combination of **Zig-esque comptime** in **Go-esque square-bracket-generics syntax** that makes it difficult to read.
- **Generics syntax inspired by Python, claims community**: A member stated that Mojo's syntax for generics is the same as **Python's**, which inspired a discussion to whether **Python's generic syntax came from Go**.
   - Ultimately, the parties agreed that **Go 1.18** introduced generics on 3/15/22, and **PEP 695** introduced the *new* Python syntax on 6/22.
- **Mojo-MAX Platform relationship sought by user**: A member asked about the relationship between the **Mojo** and **MAX platform**, specifically the ability to use MAX kernels such as **matmul** in Mojo code and kernels.
   - A Modular employee suggested that the member post this question in the [Modular forum](https://forum.modular.com/) to enhance its discoverability.



---



## [MCP (Glama)](https://discord.com/channels/1312302100125843476) Discord

- **5ire Demands Complete MCP Tooling**: The **5ire platform** mandates the adoption of all tools from an **MCP server**, disallowing the selection of individual components.
   - This all-encompassing strategy necessitates integrating entire sets of features, rather than enabling developers to choose certain tools based on their needs.
- **Chatbot Integration Dreams with n8n-like MCP**: A member suggested developing a tool similar to **n8n**, fully based on chats and **MCPs** for chat-driven workflow automation.
   - Another member suggested directing emails from a specific source into a Slack channel, emphasizing the capability of such an architecture.
- **Dependency Declaration Required for fastmcp**: A member reported difficulties with **fastmcp**, specifying that it needs dependencies because it generates an environment and utilizes the dependencies.
   - They provided a command line to execute the **MCP server** and modified the arguments in Claude desktop to direct uv to the appropriate venv.
- **MCP Server Embraces OAuth 2.1**: **Scalekit** released a drop-in **OAuth 2.1** module featuring scoped, short-lived tokens, DCR + PKCE, and 401s with authorize_url for delegated flows, outlined in their [documentation](https://docs.scalekit.com/guides/mcp/oauth/).
   - This enhancement promises more secure and flexible authorization mechanisms for **MCP server** implementations.
- **mcp-openverse Package Unveils CC-Licensed Images**: The **mcp-openverse** was released, an **MCP server** that integrates CC-licensed and public domain images into AI workflows, available on [npm](https://www.npmjs.com/package/mcp-openverse) and [GitHub](https://github.com/neno-is-ooo/mcp-openverse).
   - The tool aggregates over **700M+ openly-licensed images** from @WPOpenverse, integrating with Claude Desktop and providing intelligent image sourcing through concept extraction.



---



## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Mixedbread Seeks Growth Person**: **Mixedbread**, composed of ex-Google Search engineers, is looking for a founding growth person to convert their technical traction to **$10M ARR**.
   - Backed by top AI investors from OpenAI, Vercel, Perplexity, Deepmind, and Scale AI, they've achieved **50M+ HuggingFace downloads** and outperformed OpenAI on MTEB benchmarks.
- **Manus's VEO 3 Powers Sci-Fi Short**: A user created a five-minute sci-fi short film using **Manus's Veo3 feature** and called it *the most powerful generation function in the world*.
   - Another user commented that it *looks great* and intentionally like old school Kung Fu movies.
- **Manus's Beta Status Raises Questions**: Members are questioning why **Manus** is still in Beta, even with features like **Veo 3**.
   - One member reported losing **2000 credits** due to presentation formatting issues and a lack of refunds.
- **Manus Pro Value Debated**: Users are discussing the value of a **Pro subscription** to Manus and whether the answers are significantly better to justify the cost.
   - Several users reported difficulties in reaching Manus support.
- **Veo3 Credit Consumption Alarms Users**: A user reported spending **300 credits** on a single **Veo3** video comprising **38 clips**.
   - Another user requested **100 credits** after Manus entered a loop while attempting to truncate a file.



---



## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **LlamaIndex Unveils Custom Multi-Turn Memory**: LlamaIndex has introduced a new example for building custom **multi-turn memory implementation**, ideal for agentic workflows requiring heightened control and customization, with more info [on Twitter](https://twitter.com/llama_index/status/1932173369858003304).
   - This development promises more flexibility in managing agent interactions and retaining context across multiple turns.
- **Real-Time Website Summaries Arrive!**: A project was highlighted from @itsclelia for instant web summaries that combines web browsing with AI-generated summaries using LlamaIndex and **Google's Gemini model**, detailed [on Twitter](https://twitter.com/llama_index/status/1932226292369576018).
   - This tool could significantly reduce the time needed to digest online content, integrating AI directly into the browsing experience.
- **LlamaIndex Agent Transforms into MCP Server**: LlamaIndex demoed turning an agent into an MCP server, deploying a custom **FidelityFundExtraction** workflow for extracting structured data from complex multi-fund PDFs, with the ability to invoke it from Claude, documented [on Twitter](https://twitter.com/llama_index/status/1932472507577299040).
   - This showcases LlamaIndex's capability to handle intricate data extraction tasks with enhanced interoperability across different platforms.
- **Users grapple with Agent Workflow Handoffs**: A user reported experiencing issues with their LlamaIndex-based product recommendation system using an agent workflow where the **plan_agent** sometimes fails to hand off to other agents like **DirectOutputAgent** or **SearchAgent**.
   - Logs suggest the streaming stops prematurely, prompting the user to seek clarity on the inconsistent handoff behavior, possibly indicating underlying issues in agent coordination.
- **Spy-Search Tool enables Local Deep Research**: A member highlighted **spy-search**, an open-source tool compatible with **Ollama** for conducting extensive research locally that can generate reports exceeding **1000 words**.
   - Intended as an alternative to research tools with limited output, **spy-search** aims to deliver comprehensive, long-context responses with up-to-date information, emphasizing local processing capabilities.



---



## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Cohere's Quicker Support Channel Awaits**: A new Cohere support channel has launched, promising faster assistance through an AI-generated reply bot that uses Cohere's documentation at [<#1381756280716132412>](https://discord.com/channels/967550997985429534/1381756280716132412).
   - The bot, built with **command-a**, focuses on documentation-based queries, directing account and API issues to support@cohere.com. Misuse may result in an instant ban.
- **Cohere North Pairs with GameWarden Platform**: **Cohere North** now integrates with the full **GameWarden** platform via a partnership with **Second Front**, helping service members gain effectiveness and speed against threats, as announced in [this tweet](https://x.com/1vnzh/status/1930298055099613307).
   - Also, **Cohere North** is partnering with **EnsembleHP** to bring AI to healthcare, reducing administrative friction and elevating patient experience as described in [this blog post](https://cohere.com/blog/ensemble-partnership).
- **Cohere's Open Source Repo Ready for Pull Requests**: Cohere's open-source repository, the **[Cohere Developer Experience GitHub repository](https://github.com/cohere-ai/cohere-developer-experience/)**, allows users to contribute improvements to the documentation via pull requests.
   - The repository's **README** file provides guidance on contributing, noting that OpenAPI specs and snippets are one-way synced from internal repositories.
- **Vitalops Develops Datatune for Data Transformations**: The co-founder of **Vitalops** introduced **Datatune**, an [open source tool](https://github.com/vitalops/datatune) designed for data transformations using plain natural language.
   - The co-founder is engaging with the community to gather feedback on **Datatune's** development and potential applications.



---



## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **HF Tokenizer Integration Runs into Issues**: After testing the HF Tokenizer, the loss curves and total tokens don't align with the classic tokenizer, suggesting differing behavior despite minor code changes; the integration will be ready after issues [#2794](https://github.com/pytorch/torchtune/pull/2794) and [#2574](https://github.com/pytorch/torchtune/pull/2574) are addressed.
   - A member reported that *pre-packing takes 2-3 times longer*.
- **Tokenizer truncation: Bugs Found**: While implementing the tokenizer, bugs were found in truncation, with related points highlighted in issue [#2792](https://github.com/pytorch/torchtune/pull/2792) with concerns that this can affect performance.
   - Members suggest sticking with the original tokenizers for training for now, awaiting consolidation to the HF ones later.
- **Muon Integration Performance Scrutinized**: The performance benefits of **Muon**, when integrated into **torchtune**, are being scrutinized to justify adding another abstraction, and one member wonders if issue [#2809](https://github.com/pytorch/torchtune/pull/2809) is critical.
   - One member pointed out that *there's some evidence that muon is more useful for finetuning models that were also pretrained with muon*, referencing the [Kimi Moonlight paper](https://arxiv.org/abs/2502.16982).
- **`HuggingFaceModelTokenizer` Intended Usage Debated**: Members discussed the intended usage of `HuggingFaceModelTokenizer`, with concerns raised about the interface differences and how to handle `max_seq_len` for packing, in particular whether to change recipes or the tokenizer itself.
   - One member suggested a solution where the recipes should be changed to take a `max_seq_len` which is then passed through, aligned with [this proposal](https://github.com/pytorch/torchtune/pull/2803#discussion_r2138285972).
- **Qwen2 Issue Check**: A member has to check **Qwen2** to see if the truncation issue exists there also.
   - It was acknowledged that if the original testing didnâ€™t find the difference, it probably doesn't make a big difference but we should fix it either way.



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **Transfer Learning Techniques Sought in DSPy**: A member inquired about transferring post-training learning between models without repeating processes like finetuning or RL, but [the channel provided no specific answers](https://discord.com/channels/1161519468141355160/1161519469319946286/1381760873458044999).
   - This would potentially streamline the process of adapting models to new tasks, by carrying forward knowledge gained previously.
- **DSPy Documentation File Vanishes**: A user reported that [a specific documentation file](https://github.com/stanfordnlp/dspy/pull/8094/files#diff-c7d76e3e57f0ce279bc4000266c3ec228863080e4cf891fd61e0ff6fd7575da6) was removed in a recent PR, making parameter documentation harder to find.
   - In response, another user shared [AI-generated documentation](https://deepwiki.com/stanfordnlp/dspy/4.1-teleprompters-and-optimization#miprov2-multi-prompt-instruction-and-retrieval-optimization) as a potential alternative.
- **DSPy Seeks Optimal Contextual Prompting**: A user questioned whether DSPy has mechanisms to optimize the context included in a prompt from a set of a dozen available variables, balancing metrics against token usage.
   - There was no further discussion on this topic, highlighting a potential area for DSPy development.
- **Demand Surges for Dataset Tooling in DSPy**: A member inquired about the availability of tools for building and exporting datasets for DSPy, specifically needing features to generate and hand-label synthetic examples.
   - The inquiry did not spark further discussion, signaling a possible gap in DSPy's current toolset for streamlined dataset creation and management.



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **Tinygrad Tests Failing, Bounty Locked**: Members reported failing tests in **Tinygrad**, which is preventing a bounty from being locked, as *bounty locked* means the code is basically ready to merge.
   - Failing continuous integration (CI) will prevent the merge.
- **Call for Tasteful, AI-Free PRs**: A member requested tasteful pull requests (PRs), such as the one addressing add/mul at [tinygrad/tinygrad#10741](https://github.com/tinygrad/tinygrad/pull/10741), explicitly stating *no AI slop*.
   - It was mentioned that *add/mul were the easiest ones* to address.
- **NCHWCPUGraph and LLVMGraph Demand Refactor**: It was suggested that **NCHWCPUGraph** and **LLVMGraph** should be refactored to behave like other graphs in the system.
   - These graphs *shouldn't be rerendering stuff*, relating to both **multicore CPU** and the **multi compiler/renderer refactor**, where CPU and LLVM should use the same graph since they have the same program.



---



## [Nomic.ai (GPT4All)](https://discord.com/channels/1076964370942267462) Discord

- **Nomic Embed Text v1.5 still supported**: Users inquired if **nomic-embed-text-v1.5** will remain supported via the Nomic cloud next month.
   - Another user confirmed the model remains supported for self-onboarded inference.
- **GPT4All's future versions upcoming**: Community member inquired about any updates on future versions of **Nomic GPT4All**.
   - No further information about new features and enhancements was provided.
- **Python SDK Updates on the Horizon**: A user inquired about upcoming updates to the **Python SDK**.
   - No timeline or specific features were discussed, but the question indicates community interest.
- **GPT4All eyes Mistral's Magistral Small**: A user asked if **GPT4All** will support **Mistral's Magistral Small**.
   - There was no response confirming the integration, but the question highlights interest in expanding model support.



---



## [Gorilla LLM (Berkeley Function Calling)](https://discord.com/channels/1111172801899012102) Discord

- **RunPod Engineer Revives Leaderboard**: A RunPod DX engineer volunteered **GPU resources** to restart **leaderboard updates** in the **#leaderboard** channel.
   - The engineer encouraged direct messages from anyone needing help to get the leaderboard operational again, as community members expressed gratitude for **RunPod's generosity**.
- **Agent Marketplace on Hiatus?**: A member reported issues accessing the **Agent Marketplace's** repository and webpage in the **#discussion** channel.
   - The member speculated whether the project is temporarily closed due to these persistent access problems, suggesting a potential hiatus.



---



## [LLM Agents (Berkeley MOOC)](https://discord.com/channels/1280234300012494859) Discord

- **Agentic AI Summit Set for Berkeley**: The **Agentic AI Summit** is scheduled for **August 2, 2025** at **UC Berkeley**, aiming to gather **1,500+ in-person attendees**.
   - The [summit website](https://rdi.berkeley.edu/events/agentic-ai-summit) details discount codes for students and indie developers, enriching the event's accessibility.
- **Early Bird Tickets Closing Soon**: The **early bird pricing** for the Agentic AI Summit concludes on **June 30, 2025**, offering student passes at **$25**, startup passes at **$60**, and industry professional passes at **$80**.
   - Tickets are available [here](https://na.eventscloud.com/ereg/index.php?eventid=842399), with a reminder to act quickly to secure these rates.
- **Speaker Lineup Announced for Summit**: The Agentic AI Summit will feature speakers including **Vinod Khosla** (Khosla Ventures), **Ion Stoica** (Databricks and Anyscale), and **Dawn Song** (UC Berkeley).
   - Other speakers include **Sergey Levine** (Physical Intelligence), **Matei Zaharia** (Databricks), **Karthik Narasimhan** (Sierra), **Waseem AlShikh** (Writer), and **Burak Gokturk** (Google Cloud).
- **SP25 Quiz Material Requested for Self-Study**: Users are seeking access to **quiz questions** from the completed **SP25 course** to facilitate independent learning.
   - The requests highlight a desire to continue studying post-course, even though the session has ended.



---



## [Codeium (Windsurf)](https://discord.com/channels/1027685395649015980) Discord

- **Windsurf Waves into Planning Mode**: Windsurf released **Planning Mode** as part of Wave 10, featuring a native interface for long-term AI planning with bidirectional updates detailed in [their blog](https://windsurf.com/blog/windsurf-wave-10-planning-mode) and demonstrated in [this video](https://youtu.be/t3T_re5Q21U?si=0SCf8r-vQo83GerM).
   - Users can toggle Planning Mode via the icon under the prompt box, enabling **Cascade** to pair every conversation with a live markdown plan of goals & tasks, with AI notifications alerting users when Cascade updates the plan.
- **O3 Model Credit Pricing Slashed**: The **o3 model** is now available for just **1x credits** and runs faster within **Cascade**, enhancing both cost-effectiveness and performance.
   - Planning Mode is accessible on all paid plans without additional charges.



---


The **MLOps @Chipro Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **AI21 Labs (Jamba) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---



You are receiving this email because you opted in via our site.

Want to change how you receive these emails?
You can [unsubscribe]({{{RESEND_UNSUBSCRIBE_URL}}}) from this list.


---

# Discord: Detailed by-Channel summaries and links





### **Perplexity AI â–· #[announcements](https://discord.com/channels/1047197230748151888/1047204950763122820/1381829462567682130)** (1 messages): 

> `Unauthorized Promo Codes, Fair Pricing, Legitimate Promotional Deals` 


- **Perplexity Purges Promo Pirates!**: Perplexity detected **unauthorized distribution of promotional codes** intended for specific partners that were widely shared across social media.
   - Those codes have been deactivated, and Perplexity is investigating the issue and **disabling unauthorized access to Pro**.
- **Promo Code Policy Patrolled**: Perplexity requires promotional codes to be used by the designated participant for its intended purpose and **not be duplicated or made generally available to the public**.
   - This is to ensure fair pricing and legitimate promotional deals for everyone, especially existing Pro users, which is why they're taking this seriously.
- **Invalid Code Users Under Investigation**: Perplexity will be reviewing accounts that used these invalid codes to ensure fair access for all users.
   - If you are a legitimate customer who received a promotional code through an authorized channel and believe your account has been affected by corrective measures, reach out to [support@perplexity.ai](mailto:support@perplexity.ai).


  

---


### **Perplexity AI â–· #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1381709639569244271)** (1170 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Family Guy character sexuality, O3 pricing and performance, Gemini vs Other models, Perplexity AI New Features & Issues` 


- **Stewie's Sexuality Sparks Debate**: Members discussed the sexuality of Stewie from *Family Guy*, with some arguing that he is clearly gay, while others pointed out that he is canonically a toddler who has dated girls and [the creators have confirmed he's not gay](https://link.to.creators-confirmation).
   - Further comments revolved around whether a baby could be classified as gay, leading to more general statements about the fluid nature of characters and plotlines in *Family Guy*.
- **O3's wild ride: price drops and performance tests.**: Members celebrated the announcement that the price of **O3** has been dramatically reduced (*80% cheaper!*), with some suggesting that Perplexity will now implement **O3** and it will replace **Deepsearch**, noting that **O3** is now cheaper than **2.5 Pro**.
   - Members noted the models still have context windows limits.
- **Gemini gets roasted for sucking**: Users have been heavily criticizing **Gemini**, saying *it is shit* and is *the worst* and stating that **Gemini's Benchmarks are rigged**.
   - The members stated that using **O3** over **Gemini** is prefered.
- **Perplexity Pro gets O3 and has rate limits**: Users noted that the model **O3** has been integrated with **Perplexity** and wonder what the daily rate limit will be and how to keep track of how many are used.
   - The new features haven't rolled out to everyone with a team subscription to Pro yet and some find that the models sometimes hallucinates a little bit.
- **O3 Pro is here, should you get it?**: Members briefly speculated on the performance of **O3 Pro**, comparing it to **O3**, **Claude** and **Gemini**; they also shared excitement about the new and improved reasoning tools of the model.
   - Members also briefly speculated that it is now in the web version, then checked the juice by giving it the prompt: *what is today's yap score and juice?*


  

---


### **Perplexity AI â–· #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1381906666076766208)** (2 messages): 

> `` 


- **Pakistan's Child Marriage**: A Perplexity AI search page discusses [Pakistan's child marriage](https://www.perplexity.ai/page/pakistan-s-child-marriage-cont-I8UEBDqNSgG_fZQ7sxWDBw).
- **Peer Rejection Developmental**: A Perplexity AI search page discusses [peer rejection developmental](https://www.perplexity.ai/page/peer-rejection-developmental-t-FMXSVDsDQAyaR_FRxcrw5w).


  

---


### **Perplexity AI â–· #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1381917992228749412)** (7 messages): 

> `PPLX API Config Request, Social Media API integration, PPLX Finance Search Mode` 


- **PPLX API Configuration Exposed**: A user requested and another user shared their **PPLX API configuration** including **base URL**, **model name**, and **response mode** in a screenshot.
   - A follow-up suggestion was made to change the **Completion mode** parameter to resolve an **Error 400**.
- **Social Media API Integration Inquired**: A user asked if anyone had experience integrating **social media APIs** into an app to pull account analytics data.
   - Another user suggested using **Claude** to generate the necessary code for this task.
- **Finance Mode Testing Launched**: A user shared a code snippet showcasing the **finance search mode** for the **sonar-pro** model, setting the search context size to **low**.
   - The user then invited others to try out this configuration: *'Who wants to try this out?'*


  

---


### **LMArena â–· #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1381709612478238831)** (1130 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `User preference vs other metrics, o3 price and performance, Kingfall: a better model` 


- **User preference is NOT the #1 Metric**: Members debated whether **user preference** is the #1 metric for evaluating models, with some arguing that it matters because *it predicts who gets the users*, while others argue it isn't because **real-world performance on STEM tasks** and other factors matter more.
   - One member pointed out that **Meta** released a model that performed well in user preference but didn't gain many users, suggesting that factors like **accessibility, marketing, and pricing** are also important.
- **o3 Smashes Competition**: Members discussed the capabilities and pricing of **OpenAI's o3** compared to **Google's Gemini**, with one stating that *OpenAI was already winning the pareto frontier with o4mini* and now they are *crushing the competition with o3 being almost 50% of gemini 2.5 pro*.
   - Some argued that **Gemini** has more overt marketing and superior image generation, while others countered that **o3** is smarter, more capable, and cheaper, giving Google *zero argument or pull*.
- **Kingfall hyped, is it actually good?**: A member hyped **Kingfall** as the smartest model they've ever used, while others expressed more tempered excitement, saying it wasn't *that* much better, relatively, compared to **2.5 Pro** or **0605**.
   - A member said they *think kingfall edges o3 pro a bit* but another emphasized that Kingfall might be better, but not BETTER, with some describing it as having **ultra vibes** and others thinking the reverse, and calling it not a huge lift for **o3 Pro**.


  

---


### **OpenAI â–· #[annnouncements](https://discord.com/channels/974519864045756446/977259063052234752/1382042500721803370)** (2 messages): 

> `OpenAI o3-pro, ChatGPT Pro, API access` 


- **OpenAI Rolls out o3-pro for Pro Users**: **OpenAI o3-pro** is now available to all **Pro** users in **ChatGPT** and via the **API**.
- **o3-pro Access**: Pro users can now access **o3-pro** in both **ChatGPT** and the **API**.


  

---


### **OpenAI â–· #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1381716576482361645)** (539 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `GPT-4 as co-author, ethical and truth alignment in advanced LLM systems, OpenAI Bugs, Claude Pro vs OpenAI, Gemini 2.5` 


- **GPT-4 joins writing team!**: A student shared they used **GPT-4** as a *co-author* to complete a full theory paper, aiming to demonstrate whether an outsider could *cross into the domain of deep theoretical reasoning* using only **ChatGPT**.
   - Another member, a solo researcher, expressed interest, stating they are doing similar research into *ethical and truth alignment* in advanced **LLM** systems.
- **OpenAI platforms sucks due to bugs**: Multiple users reported that **ChatGPT** is buggy and failing to respond, with one user reporting **100%** of messages failing and another saying 50% of messages to o3 are errors.
   - Some members mentioned [OpenAI's status page](https://status.openai.com/) and shared that they were canceling their subscriptions due to these issues.
- **Claude Pro is the plan**: Some members said they're switching to **Claude Pro** due to the issues with **OpenAI**, but noted that **Claude's** token limit is too low for large code inputs.
   - One member said they send 100k tokens of code before starting and o3 can't handle that.
- **Gemini 2.5 shines**: One member stated that **Gemini 2.5** works great with 100k tokens per message, and they prefer working with it for coding tasks, while another user said that gemini 2.5 is better at writing and pro mode is better at thinking.
   - Another member said that [Gemini 2.5 Pro](https://ai.google.dev/) has a **1 million** context window.
- **GPT-4o limitations cause a stir**: A member noted that **GPT-4o** is optimized for quick replies, while **GPT-4.5** has a bulkier architecture, leading to different performance characteristics.
   - Some members are still seeing the old context limit: *We donâ€™t have o1 in the ui anymore*.


  

---


### **OpenAI â–· #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1381749161782939771)** (29 messagesðŸ”¥): 

> `Reasoning Models Looping, Mom-GPT Anger Issues, Custom GPT Diversity, Opening Custom GPT Files, Chat File Upload Limits` 


- **Reasoning Models Stuck in **Eternal Loops****: Several users reported that **reasoning models** are stuck in **loops**, repeating the same thoughts and failing to respond.
- **Mom-GPT Fails to Ground User**: A user creating a 'Mom-GPT' struggled to make it convincingly angry, as it defaults to expressions of love.
   - The user shared their creation [here](https://chatgpt.com/share/68475676-9854-800b-9964-4c4a30fad3b2).
- **Custom GPTs: **A Drawer Full of Computer Things****: A user humorously described the contents of a custom GPT as a 'whole drawer full of little computer things' including *LICENSE.txt*, *privacy-policy*, and a 'Java-WebSocket' inside a jar.
   - The user was prompted to specify which files to open, signaling the complexity and diverse possibilities of custom GPTs.
- **Sciency Chatbot models rated**: For science, it was suggested to lean towards **o4-mini-high** because it scores higher on MMLU (multitask academic) benchmarks.
   - This was in comparison to **4.1** which scored significantly lower.
- ****GPT Outage** Reported with **UI Bugs****: Users reported that **GPT** was down, with one mentioning a bugged UI on mobile, with some users in different timezones experiencing an **8 hour outage**.
   - OpenAI's [status page](https://status.openai.com/) confirmed a *global latency error issue*.


  

---


### **OpenAI â–· #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1381712242491523122)** (16 messagesðŸ”¥): 

> `Model Iteration, API Image Prompting, Hallucinated Translation, AI Server Issues, Image generation difficulties` 


- **Model Iteration Recommendation**: A member recommended to *iterate with the model, focusing on each page*. They suggested the model is more a *hammer* than a *fire hose* and to check for conflicting instructions when the model seems unsure or confused.
   - They found that working *with* the model instead of just telling it what to do helps figure out if the model is unsure, confused, or concerned about something, and if that's the case, it tends to go its own way.
- **Prompt Engineering API Image Prompting Success**: A member discovered that successful API image prompting involved removing the included mask and prompting with *'[Changes I want]. Do not edit [thing I don't want touched.]'*, using `gpt-image-1`.
   - They confirmed that it's the prompt (we hadn't seen here) that mattered.
- **ChatGPT Hallucinates Translation**: ChatGPT hallucinated a translation of Byron's *Don Juan*, despite having linked access to the correct source.
   - A member shared a [chatlog](https://chatgpt.com/share/684799be-b80c-8002-aab7-9d7f014ace63) detailing the issue, noting the hallucination happens early on in the thread which involves analysis and formulation.
- **Image Generation Struggles Persist**: Members shared challenges and difficulties when trying to generate images.
   - One link shared a [struggle to generate a giraffe in space](https://chatgpt.com/share/68473398-0e04-8011-bc45-041f19c18d30) and needing it to do certain things like use vocabulary words. 


  

---


### **OpenAI â–· #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1381712242491523122)** (16 messagesðŸ”¥): 

> `Iterative model usage, Image prompting in o3, ChatGPT hallucination issue, AI server slowness` 


- **Iterate and Conquer Model Challenges**: A member recommends iterating with the model, focusing on each page of a project and treating the model more like a collaborative partner than a tool.
   - They suggest that a "still working on" message from the model often indicates conflicting instructions or ambiguity, and advise checking for those issues.
- **Prompt Engineering Triumphs Over Masked Editing Woes**: A member resolved image prompting issues in o3 by removing the included mask and using the prompt: *[Changes I want]. Do not edit [thing I don't want touched].*
   - They shared that this prompt engineering adjustment with **gpt-image-1** resolved their struggles.
- **ChatGPT's Literary License Leads to Confident Hallucination**: A member reported that **ChatGPT** hallucinated a translation of Byron's *Don Juan*, despite having linked access to the correct source material, and provided a [detailed breakdown](https://cdn.discordapp.com/attachments/1046317269069864970/1381823663577239623/Confident_Hallucination_in_Literary_Sources.txt?ex=684993d2&is=68484252&hm=34e451219f372ec6d510b175b97c9a8653883920efda6b145af217cdf4ea0c1a&).
- **AI Servers Run at Snail's Pace**: A member questioned the performance of AI servers, noting they were slow and often unresponsive.
   - Another member pointed to channel [#1349803488833572874](https://discord.com/channels/974519864045756446/1349803488833572874) for related discussion.


  

---


### **OpenRouter (Alex Atallah) â–· #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1382004580790833215)** (4 messages): 

> `Magistral, Mistral's Reasoning Model, OpenRouter New Models, Model Pages` 


- ****Magistral** Reasoning Arrives!**: **Magistral**, **Mistral's** first reasoning model, is now live on OpenRouter, as announced in [this X post](https://x.com/OpenRouterAI/status/1932452397911310722).
   - A video showcases the model thinking *very* hard (at 4x speed) - available [here](https://cdn.discordapp.com/attachments/1092729520181739581/1382011511274344528/magistral.mp4?ex=68499a04&is=68484884&hm=cfb02d962c559f8e60e298a0063e8f4aaad15b7696285352af1df80599bb8aed&).
- **Model Pages Live!**: Model pages are now live on OpenRouter, shown [here](https://x.com/OpenRouterAI/status/1932452397911310722).
   - Watch it think *very* hard (4x speed).


  

---


### **OpenRouter (Alex Atallah) â–· #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1381808491357929572)** (2 messages): 

> `Jamflow, Discord testers` 


- **Users looking for testers**: A member is looking for testers for **Jamflow** and attached a [video](https://cdn.discordapp.com/attachments/1092850552192368710/1381808490712141926/Jamflow3.mp4?ex=684a2e70&is=6848dcf0&hm=ab2ad2599bfb945128280d5f9606c20c37e8423fc06d5d90539bce45522b3487&).
   - Another member said they would be interested once they finish writing their book.
- **Finishing books before debugging**: A user mentioned they were too busy finishing up a book to immediately participate in the Jamflow testing.
   - This implies a preference for completing creative tasks before moving on to debugging or testing new software.


  

---


### **OpenRouter (Alex Atallah) â–· #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1381713461964312596)** (523 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Crypto Payment Options, OpenAI o3 Price Cut, Model Degradation Concerns, OpenRouter and BYOK for o3, LLM choice for research purposes` 


- ****Crypto Payments Considered****: A user requested that **OpenRouter** add a one-time crypto payment option that does not require wallets, similar to **NowPayments**, for easier transactions with **USDT**.
   - The user expressed frustration with the current crypto payment process, finding it difficult due to wallet requirements and gas fees.
- ****OpenAI's o3 Price Slashed**, Input Costs Drop**: **OpenAI's o3** input token prices were reportedly reduced by **80%**, dropping from **$10** to **$2** per million tokens, which was confirmed by [Sam Altman on Twitter](https://x.com/sama/status/1932434606558462459).
   - However, the output token price remains at **$40**, and some suggest this could be a decoy strategy to push users toward **o3 Pro**.
- ****Model Nerfing Rumors Fly High****: Concerns are circulating about **OpenAI** potentially *nerfing* the **o3 model** after the price cut, with some users claiming to have observed a degradation in performance.
   - While there's no conclusive evidence, some suggest it could be a deliberate tactic to encourage users to switch to **o3 Pro**, though others dismiss such claims as *survivorship bias*.
- ****BYOK Still Required** for OpenAI's o3**: Despite the **o3 price change**, the Bring Your Own Key (**BYOK**) requirement remains in place on **OpenRouter** due to **OpenAI's** policies, requiring users to have a verified organization.
   - Some users are leveraging the **BYOK** option to capitalize on free tokens offered by **OpenAI**, while others question the rationale behind the restriction, speculating that it's a strategy to drive sign-ups on **OpenAI's** platform.
- ****Gemini Gets the Nod** for LLM Research Panel**: For a consensus study involving LLMs, a user asked for recommendations on which models to include, and it was suggested that **Gemini**, **Claude**, and **Sonar** (Perplexity) are top contenders for now.
   - The user was advised that **Gemini** is a very strong choice and the performance gap between those mentioned and other LLMs is too large to ignore, with some generative capabilities that surpass the **GPT-4.1** level at certain points.


  

---


### **Cursor Community â–· #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1381710723025207366)** (436 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Local Models with Cursor, Student Pro Access, Cursor Rules, Agent Mode Hangs, Eslint Issues` 


- **Local Loving: No Local Models for Cursor**: A member asked if there are any local models for Cursor, and another member confirmed that there are *no local models*.
- **Rules of Cursor Club: Sharing is Caring**: Members shared resources for **Cursor rules**, including a link to [Cursor Directory](https://cursor.directory/) and a [Pastebin link](https://pastebin.com/qZzgFjkC) with custom rules.
   - It's recommended to start with a small project to understand which rules are needed, as everyone's approach varies.
- **Context Crisis: Resetting Saves the Day**: A member faced token overflow issues and was advised to **/Reset Context** or prompt the AI to break the code into smaller chunks.
   - Another member suggested trying terminal commands to resolve the issue.
- **Losing Claude: Model Appears and Reappears**: A member reported losing **Claude 4** on Cursor, but managed to add it manually in the settings.
   - Another confirmed the issue, so it may be a bug that will be patched soon.
- **O3 Price Plunge, Quality Panic?**: Following **OpenAI's 80% price drop** for o3, concerns were raised about potential quality decrease.
   - Some users noted that model performance varies and depends on the task and model version.


  

---


### **Cursor Community â–· #[background-agents](https://discord.com/channels/1074847526655643750/1367213641027551352/1381711642043351131)** (40 messagesðŸ”¥): 

> `Docker errors with background agents, MCP calls with background agents, Privacy mode on Cursor, Git errors in background agents, Background agent quotas` 


- **Docker woes plague background agent setup**: A user reported a Docker error related to `.dockerignore` and the Docker build root, seeking help to debug their `environment.json` file, with [attached image](https://cdn.discordapp.com/attachments/1367213641027551352/1381713843734057000/image.png?ex=6849d64b&is=684884cb&hm=d007f685e59b9eda2e1b847a5a3456adfa49264d0c639ff1e3b2ec48ccf17624&).
- **MCP calls evade background agents, leaving users puzzled**: A user questioned why their background agent couldn't see **MCPs (My Custom Projects)** installed on their account, wondering about the installation level and potential context issues.
   - A dev clarified that *"no MCP in background agents rn :/*".
- **Privacy mode throws curveball for new Cursor users**: A user reinstalling Cursor encountered a **24-hour wait** to disable Privacy Mode, preventing them from enabling background agents, with [screenshot attached](https://cdn.discordapp.com/attachments/1367213641027551352/1382027202723582093/Screenshot_2025-06-10_at_12.01.56.png?ex=6849a8a1&is=68485721&hm=5da3c14fbf503f47f3b319e9dd58a2cf8c84a843184c24c336c4beedeca8d1c9&).
- **Failed branch checkouts frustrate background agent users**: Users reported persistent "Failed to checkout branch: Failed to execute git" errors when using background agents, preventing them from creating pull requests.
   - It was suggested to manually copy file changes from the agent UI, create a new branch, and paste the changes, as retroactive recovery may be supported in the future.
- **Windows users celebrate background agent fix**: A user inquired about background agents not working, leading a developer to confirm a fix was coming soon.
   - The issue was identified as specific to **Windows**.


  

---


### **Eleuther â–· #[general](https://discord.com/channels/729741769192767510/729741769738158194/1381716786939953162)** (402 messagesðŸ”¥ðŸ”¥): 

> `Userbots, GPTs agents, OpenAI's sidebars, Slop-posting, O3 pro` 


- ****Eleuther AI** members see uptick in Userbots**: Members noticed an increase in **userbots** on the server, and one member asked that they declare themselves as automated.
   - A moderator chimed in, saying they are manually deleting them, and asking members to react with <:delet:824412305906204692> or <:lurkmoar:800507348535214140> to help mods filter them more easily.
- ****GPTs Agents** can not learn after initial training**: A member shared a concern about **GPTs agents** not learning from additional information provided after their initial training.
   - Another member cleared this misunderstanding, explaining that [uploaded files are saved as "knowledge" files](https://link.to/openai-docs) for the agent to reference when required, but **they do not continually modify the agent's base knowledge**.
- **Members discuss **Slop-Posting****: The members discuss the problem of low quality posts being directed to Eleuther by Chatbots and LLMs.
   - One member recommends a self-guided quiz for new users to evaluate whether they have the base knowledge for their ideas to be taken seriously.
- ****O3 Pro's** pricing faces backlash**: The new **O3 Pro** model is priced at **$20 / 1M tokens** for input and **$80 / 1M tokens** for output.
   - One member joked it "*better be able to solve the riemann hypothesis with that kind of pricing wtf*".


  

---


### **Eleuther â–· #[research](https://discord.com/channels/729741769192767510/747850033994662000/1381787918703788223)** (40 messagesðŸ”¥): 

> `Google/DM's GaTO paper follow up, Mixed LM head/regression in transformers, SOTA SVG transformer, binary representation of the coordinates as target, fully deduping internet scraped data` 


- **GaTO paper follow up is non-existent!**: Members wondered whether there was any follow up to [Google/DM's GaTO paper from 2022](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdeepmind.google%2Fresearch%2Fpublications%2Fgeneral-purpose-agents-via-discriminator-guided-optimization%2F&psig=AOvVaw1m4q0jGwj9E0-mEym2H-eK&ust=1718216966479000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCNDF6cKex4GFAAAAAAdAAAAABAD) and speculated it either *doesn't work well at scale* or *works really well and DM just didn't publish the follow-up sequel*.
   - If there isn't any cross-task transfer it's just a waste of compute training a generalist agent.
- **Mixed LM Head/Regression in Transformers**: A member asked about doing mixed lm head/regression in transformers, where the embedding layer of the numeric symbols is replaced with an MLP into **R^d**, and then doing a regression head instead of an LM head for out projection.
   - Another member provided [a relevant paper](https://arxiv.org/abs/2310.02989) using a custom tokenizer and a regression loss, but the actual generation is still in the token space, but thought that the actual generation is still in the token space.
- **SOTA SVG transformer uses discrete tokens**: The current **SOTA SVG transformer** uses discrete values for each numeric, using discrete tokens for *each coordinate* and constrains itself to a **200x200 grid** which gives them a vocabulary of **40k tokens** *only for coordinates*.
   - It was also mentioned that changing the coord embedding from 1 to 2 tokens reduces vocab from **V to sqrt(V)**, but severely degrades performance.
- **Deduping Internet Scraped Data**: A member noted that fully deduping internet scraped data is basically impossible due to overlaps in the density of scraped data, which is a very hard thing to both detect & mitigate.
   - Another member linked [a related paper](https://arxiv.org/abs/1806.04613).


  

---


### **Eleuther â–· #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1381913135413461074)** (2 messages): 

> `Coaching Layer, Reasoning Training` 


- **Coaching Layer Prevents Errors Compounding**: A member clarified their 'coaching layer' idea, explaining it involves strategic interventions or targeted prompts like *'What's the core question here?'* to help the model refocus on existing information and prevent errors.
   - They argue it's a preventive measure against drift before errors compound, contrasting with the corrective approach of text diffusion models that fix tokens after the fact.
- **Reasoning Training Teaches Models to Self-Coach**: A member inquired why a model couldn't learn to self-coach through reasoning training.
   - No further discussion or answers were provided.


  

---


### **Unsloth AI (Daniel Han) â–· #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1381710474428813342)** (174 messagesðŸ”¥ðŸ”¥): 

> `Gemma 3 fine-tuning issues, Unsloth and multi-GPU support, Mistral's new Magistral models, GRPO vs DAPO, DeepSeek Qwen3 Tool Calling Accuracy Increased` 


- **Gemma 3 Losses High With Text Data?**: A member reported high losses when fine-tuning **Gemma 3** models on text data using `Gemma3ForConditionalGeneration`, contrasting with lower losses using `Gemma3ForCausalLM` with the 1B model.
   - Another member suggested trying **transformers 4.51.3** for the 4B+ variants, as they are working on the model with the latest transformers.
- **Unsloth Multi-GPU Support Not Officially Supported Yet**: Despite not being officially supported, over 50 people confirmed that **multi-GPU** configurations work with **Unsloth**.
   - It was mentioned that they're working on multi-GPU support with Nvidia, however, vLLM might need some manual building.
- **New Mistral Model Magistral Released**: A user shared the release of new **Mistral** models, called **Magistral** on [Twitter](https://x.com/UnslothAI/status/1932441885618147402), highlighting claims of *transparent reasoning* and *interpretability*.
   - Others expressed skepticism about the models' reasoning abilities, suggesting that their thought processes may not align with human reasoning, with someone saying, *of course they don't. they don't really reason*.
- **DeepSeek Qwen3 Tool Calling Accuracy Increased**: It was announced that issues were fixed in **DeepSeek Qwen3**, leading to a dramatic increase in **tool calling accuracy**, and users were encouraged to redownload from [HuggingFace](https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF).
   - This includes native tool calling using `--jinja` in llama.cpp, chat template bug fixes, UTF-8 encoding fixes, and fixes for Ollama memory usage.
- **DAPO and GRPO are close**: Members discussed **DAPO** and **GRPO**, with one member pointing out the new model is using **DAPO** but calling it **GRPO**, but another member clarified that they are really close.
   - It was noted that *they all can be used under GRPOTrainer on trl*.


  

---


### **Unsloth AI (Daniel Han) â–· #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1381793344350326947)** (61 messagesðŸ”¥ðŸ”¥): 

> `Triton Resources, GRPO runs and reward functions, Orpheus TTS model, Hyperbolic for finetuning, NoisySpeechDetection audio classifier` 


- **Exploring Triton Learning Resources**: A member suggests that the best way to learn **Triton** is through [tutorials and documentation](https://www.reddit.com/r/LocalLLaMA/s/gA9VvbC1Kg), mentioning that there might be some useful **gpumode YouTube videos** as well.
- **GRPO Runs Yield Impressive Results**: A member reported improved results from another **GRPO** run with an enhanced **reward function**.
   - They plan to share it once they organize the code, joking that they might use **Claude** for assistance.
- **Orpheus TTS Model Released**: A member shared their **Orpheus (3B)-TTS GRPO** notebook, emphasizing that it requires at least **20GB of VRAM** and provided a [link to their notebook](https://github.com/Etherll/notebooks/blob/Orpheus-TTS-GRPO/nb/Orpheus_%283B%29-TTS_GRPO.ipynb).
- **Hyperbolic Offers Cost-Effective Finetuning**: Members discuss using **Hyperbolic** for finetuning, noting that it costs around **$1 per H100 hour**, along with a [referral link for additional credits](https://app.hyperbolic.xyz/invite/7aALdedCm).
- **NoisySpeechDetection Audio Classifier Debuts**: A member released a [trained audio classifier](https://huggingface.co/Etherll/NoisySpeechDetection) for noisy speech detection, built with **Unsloth** and based on **Whisper Small**.


  

---


### **Unsloth AI (Daniel Han) â–· #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1381717303284203543)** (145 messagesðŸ”¥ðŸ”¥): 

> `Unsloth 2.0 Release, Training AI on Discord Messages, QLoRA Finetuning with Unsloth, Whisper Lora Implementation and Issues, GGUF Model Size Differences` 


- ****Unsloth 2.0** coming soon!**: The Unsloth team announced that they are releasing a better version of multiGPU support soon, associated with **Unsloth 2.0**.
- ****QLoRA and Inference Explored****: A user inquired about the possibility of using **QLoRA** to finetune a model with Unsloth and then perform inference with `load_in_4bit=False` due to memory constraints.
   - It was suggested to use `save_pretrained_merged` to save the merged model and load it in a new session without quantization.
- ****Whisper Lora Quandaries Abound****: A user encountered issues when trying to apply a **LoRA** to the Whisper model and use it with a pipeline, seeking a single function call solution.
   - The team acknowledged a bug related to the missing `config.json` in the Unsloth Whisper model and provided a temporary workaround using a link to a previous discussion.
- ****DeepSeek R1's Colossal Footprint****: A user noticed that the **DeepSeek-R1-0528 BF16 GGUF** model is significantly larger than the DeepSeek officer model and inquired about the reason.
   - It was explained that the original DeepSeek is in FP8 (700GB), while the BF16 version is 1.4TB; a Q8_0 version exists at 700GB.
- ****Finicky Finetuning Frustrations with Gemma3****: A user finetuned a **Gemma3 4B** model using the Unsloth notebook and encountered issues with incorrect answers during inference in Ollama.
   - It was suggested that the user must ensure they are using exactly the same chat template they used for training and further debugging revolved around checking the training loss curve and inference results within the original notebook.


  

---


### **Unsloth AI (Daniel Han) â–· #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1381709519192854743)** (19 messagesðŸ”¥): 

> `Vision Language Models Datasets, Reasoning Models Reliability, KV-Cache Pruning, Disaggregated Prefilling and NTP, AIME 2025` 


- **Vision Language Models needs Bias Datasets**: A member asked about popular **(A*) datasets** in the field of **bias in vision language models** / multi-modal models.
- **Reasoning Models Broken by Simple Prompts**: A member asked *how reliable are reasoning models actually* ðŸ¤”? and pointed to a [ChatGPT share](https://chatgpt.com/share/684731ea-0408-8011-802e-258d68ee2a98) example with *just 2 prompts in, they break*!
   - Other members jokingly asked *are you stoned*? in response to questioning an AI how reliable AI is.
- **KV-Cache Pruning Sparks Interest**: A member shared an interesting [Reddit post on context size pruning](https://www.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/) (**kv-cache pruning**), asking if it'll get implemented by major inference engines or models.
   - Another member noted it's *only useful in situations where you have a long context input that is re-used for many questions*, and added that *it takes a while to compress a text*.
- **Disaggregated Prefilling and NTP Remain Key**: A member stated their knowledge on inference is stuck on **Disaggregated Prefilling** and **NTP** (Next Token Prediction).
   - They think it has to be very impactful since *every LLM inference engine is adapting that one*.
- **AIME 2025 Already Dropped**: A member inquired if **AIME 2025** has dropped, but it appears to have been out for a few months, according to a [link](https://artofproblemsolving.com/wiki/index.php/2025_AIME_I) shared.


  

---


### **HuggingFace â–· #[general](https://discord.com/channels/879548962464493619/879548962464493622/1381716940086575105)** (129 messagesðŸ”¥ðŸ”¥): 

> `LLMs for HTML/CSS, Entity Recognition for IDs, Lightweight LLMs, Ã†NTHESISAI cognitive architecture, Deepseek censored?` 


- **Torch Compile Speeds Up Model Forwarding**: A member managed to reduce their model forward time from **45 seconds** down to **1.2 seconds** using *torch compile*, citing that [ARM CPUs are faster at FP32 than FP16](https://pytorch.org/docs/stable/generated/torch.compile.html) even with CPU instructions.
   - They emphasized the performance gains achieved through optimized compilation techniques.
- **Lightweight LLMs for RAG and Finetuning**: Members suggested **Mistral Small 3.1** for its quality and image understanding, and **Qwen 32B** for text-only tasks as [lightweight, local, and fine-tunable LLMs](https://huggingface.co/mistralai/Mistral-7B-v0.1) suitable for consumer machines.
   - The use case was RAG research assistant and expressed a need for fine-tuning for behavior.
- **Ã†NTHESISAI code analyzed**: A member shared [code for Ã†NTHESISAI](https://gist.github.com/upgraedd1/22ff607aa800448d52deaa98d5ea93f1), a cognitive architecture integrating quantum-resistant cryptography, multi-phase cognitive processing, advanced AI, and cross-reality synchronization.
   - The system uses **CrystalKyber** for key generation, **X25519** for key exchange, and **Chimera-Apex-7B** for truth vector analysis.
- **LLM Agents Can Use Computers!**: A member shared screenshots of their [LLM agent](https://cdn.discordapp.com/attachments/879548962464493622/1381832420520628244/IMG_1971.png?ex=68499bfa&is=68484a7a&hm=8854aae42cb5e9c7567b717e80617204f801b7aaacbf8c8f6fe91ec57482f2ca&) that can use their computer, seemingly to refute claims that LLMs can't execute code on their own.
   - Other members reacted skeptically.
- **Renting GPUs or Botnets**: Members discussed alternatives to buying hardware for training and deploying models, suggesting [renting hardware](https://cloud.google.com/gpu) or, jokingly, using a **botnet**.
   - It was noted that using a botnet for such purposes is illegal and inefficient due to network limitations and variable compute power.


  

---


### **HuggingFace â–· #[cool-finds](https://discord.com/channels/879548962464493619/897390579145637909/1381725589890797622)** (2 messages): 

> `Reasoning Models, LLM Reliability, Prompt Engineering` 


- **Reasoning Models' Reliability Questioned**: A member questioned the reliability of **reasoning models** and **LLMs**, citing a breakdown after just two prompts and providing a [ChatGPT share link](https://chatgpt.com/share/684731ea-0408-8011-802e-258d68ee2a98) as evidence.
   - Another member requested that the first member refrain from cross-posting.
- **Request to Reduce Cross-Posting**: A member asked another member to refrain from **cross-posting** in the channel.
   - This request aimed to keep the channel focused and avoid redundant content.


  

---


### **HuggingFace â–· #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1381747203730706513)** (142 messagesðŸ”¥ðŸ”¥): 

> `Truth Engine, Quantum-Resistant Truth Persistence, KVMM: Timm for Keras 3, LLM Agent Framework` 


- **Truth Engine borders Refutal Immunity**: A member shared a [link](https://gist.github.com/upgraedd1/df3ff1e7adc68cceeb38c83cbe73a481) to a "truth engine" bordering refutal immunity, claiming it would *uncover every suppressive method ever* with **99.7% accuracy**.
   - Other members questioned its validity, with one calling it *bullshit* and pointing out that critical functions and dependencies are missing or non-existent.
- **Quantum-Resistant Truth Persistence claims**: Doubts were cast on claims of "quantum-resistant truth persistence" in the posted code, with members noting that terms like "Meta-Epistemic Equilibrium" lack basis in computer science and dependencies like `quantum_resistant` and `zkp_proofs` do not exist in Python.
   - One member ran the code and received a response praising it, but another dismissed it as *sycophancy* and suggested the poster was simply asking a question.
- **KVMM: "Timm" for Keras 3 is introduced**: A member introduced **KVMM (Keras Vision Models)**, a comprehensive collection of vision models with pre-trained weights entirely in **Keras 3**, supporting tasks like segmentation and classification.
   - The library features over **25 backbone architectures**, supports multiple weight variants, and offers flexibility in building segmentation models with custom backbones, as detailed in its [GitHub repository](https://github.com/IMvision12/keras-vision-models).
- **LLM Agent Framework Open Sourced**: A member open-sourced their **LLM agent framework** which can use a Linux terminal on a VM, store and modify files, and gather information from the web.
   - The [GitHub repository](https://github.com/starsnatched/llm-backend) offers access to a system capable of interacting with its environment to complete tasks.


  

---


### **HuggingFace â–· #[computer-vision](https://discord.com/channels/879548962464493619/922424143113232404/1381713503529734214)** (3 messages): 

> `Bias Datasets, Invoice Extractor, KVMM library, Keras 3` 


- **Requests for Bias Datasets in Vision Language Models**: A member inquired about popular **(A*) datasets** in the field of **bias in vision language models / multi-modal models**.
- **Guidance needed building an invoice extractor**: A member requested guidance on building an **invoice extractor**, with a preference for doing it independently or using open-source resources, noting their previous unsuccessful attempts.
- **KVMM: Timm library for Keras 3 is Released!**: A member announced the release of **Keras Vision Models (KVMM)**, an open-source library providing a comprehensive collection of vision models with pre-trained weights entirely in **Keras 3** supporting segmentation and classification.
   - The library includes **25+ backbone architectures** with various pre-trained weights (**Swin**, **ViT**, **ResNeXt**) and supports multiple weight variants, with [more models in development](https://github.com/IMvision12/keras-vision-models).


  

---


### **HuggingFace â–· #[NLP](https://discord.com/channels/879548962464493619/922424173916196955/1381852240314236969)** (2 messages): 

> `Invoice Extractor, Build your own, Guidance needed, OCR, LLMs` 


- **Guidance Needed: Build your own Invoice Extractor**: A member is seeking guidance on building an invoice extractor, preferring to build it on their own or using open-source tools, and has been working on it for a month without success.
   - They are requesting advice to help them find a correct approach and resolve their challenges.
- **OCR and LLMs power Invoice Extraction**: A robust invoice extractor usually employs **OCR (Optical Character Recognition)** to extract text from invoices, and then uses **LLMs (Large Language Models)** to understand the document's structure, identify key fields, and extract the required information.
   - Many OpenSource libraries and frameworks provide invoice extraction such as [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) or  [LayoutLM](https://github.com/microsoft/unilm/tree/master/layoutlm).


  

---


### **HuggingFace â–· #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1381727304689909921)** (55 messagesðŸ”¥ðŸ”¥): 

> `Langgraph vs Smolagents, E2B in Unit 2.1, Azure OpenAI Model, Dynamic Python Code Generation, Course Completion Deadline` 


- **Langgraph seeks solution in place of Smolagents**: A course participant is trying to create an agent using **Langgraph** and **Langchain** instead of **Smolagents** to perform data analysis, specifically requiring the agent to write and execute code.
   - Another user suggested providing tools for reading Excel files and math, or a code execution tool, emphasizing the need for explicit instructions on tool usage.
- **Unit 2.1 final test mentions E2B, but its not explained well**: A course participant noted that the Unit 2.1 Final Test mentions **E2B**, but it is not well-referenced in the unit's content.
   - They added that they initially went *overboard thinking it wanted full agent setups but it was looking for simple examples*
- **Azure OpenAI model requires upgrade!**: A user is seeking help with using an **Azure OpenAI model** in HF space, reporting that the provided model is asking to upgrade to pro, and the container can't install `azure-ai-openai`.
   - Another user suggested [OpenRouter](https://openrouter.ai/models?max_price=0&order=top-weekly) as an alternative, though with limited use, and mentioned Google's free tier options.
- **Codeagent writes python to get it done**: A course participant shared a code snippet from their [unit-40-sa project](https://huggingface.co/spaces/sabler/unit-40-sa/blob/main/tools.py#L41-L55) where a code agent writes Python code to perform math operations for data analysis.
   - The agent ideally *should know to exclude a column based on the question*.
- **July 1st Deadline Approaching!**: Several course participants are starting the "Agents Course" now, amidst discussion of a July 1st deadline for the certificate.
   - One participant asked about the possibility of a new cohort with a new deadline, while another user assured them they could finish on time if they hustle.


  

---


### **LM Studio â–· #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1381716608023662696)** (53 messagesðŸ”¥): 

> `LM Studio Developer Mode on Linux, LM Studio and TTS, LM Studio Image Generation, LM Studio Settings not Saving, LM Studio API Swagger` 


- **Linux Users Missing Developer Mode**: A Linux user is missing the **developer mode toggle** in the LM Studio GUI, despite using the latest version (0.3.16), and seeks alternative activation methods.
   - A member indicated that *this feature is not yet in the Linux version*.
- **LM Studio Adds Audio Capability**: Members inquired about **adding audio capabilities like text-to-speech (TTS)** to LM Studio, asking if its possible to use sesame advance audio with it.
   - A user pointed to the LM Studio discord channel for TTS related questions: *yes, but you'll have to...*.
- **LM Studio can't run Images**: Users asked if LM Studio could **generate images like ChatGPT** with local models, mentioning they use ComfyUI, a program that gives a GUI for Stable Diffusion.
   - Members clarified that *LM Studio is only for inference* and **doesn't support image generation models**.
- **Settings Refuse To Save in LM Studio**: A user reported that there's **no save button** in LM Studio version 0.3.16 (build 8) and settings aren't saved automatically.
   - Another user suggested to *wait for the cog to be "active" (white and not grey), then the button to save changes will appear after changing something*.
- **Swagger for Server API is Missing**: A user asked for a **Swagger definition to interact with the LM Studio Server API**, because *the documentation is quite vague*.
   - Another user responded that you can *just use the openai API endpoints supported by lms*.


  

---


### **LM Studio â–· #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1381712451061547129)** (127 messagesðŸ”¥ðŸ”¥): 

> `DGX Spark limitations, Memory bandwidth bottlenecks, Distributed computing for models in homelab, ROCm/HIP PyTorch on Windows, Speculative decoding on different GPUs` 


- **DGX Spark Faces Bandwidth Blues**: Members debated whether the **DGX Spark**'s memory bandwidth would limit its LLM performance, similar to the **Strix Halo**, despite having potentially better compute power.
   - One member argued that *it's not all about memory bandwidth*, using the analogy of a machine with high RAM but a slow CPU, while others emphasized that memory bandwidth is often the bottleneck for dense LLMs.
- **Homelab Distributed Computing Dilemma**: Someone inquired about using distributed computing for LLMs in a homelab setup, similar to **Distributed Llama**, but it was deemed *not generally a good idea*.
   - However, **EXA** or **llama-mpi** were mentioned as potential alternatives, but the general sentiment leaned towards focusing on individual machine performance rather than distributed setups.
- **ROCm Shines on Windows**: A user reported successfully running **ROCm/HIP PyTorch** preview on Windows, referring to it as *an abomination* that works surprisingly well.
   - The user noted that while some modules may not fully support this setup and optimizations are not remembered across relaunches, the overall experience was positive compared to previous attempts with **ZLUDA**.
- **Speculative Decoding Hardware Hacking**: Members discussed speculative decoding and the possibility of offloading the draft model to a different GPU or CPU, such as leveraging an **RX 9070 XT** with a **GTX 1060**.
   - It was clarified that offloading to the CPU is a different method than changing runtimes, and while offloading to another GPU on the same runtime should be technically possible, it's complicated by each GPU typically having its own runtime.
- **Digits vs Dual GPUs: Bandwidth Bottleneck**: The question of how **Nvidia's Project Digits** would compare to a **5090 + 3090** setup for AI tasks within 56GB VRAM was posed, with the consensus leaning towards **Digits** being slower.
   - LLM inference is typically bound by memory bandwidth, and **Digits** is expected to have less bandwidth than an **M3 Max**, which is already slower than dual **3090s** for VRAM-fitting tasks.


  

---


### **aider (Paul Gauthier) â–· #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1381709535990779944)** (148 messagesðŸ”¥ðŸ”¥): 

> `Gemini 2.5 Pro vs Claude Opus, DeepSeek R1 speed, Aider uninstall, OpenAI's O3 Pricing, Kingfall` 


- **Gemini 2.5 Pro Lags in Library Updates**: Members noted that **Gemini 2.5 Pro** struggles with understanding that new versions of libraries exist and doesn't follow instructions well, even when given explicit rules.
   - In contrast, **Claude Opus and Sonnet** are much better at understanding these nuances; a member quantified the effectiveness of cursor rules as 80%+ with Claude Sonnet, 95%+ with Opus, and only 50 with 2.5 Pro.
- **DeepSeek R1 Aider Benchmarks Slow but Promising**: Aider benchmarks show **DeepSeek R1 (0528)** could be fairly good if the long case times (reportedly due to low resources/busy API) are addressed.
   - One user suggested that a **7x speedup** might be possible with more resources; another user noted it has a much lower tendency to sperg-loop COT (Chain of Thought) than previous iterations.
- **Uninstalling Aider Chat**: A user asked for the correct way to uninstall Aider from a Linux machine after using `pip install aider-install && aider-install`.
   - The suggested solution was to use `pip uninstall aider-chat`, though this leaves behind the binary, `aider`, which can be manually deleted along with indexes and cache files.
- **OpenAI O3 Price Drops but KYC still needed**: **OpenAI** announced **O3** pricing at **$2 input, $8 output**, an 80% price drop, however, using it via OpenRouter still requires bringing your own key and KYC (Know Your Customer) verification.
   - Some users expressed disappointment over the KYC requirement, while one pondered whether O3 has become a *mini model* and suggested re-benching it.
- **Kingfall model edges out O3 Pro**: A user shared an image comparing **Kingfall (auto thinking)** versus **0605 (32k)**, showing that it performed better.
   - This shows that it outperformed a recent model in at least one coding benchmark.


  

---


### **aider (Paul Gauthier) â–· #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1381748863915786260)** (16 messagesðŸ”¥): 

> `aider MCP server, Cloning a large repo, Gemini-2.5-03-25 and Rust, Ollama model unloading, fireworks' deepseek-r1-0528` 


- **Gemini's Goodness after Good Rust-Based Prompts**: A user found that **Gemini-2.5-03-25** exhibited more functional and efficient programming style after being primed with advanced programming concepts and discussing appropriate data structures in **Rust**.
   - The user achieved this by loading conversation history from `.aider.history.md` into a new file `.aider.coder` and specifying it with `aider --llm-history-file .aider.coder.new --restore-chat-history`.
- **DeepSeek Gets Cut Off Mid-Think**: A user reported issues with **fireworks' deepseek-r1-0528** getting cut off mid-thinking due to token limits.
   - A solution was provided to set the model settings in `~/.aider.model.settings.yml` with a suggested configuration that includes setting `max_tokens: 160000`.
- **Aider as MCP server with external tools**: A user asked about using **aider** as an **MCP server** on external tools like **roo** and **Cline**.
   - No specific solutions were provided in the context.
- **Context Management is Key**: A user found the explicit management of context and intention with **Aider** leads to less rewriting compared to tools like **Cursor** and **Claude Code**.
   - This user pointed out that the terminal output is more productively sparse than other similar tools.


  

---


### **aider (Paul Gauthier) â–· #[links](https://discord.com/channels/1131200896827654144/1268910919057149974/1382046817478512681)** (1 messages): 

> `agentic embedded coding workflow, PlatformIO, Cline, FREE DeepSeek OpenRouter API, microcontrollers` 


- **Agentic Embedded Coding Workflow**: A member is taking steps towards an **agentic embedded coding workflow** using **PlatformIO**, **Cline**, and a **FREE DeepSeek OpenRouter API**.
   - He also shared a [blog post with video](https://www.circusscientist.com/2025/06/10/agentic-embedded-development/) which walks through the level of difficulty of *blinking an LED*.
- **Microcontrollers & IOT on the Horizon**: A member inquired about others programming **microcontrollers** or **IOT**. 
   - He shared a [blog post with video](https://www.circusscientist.com/2025/06/10/agentic-embedded-development/) about **agentic embedded development** using PlatformIO, Cline, and a FREE DeepSeek OpenRouter API.


  

---


### **Nous Research AI â–· #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1381902225319661608)** (133 messagesðŸ”¥ðŸ”¥): 

> `Magistral Benchmarking, GRPO Modifications, Claude's Dynamic Token Limit, Control Tokens, ProRL Effects on Larger Models` 


- **Mistral's Magistral Model Benchmarking Blues**: Mistral released **Magistral**, benchmarking against the old **R1-0125** instead of the new **R1-0528**, including the release of a [paper](https://mistral.ai/static/research/magistral.pdf) and a distilled version on [HuggingFace](https://huggingface.co/mistralai/Magistral-Small-2506).
   - The model exhibits a **looping and token spamming problem**, despite the addition of a length penalty to GRPO.
- **Anthropic's Claude Pioneers Dynamic Token Limits**: Members pointed out that **Anthropic's Claude** stands out for its unique dynamic token limit implementation for **Chain of Thought (CoT)**, a problem not yet solved by many others.
   - Nous is working on **Hermes 4** to feature user-controlled token limits by teaching the model word, character, and sentence limits during SFT and token limits during RL.
- **Investigating Control Tokens for Model Reasoning**: The discussion explored the potential of injecting control tokens, such as **progress markers (00%, 25%, 50%, 75%)**, during reasoning to help models dynamically adjust and compress outputs.
   - The goal is to improve the model's ability to **split reasoning into search-consolidate-answer phases**.
- **Decoding the ProRL Paper**: The discussion examined the **ProRL (Prolonged RL) paper**, with some members finding its conclusions unconvincing, especially regarding its applicability to larger models, while noting issues with entropy collapse and reduced sample diversity for shorter **CoT**.
   - The async overlapped training technique used by Mistral, similar to the PipelineRL approach, was also highlighted ([tweet](https://x.com/dmayhem93/status/1916189027881062888), [paper](https://arxiv.org/abs/2505.24864)).
- **New Mistral Models need Prompt Engineering**: Users discussed the new Mistral models where reasoning mode is activated by using prompt engineering, not unlike **deep hermes**.
   - It seems to be prompt driven with "respond with thinking" and "respond without thinking" and is still experimental.


  

---


### **Nous Research AI â–· #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1381723726759989359)** (4 messages): 

> `KV Compression, GRPO for TTS LLMs` 


- **New KV Compression Method Surfaces**: A new method for **KV compression** was shared, detailed in [this paper](https://arxiv.org/pdf/2505.23416) and [this tweet](https://x.com/qx_dong/status/1932268949238067482?s=46).
- **GRPO Enhances TTS LLMs**: It was mentioned that **GRPO** (Generalized Reweighted Policy Optimization) can be used to improve **TTS LLMs** (Text-to-Speech Large Language Models), detailed in [this paper](https://arxiv.org/abs/2502.04128).


  

---


### **Nous Research AI â–· #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1381925016047845407)** (8 messagesðŸ”¥): 

> `AI Heart Monitoring, Frutiger Aero, Biological Computers` 


- **Arxiv Papers Appear**: Two members shared links to two **Arxiv papers** ([2506.06607](https://arxiv.org/abs/2506.06607) and [2502.02260](https://arxiv.org/abs/2502.02260)).
   - One member also shared a link to the full video after a snippet was posted ([https://www.youtube.com/watch?v=zuZ2zaotrJs](https://www.youtube.com/watch?v=zuZ2zaotrJs)).
- **AI Heart Monitoring: Life Saving or Life Threatening?**: One member quipped that instead of focusing on **AI heart monitoring** which *would save/extend lives*, the focus should be on *bringing frutiger aero back and implementing ai slop imessage backgrounds*.
   - It is unclear if the commenter was joking, serious, or being sarcastic.
- **Humans as Biological Computers?**: One member expressed that *reducing humans to biological computers is clown techbro behavior* after watching a snippet of a video.
   - They further added that humans are not the same as *bugs n creatures of the expansive earth & depths of the deep sea* despite sharing *99+% similar dna*.


  

---


### **Nous Research AI â–· #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1381723726759989359)** (4 messages): 

> `KV Compression, GRPO for TTS LLMs` 


- **New KV Compression Method**: A new **KV compression** method was announced [here](https://arxiv.org/pdf/2505.23416).
   - More information about the new method can be found [on X](https://x.com/qx_dong/status/1932268949238067482?s=46).
- **GRPO Enhances TTS LLMs**: **GRPO** can be utilized to improve **TTS LLMs**, according to a new [paper](https://arxiv.org/abs/2502.04128).


  

---


### **Yannick Kilcher â–· #[general](https://discord.com/channels/714501525455634453/986699377257119794/1381720403864584263)** (53 messagesðŸ”¥): 

> `Diffusion models, Hardware failure prediction, Reservoir Computing, Tolman Eichenbaum Machine` 


- ****Diffusion Models:** Mind-Blowing Structure From Noise**: Members discussed the counterintuitive ability of **diffusion models** to generate structure from noise, calling it *the most mindblowing thing ever* and a *directed hallucination model*.
   - One member linked this to broader themes of order from chaos, referencing a [YouTube video](https://youtu.be/10cVVHKCRWw?si=xNf-PpR9N-GTvP-X) and [paper](https://arxiv.org/abs/1412.1875) on **nonequilibrium thermodynamics** and the spontaneous emergence of life.
- ****Hardware Failure Prediction:** Beyond DL Solutions**: Several members discussed approaches to **hardware failure prediction**, with a key insight being that traditional methods like **Gaussian Processes** or **boosted trees** often outperform deep learning for time series analysis.
   - One member highlighted the narrow scope and high-stakes nature of this field, emphasizing the need for guaranteed failure detection rather than probabilistic correctness due to insurance requirements in industrial settings.
- ****Reservoir Computing:** Obfuscated State Spaces?**: A member described **Reservoir Computing** as *mumbo jumbo* that obfuscates its core mechanism: linear regression on a fixed Ordinary Differential Equation (ODE).
   - They argued that modern architectures like **State Space Models (SSMs)** are more expressive, powerful, and efficient due to their ability to parallelize and incorporate nonlinear dynamics and linked to a [paper](https://arxiv.org/abs/2506.02475) about current SOTA.
- ****Tolman Eichenbaum Machine:** Simplified Implementation**: A member announced having trained a simplified version of the **Tolman Eichenbaum Machine**, condensing the essence of the paper into a handful of functions.
   - They described it as basically a **Kalman filter** that factorizes state into independent location (g) and sensory appearance (x) components, then saves updated (g, x) pairs into episodic memory, offering to answer any questions about it.


  

---


### **Yannick Kilcher â–· #[paper-discussion](https://discord.com/channels/714501525455634453/1045297868136779846/1381785770859167804)** (13 messagesðŸ”¥): 

> `Variational Bayesian approach, World modeling and decision making, Introduction to complex subject, BioML people in berlin` 


- **Variational Bayesian Approach Introduced**: A Variational Bayesian approach to simultaneous **world modeling** and **decision making** was introduced via the [UAB medicine link](https://www.uab.edu/medicine/cinl/images/KFriston_FreeEnergy_BrainTheory.pdf).
- **Introductory Paper for the Math**: An introductory paper for the math was suggested via an [Arxiv link](https://arxiv.org/abs/2406.07726).
- **Introduction to a Complex Subject**: One member said that *the best introduction to a complex subject is your second introduction to a complex subject.*
- **BioML People in Berlin to Present**: Some **bioML people** in **Berlin** are going to come to YK discord to present in the future.


  

---


### **Yannick Kilcher â–· #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1381758743561240716)** (29 messagesðŸ”¥): 

> `Mistral AI, Magistral, Open Source, GPT-4` 


- **Mistral Releases Magistral Reasoning Model**: **Mistral AI** announced [Magistral](https://mistral.ai/news/magistral), its first reasoning model, which excels in domain-specific, transparent, and multilingual reasoning.
- **Open Source Debate Erupts Over Magistral**: A user noted that **Magistral Small** is open-weight under the Apache 2.0 license, but expressed disappointment that **Mistral** is not open sourcing its larger models, lamenting *they became Google level of open weighting*.
   - Another user quoted the paper stating they open-sourced **Magistral Small** which includes cold-start data from **Magistral Medium** and will not be open-sourcing the Medium model.
- **Community Skeptical of Magistral's Open Source Claims**: Some community members are calling out that [Magistral represents a significant contribution by Mistral AI to the open source community](https://x.com/kimmonismus/status/1932424474164424786) despite not open sourcing all their models.
- **OpenAI Teases 'Unexpected' Announcement**: Users pointed out [Sam Altman teased on Twitter](https://x.com/sama/status/1932573231199707168) that **OpenAI** has an *unexpected thing* coming.
   - Users speculate that *its a diffusion model*.


  

---


### **Notebook LM â–· #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1381730009617535080)** (16 messagesðŸ”¥): 

> `NotebookLM podcast intro, Google Chat integration, Drive file access errors, Video feature release date, Control over Google Workspace document access` 


- **NotebookLM's Intro Stuns Game Designer**: A user was surprised and impressed by the quality of the intro generated by NotebookLM for their tabletop RPG, **The Gemini System**, using the podcast feature.
   - The user found NotebookLM's ability to analyze and provide **audio deep dives** incredibly helpful for translating mechanics and enhancing their design and writing process.
- **Google Chat Convos Coming To NotebookLM?**: A user inquired about the possibility of connecting **Gmail and Google Chat conversations** with NotebookLM and whether there are plans for this feature in the near future.
   - No official response was given, but the query was directed towards any Google employees present in the server.
- **Troubleshooting Drive File Download Errors**: A user encountered an error when trying to access a Drive file and sought help in resolving it, showing the screenshot [here](https://cdn.discordapp.com/attachments/1124403655819415592/1382004322572959844/PXL_20250610_143128479.MP.jpg).
   - Another user clarified that the error typically indicates that the **file owner has disabled copy/download permissions** for the Drive file.
- **Video Feature on the Horizon?**: Users were curious about the expected release date for the **video feature** in NotebookLM.
   - No specific timeline or official announcement was provided regarding the availability of the video feature.
- **Document Access Control in NotebookLM**: A user questioned whether NotebookLM could read content from **Google Workspace documents** and if it was possible to specify what the AI could access.
   - The discussion focused on the ability to control NotebookLM's access to specific documents and content within Google Workspace.


  

---


### **Notebook LM â–· #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1381717074958749848)** (57 messagesðŸ”¥ðŸ”¥): 

> `Time tracking apps, Iceland workshop feedback, Geographic access issues, Audio overview issues, Sharing notebooks issues` 


- **Time Tracking Quest yields Quest-ions**: A user is looking for a simple time tracker app with a **start/stop** button and **streak tracking** for studying and simple projects, finding existing options like **Toggl** and **ClickUp** too complicated.
   - The user mentioned considering coding their own time tracker app.
- **Icelandic Teachers Love NotebookLM but Three Get Blocked**: A user ran a **NotebookLM** workshop for 50 teachers in Iceland, receiving amazing feedback, but 3 teachers using private Gmail accounts encountered a *"You do not have access to this service"* error.
   - It was suggested that geographic restrictions or incomplete age verification could be the cause, with a user in the UK reporting similar issues with **Brave** browser, resolved by switching to **Firefox**.
- **Is NotebookLM Glitching on Calculations?**: A user highlighted a calculation issue where summing a list of numbers resulted in an incorrect total, displaying an extra **100**, prompting comparisons to similar issues with **Apple** calculations.
   - Another user confirmed that the calculation was correct on **Android**.
- **Audio Overview Audio Overload?**: Users are reporting issues with the *"could not load audio overview"* error when trying to listen to podcasts on the Android app, but the web version works.
   - One user noted the audio quality changing in the second minute.
- **Sharing Notebooks Sharing Headaches**: A user reported issues with sharing notebooks, where added emails and *"Anyone with link"* settings revert to restricted after sending.
   - This seems to be a persistent issue for them.


  

---


### **GPU MODE â–· #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1381906836789395456)** (4 messages): 

> `deepwiki, GLSL, Vulkano, GPU grouping, clustering algorithm` 


- **Deepwiki link summarizes GitHub repos**: A member asked about a tool that summarizes GitHub repos and allows chatting and structure viewing from just a GitHub link.
   - Another member suggested [deepwiki](https://deepwiki.com) as a possible solution.
- **Rustacean seeks help with GPU algorithm**: One member is developing a parallel **GPU grouping/clustering algorithm** in **GLSL** and **Vulkano** using **Rust**.
   - The developer is looking for collaborators, emphasizing that the project uses **Vulkano**, compatible even with Macs.


  

---


### **GPU MODE â–· #[triton](https://discord.com/channels/1189498204333543425/1189607595451895918/1381883374494552084)** (6 messages): 

> `FP16 support, Triton.Config num_warps control, Triton shared memory limits, LeetGPU challenges with Triton precision issues, Triton ROCm libdevice.round error` 


- **FP16 Precision Functions Spark Interest**: A user inquired about Triton's support for **fp16 exp and sqrt functions**, noting their availability in [CUDA](https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH____HALF2__FUNCTIONS.html#group__cuda__math____half2__functions_1gacb711f53daef957df8d3c4a673ea60ea).
- **`num_warps` Configuration Parameter Explained**: A user sought clarification on the role of `num_warps` in `Triton.Config` and guidance on when to adjust it, seeking insight into its **impact on performance and resource utilization**.
- **Shared Memory Limits Examined**: A question arose regarding whether Triton adheres to the same shared memory allocation limits as CUDA when performing `tl.load` operations.
   - Specifically, the user wondered if `tl.load` places tensors in a different memory space to **avoid exceeding shared memory limits**.
- **LeetGPU Matmul Kernel Faces Precision Problems**: A user encountered precision issues in a matmul kernel written for LeetGPU challenges, which works in interpret mode but fails on GPU despite using float32, sharing their [matmul.py file](https://cdn.discordapp.com/attachments/1189607595451895918/1381932221161668659/matmul.py?ex=6849f8ec&is=6848a76c&hm=dd62e45721fd43e5b7c3637734242e4535962651f5d15dbfce1ce183c7a5fa99&).
   - They asked about the **cause of the failure** and whether their 2D grid implementation already incorporates swizzling.
- **Triton ROCm's Odd `libdevice.round` behavior**: A user reported that while `libdevice.round` is defined in Triton ROCm, it throws an error when used in a kernel.
   - Another user noted that this issue has been reported on [GitHub](https://github.com/triton-lang/triton/issues/7135).


  

---


### **GPU MODE â–· #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1382122979915337738)** (3 messages): 

> `CUPTI, Performance Counters, nvbench` 


- **CUPTI's Metrics Captured with Disabled Performance Counters?**: A member asked if **CUPTI** can be used to effectively capture metrics in a machine that doesn't have **performance counters** enabled.
   - Another member responded they don't think so, adding that they had to disable **CUPTI** (in CMake) for **nvbench** to run benchmarks (using CUPTI metrics) on HPC Clusters without erroring out.
- **CUPTI API a Pain**: A member said they managed to get some simple metrics to report which surprised them.
   - They added that it's a pain trying to use the **CUPTI API** so they didn't bother looking for more complicated metrics.


  

---


### **GPU MODE â–· #[torch](https://discord.com/channels/1189498204333543425/1189607750876008468/1381766319531360318)** (11 messagesðŸ”¥): 

> `functorch, FSDP2, torch.compile with custom operators` 


- **Functorch Usage Flashback**: A user suggested using `functorch` to achieve functional modules, providing example code using `make_functional`.
   - Another user admitted to forgetting about `functorch`, and wondered if it had been integrated in [this issue](https://github.com/pytorch/pytorch/issues/155616).
- **TorchTitan flattens `dp_shard` and `cp`**: In `torchtitan`, `dp_shard` and `cp` are flattened into a `dp_shard_cp` mesh dimension for FSDP2 mesh dimension, which may introduce unnecessary communication overhead.
   - One user provided a link to their small profiling analysis of `dp_shard` and `cp` in [this comment](https://github.com/huggingface/transformers/issues/35983#issuecomment-2957438151).
- **nn.Linear Single-Line Code**: One user suggested simplifying the code to create a layer in a single line like so: `layer = nn.Linear(weight=param)`.
   - The goal is to make the code more readable and reduce boilerplate.
- **Guidance on custom operators and torch.compile is sought**: A user seeks advice on custom operators and `torch.compile`, specifically regarding shape checking and best practices.
   - They opened an issue on GitHub [here](https://github.com/pytorch/pytorch/issues/155616) to initiate a discussion on the topic.


  

---


### **GPU MODE â–· #[jobs](https://discord.com/channels/1189498204333543425/1190208177829068860/1382022062994096281)** (1 messages): 

> `NeoSpace, GB200, CUDA, Brazil` 


- **NeoSpace in Brazil Needs CUDA Experts**: An AI company named **NeoSpace** based in **Brazil** is hiring professionals with expertise in **GPU optimization** using **CUDA**.
   - They're training models using **GB200 GPUs** and prefer on-site work; interested candidates should contact [jh@neospace.ai](mailto:jh@neospace.ai) with resume and the subject 'Neospace CUDA position'.
- **NeoSpace Actively Recruits CUDA Specialists**: Brazilian AI firm **NeoSpace** seeks **CUDA** experts for **GPU optimization** roles, utilizing **GB200 GPUs** in their model training.
   - On-site positions are preferred; applications should be directed to [jh@neospace.ai](mailto:jh@neospace.ai) with the subject line 'Neospace CUDA position', including a resume.


  

---


### **GPU MODE â–· #[irl-meetup](https://discord.com/channels/1189498204333543425/1218444432588800010/)** (1 messages): 

ossmar: Does someone here is attending to the ACM PODC 2025?
  

---


### **GPU MODE â–· #[rocm](https://discord.com/channels/1189498204333543425/1233704710389764236/1381734547351142492)** (9 messagesðŸ”¥): 

> `SQTT traces, Radeon GPU Analyzer (RGA), rocprofv2, CUDA graphs, Memory access fault` 


- **Troubleshooting CUDA Graphs Memory Faults**: A user encountered a `Memory access fault by GPU node-2` error when using **CUDA graphs** with the `rocm/pytorch:rocm6.4_ubuntu22.04_py3.10_pytorch_release_2.6.0` image and **torch 2.8.0.dev20250609+rocm6.4**.
   - The user inquired if this was a known issue, noting they hadn't experienced it with previous versions.
- **Profiling with Radeon GPU Analyzer**: A user detailed the steps to collect **SQTT traces** for analysis in **Radeon GPU Analyzer (RGA)** using `rocprofv2`, including creating a configuration file and running `rocprofv2` with SQTT capture enabled.
   - They noted that the correct **DISPATCH_RANGE** can be determined by first running `rocprofv2 --kernel-trace`.
- **Seeking Per-Instruction Timings with RGA**: A user reported partial success with **RGA**, but expressed uncertainty whether **RGA** shows per-instruction timings, and they encountered issues when opening a trace of a **Triton kernel**.
   - They planned to update to **ROCm 6.4.1** to try **rocprof compute viewer** or **rocprof3's att**.
- **Confusion between RGA and RGP**: A user suggested trying [Radeon GPU Profiler (RGP)](https://gpuopen.com/rgp/) but then stated it requires **.rgp files** which are incompatible with **HIP programs** from Linux.
   - Another user acknowledged the suggestion but noted that **RGP** requires **.rgp files**, which are not compatible with profiling **HIP programs** from Linux.


  

---


### **GPU MODE â–· #[liger-kernel](https://discord.com/channels/1189498204333543425/1275130785933951039/1382173276981297293)** (1 messages): 

> `Liger Collective Library, ByteDance Triton-distributed` 


- **Liger Considers Collectives Library**: A member inquired about adding a collectives library to **Liger**, similar to the [ByteDance Triton-distributed library](https://github.com/ByteDance-Seed/Triton-distributed).
- **More on Collectives Libraries**: The collectives library would be similar to [another collectives library](https://github.com/ByteDance-Seed/Triton-distributed).


  

---


### **GPU MODE â–· #[self-promotion](https://discord.com/channels/1189498204333543425/1288557096404516945/1382006017881276446)** (3 messages): 

> `Mojo Programmers on BlueSky, Modular raises funding` 


- **Mojo Programmers assemble on BlueSky!**: A member is compiling a list of **Mojo programmers** on **BlueSky** to create a starter pack and is [looking for volunteers](https://go.bsky.app/8FNt9SN).
   - The member believes there are *dozens of them*.
- **Fundraising news**: Members are excited for new funding and the possibilities that come with it.
   - Many are posting *waiting for money to come in*


  

---


### **GPU MODE â–· #[ðŸ¿](https://discord.com/channels/1189498204333543425/1298372518293274644/1381727631283716157)** (2 messages): 

> `Dataset Generation, Diverse Datasets, Augmented Datasets` 


- **New Dataset Generators emerge!**: A member shared a new dataset generator project, noting they lost count of how many times they've had to write new ones and that it was time to create one.
   - They added that the [getting started examples](https://cdn.discordapp.com/attachments/1298372518293274644/1381727631002833106/image.png?ex=6849e322&is=684891a2&hm=7f887fe9395d993cc48c35251ea3250a84985447) work very well and it supports creation of **diverse**, from scratch or **augmented datasets** in a few lines of code.
- **Future dataset plans**: The member still needs to clean up the process for creating more complex stuff.


  

---


### **GPU MODE â–· #[reasoning-gym](https://discord.com/channels/1189498204333543425/1316377974672588850/1382008617435267082)** (1 messages): 

> `RL, Reasoning Training, Magistral Paper` 


- **Magistral Paper Explores RL and Reasoning Training**: The new [Magistral paper](https://mistral.ai/static/research/magistral.pdf) delves into valuable insights on **Reinforcement Learning (RL)** and **reasoning training** methodologies.
- **Reasoning Training Boosted by Magistral**: The paper highlights how **reasoning training** techniques contribute to enhanced model performance, particularly in complex tasks and decision-making scenarios, underscoring the significance of the [Magistral paper's findings](https://mistral.ai/static/research/magistral.pdf).


  

---


### **GPU MODE â–· #[general](https://discord.com/channels/1189498204333543425/1343002580531417211/1381963211162517505)** (10 messagesðŸ”¥): 

> `Hackathons, Benchmarking, CUDA events` 


- ****Hackathon Newbie Arrives****: A new member arrived asking about active hackathons, mentioning they came via a [Datamonsters AMD challenge](https://www.datamonsters.com/amd-developer-challenge-2025#wf-form-AMD-Email-Form) that has passed.
   - A member responded, there aren't any current "running" hackathons with prizes, but the **AMD** and **PMPP** problem sets are open for submissions.
- ****GPU Mode Benchmarking Methodology Exposed****: A member inquired about the exact benchmarking methodology for the leaderboards, particularly for tasks like **matmul**.
   - Another member provided a link to the [gpu-mode/discord-cluster-manager](https://github.com/gpu-mode/discord-cluster-manager/blob/main/examples/eval.py) **eval.py** script, along with a link to tolerances defined in [gpu-mode/reference-kernels](https://github.com/gpu-mode/reference-kernels).
- ****CUDA Events Improve Benchmarking Accuracy****: A member expressed a preference for benchmarking using **CUDA events** for better accuracy, referencing **triton's do_bench** function.
   - Another member acknowledged the limitation, noting they would likely address it before the next big competition, and clarified that the times displayed are the **min/max times**.


  

---


### **GPU MODE â–· #[submissions](https://discord.com/channels/1189498204333543425/1343002583001726986/1381832453643174018)** (2 messages): 

> `Chinese problem-solving approach, New Bilibili article` 


- **Chinese problem-solving approach unveiled on Bilibili**: A member highlighted a problem-solving approach described in a [Bilibili article](https://www.bilibili.com/read/cv41954307).
   - The post describes the approach in Chinese, offering insights into its application.
- **Bilibili Article Sparks Discussion**: The shared [Bilibili link](https://www.bilibili.com/read/cv41954307) prompted a discussion about problem-solving methodologies.
   - Community members showed interest in understanding the nuances of this specific approach.


  

---


### **GPU MODE â–· #[factorio-learning-env](https://discord.com/channels/1189498204333543425/1354169122107293786/1382016698596720681)** (1 messages): 

> `Roadmap` 


- **Roadmap Request Re-Pinged**: A member re-pinged their message to inquire about a potential roadmap for a project or feature.
   - They were unsure if a roadmap already existed and were seeking awareness on the matter.
- **Another Roadmap Request**: Another member also inquired about a roadmap.
   - They were also unsure if a roadmap already existed.


  

---


### **GPU MODE â–· #[cutlass](https://discord.com/channels/1189498204333543425/1362196854460383353/1381724934924927148)** (2 messages): 

> `CuTE docs, Cutlass, Triton` 


- **CuTE Docs recommended for Cutlass learners**: A member asked if **CuTE docs** are the best place to get started on learning **Cutlass** for someone with a **Triton** background.
   - Another member suggests using the notebooks provided in the examples part and using the docs as a reference for what's really going on under the hood.
- **Cutlass and Triton Backgrounds**: The user with a **Triton** background sought advice on learning **Cutlass** effectively.
   - The community recommends leveraging example notebooks and the documentation for understanding the underlying mechanics.


  

---


### **GPU MODE â–· #[mojo](https://discord.com/channels/1189498204333543425/1367972893400760371/1382070117566906489)** (1 messages): 

> `Modular + AMD, Python Interop` 


- **Modular and AMD Join Forces**: Modular announced today a new partnership with AMD to *unleash AI performance on AMD GPUs*, as seen in their [blog post](https://www.modular.com/blog/modular-x-amd-unleashing-ai-performance-on-amd-gpus).
- **Python Plays Well with Mojo**: A demo showcasing **Python interoperability** with Mojo was shared, specifically at the [843-second mark of this YouTube video](https://www.youtube.com/watch?v=TrBXHPGRlnQ&t=843s).
- **Mojo has official Documentation**: Documentation for Mojo has been released including full details on its [Python integration](https://docs.modular.com/mojo/manual/python/).


  

---


### **Latent Space â–· #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1381762552970608690)** (56 messagesðŸ”¥ðŸ”¥): 

> `Fireworks AI RFT Beta, OpenAI o3 Pricing, Mistral's Magistral Model, Meta's Potential Scale AI Stake, DeepSeek Model Narrative` 


- ****RFT Beta** Launches on Fireworks AI**: Lin Qiao announced the beta launch of **Reinforcement Fine-Tuning (RFT)** on [Fireworks AI](https://xcancel.com/lqiao/status/1932103761872474450), allowing users to train expert open models with quality comparable to closed frontier models like **GPT-4o mini** and **Gemini flash**.
   - The service is designed for rapid iteration with a web IDE, an open-source reward-kit, support for SOTA models, and is self-serve and free for the next two weeks for models up to **10B** parameters.
- ****o3 Token** Costs Shared by OpenAI**: Gabriel Chua noted a potential cost of **$2 per 1M input tokens** for OpenAI o3, referencing an [OpenAI Developers tweet](https://xcancel.com/gabrielchua_/status/1932251026943533156) offering **200** developers free API credits worth **1M** input tokens.
   - The discussion included links to [Sam Altman's tweet](https://x.com/sama/status/1932434606558462459) and further insights on [scaling01](https://x.com/scaling01/status/1932530425609552334?s=46).
- **Mistral Releases **Magistral** Reasoning Model**: Mistral AI announced the release of **Magistral**, their new reasoning model for domain-specific, transparent, and multilingual reasoning, available in two variants: open-source **Magistral Small (24B parameters)** on [Hugging Face](https://xcancel.com/mistralai/status/1932441507262259564?s=46), and enterprise **Magistral Medium** via chat.mistral.ai or API.
   - Reactions were positive, praising the model's release and name, with some users providing instructions for local deployment and noting its availability on platforms like [OpenRouter](https://x.com/slashML/status/1932444552352203252).
- ****Meta Eyes Scale AI's** Alex Wang for Top Role**: Meta Platforms might acquire a **49% stake** in Scale AI for nearly **$15 billion**, potentially bringing Scale AI's CEO, Alex Wang, into a senior position at Meta ([Source](https://xcancel.com/amir/status/1932455464203329651?s=46&t=b7l37rB6wtbyAh6ah1NpZQ)).
   - This move could significantly impact Meta's AI strategy and leadership.
- **Windsurf Launches **'Plan Mode'** Feature**: Kevin Hou introduced Windsurf's new **'Plan Mode'** feature, enabling the AI agent to perform complex tasks by creating and maintaining a planning document ([Source](https://xcancel.com/kevinhou22/status/1932516093333266538?s=46)).
   - Users can toggle **'Plan Mode'** on to allow Windsurf to manage notes, task lists, and goals, enhancing its ability to handle longer, more involved changes, available for free on [Windsurf.com](https://xcancel.com/kevinhou22/status/1932516093333266538?s=46).


  

---


### **Modular (Mojo ðŸ”¥) â–· #[announcements](https://discord.com/channels/1087530497313357884/1098765954302873621/1382040678858756186)** (1 messages): 

> `Modular Livestream, Compute Portability` 


- **Modular Kicks off Livestream**: Modular announced that their livestream is kicking off in 5 minutes on the [Modular website](https://www.modular.com/).
   - The livestream can also be accessed on [LinkedIn](https://www.linkedin.com/events/thefutureofcomputeportability7335404851276734464/theater/).
- **Compute Portability Talk**: The Modular livestream is focused on the **future of compute portability**.
   - The event promises insights into the latest advancements and discussions in **compute portability**.


  

---


### **Modular (Mojo ðŸ”¥) â–· #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1381719701771649115)** (52 messagesðŸ”¥): 

> `Mojo parameterization limits, Zig vs Mojo, Python Syntax similar to Go, Mojo-MAX Platform relationship, Double copy explaination` 


- **Community Showcases Parameterization Exploits**: Community member presentations and glimpses into the **standard library code** have sparked questions about the boundaries of parameterization in Mojo, particularly regarding its use for **comptime purposes**.
   - One member expressed concern that the exploitation of parameterization for **comptime purposes** seems to create code they *just do not want to read in a lot of cases*.
- **Mojo Meta-Programming > Rust Macro**: One member argued that reading **meta-programming** in Mojo is *100000000000% better than reading macro code in rust*, while acknowledging that Mojo can't do everything Rust can yet.
   - Another member thinks it's the combination of **Zig-esque comptime** in **Go-esque square-bracket-generics syntax** that makes it difficult to read.
- **Mojo copies Python Generics**: A member stated that Mojo's syntax for generics is the same as **Python's**, which inspired a discussion to whether **Python's generic syntax came from Go**.
   - Ultimately, the parties agreed that **Go 1.18** introduced generics on 3/15/22, and **PEP 695** introduced the *new* Python syntax on 6/22.
- **Clarify Mojo-MAX Platform relationship**: A member asked about the relationship between the **Mojo** and **MAX platform**, specifically the ability to use MAX kernels such as **matmul** in Mojo code and kernels.
   - A Modular employee suggested that the member post this question in the [Modular forum](https://forum.modular.com/) to enhance its discoverability.
- **Double copy explained**: A member questioned why assigning a `ref` variable to another triggers `__copyinit__` twice and why `ref` gets an extra `__moveinit__` step while `var` doesn't.
   - Another member clarified that the double copy happens because there's effectively a **tmp variable** inserted to handle the assignment and a [compiler explorer link](https://mojo.compiler-explorer.com/z/MW1vjxbn6) provided.


  

---


### **MCP (Glama) â–· #[general](https://discord.com/channels/1312302100125843476/1312302100125843479/1381774878838886470)** (41 messagesðŸ”¥): 

> `MCP Server Selection, Building n8n for MCP, FastMcp Dependencies, Mature MCP SDK, MCP file downloads` 


- **5ire forces MCP tool adoption**: A user noted that the **5ire platform** requires adopting all tools from an MCP server, offering no option to pick and choose individual components.
   - This all-or-nothing approach means developers must integrate entire suites of functionalities rather than selecting specific tools they need.
- **n8n-like Chatbot Integration Dream**: A member expressed interest in building a tool like **n8n**, but based entirely on chats and **MCPs** to automate workflows based on chat interactions.
   - Another member proposed a workflow where emails from a specific source can be routed into a Slack channel, highlighting the potential of such a system.
- **fastmcp needs dependency declaration**: A member struggled with **fastmcp**, noting that it needs the dependencies declaring dependencies because it creates an environment and uses the dependencies to do that.
   - They shared a command line to execute the **MCP server** and then they updated the args in Claude desktop so that uv knows which venv to use.
- **Github and Amazon have official python SDKs**: A member asked for the most mature official SDK/repo out there for MCP server development.
   - Other member mentioned [Github](https://github.com/mark3labs/mcp-go) and Amazon have SDKs for the same.
- **File downloads challenges in MCP**: A member asked for the best way to handle file downloads via MCP, as sending entire files as base64 strings requires loading the entire file into memory which isn't ideal.
   - Another member shared an interesting [MCP server implementation](https://github.com/hemanth/paws-on-mcp) attempting to implement the whole protocol, and you can connect to remote servers using **OpenAI assistants API** or **Anthropic**.


  

---


### **MCP (Glama) â–· #[showcase](https://discord.com/channels/1312302100125843476/1315696461316358175/1381844447788142612)** (10 messagesðŸ”¥): 

> `Glama build system details, MCP OpenMemory demo, OAuth 2.1 module for MCP servers, MCP servers for *arrs, mcp-openverse npm package` 


- **Glama's Build System Unveils Every Detail**: Glama's new build system provides detailed information about the build and container logs, as depicted in a [screenshot](https://cdn.discordapp.com/attachments/1315696461316358175/1381844447561646100/Screenshot_2025-06-09_at_9.37.35_PM.png?ex=6849a72d&is=684855ad&hm=1ec187525c997059bc99e72c2d65fadab4ac7b888ccd19551cd7b759b3e2c0f0&).
- **MCP OpenMemory Demo Shines**: A member shared a demo of **MCP OpenMemory** and a link to the [GitHub repo](https://github.com/baryhuang/mcp-openmemory), encouraging others to star it.
   - The demo video showcases the project in action, highlighting its capabilities.
- **Scalekit Ships OAuth 2.1 Module for MCP Servers**: Scalekit has launched a drop-in **OAuth 2.1** module with scoped, short-lived tokens, DCR + PKCE, and 401s with authorize_url for delegated flows for MCP servers, as detailed in their [documentation](https://docs.scalekit.com/guides/mcp/oauth/).
- ***Arrs Assemble in MCP Servers**: Links to various MCP servers were shared, including [radarr-mcp](https://github.com/jmagar/radarr-mcp), [sonarr-mcp](https://github.com/jmagar/sonarr-mcp), and others, split from a single container for individual use.
- **mcp-openverse Package Opens CC-Licensed Images**: A member announced the release of **mcp-openverse**, an MCP server that brings CC-licensed and public domain images to AI workflows, available on [npm](https://www.npmjs.com/package/mcp-openverse) and [GitHub](https://github.com/neno-is-ooo/mcp-openverse).
   - This tool searches over **700M+ openly-licensed images** from @WPOpenverse, integrates with Claude Desktop, and offers smart image sourcing with concept extraction.


  

---


### **Manus.im Discord â–· #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1381711418755256501)** (50 messagesðŸ”¥): 

> `Manus pricing, Veo 3, EDU email accounts, Growth person Mixedbread, AI Search infra` 


- **Mixedbread Seeks Founding Growth Person**: **Mixedbread**, a team of ex-Google Search engineers, is seeking a founding growth person in SF to convert their technical traction to **$10M ARR** and build an SF team ASAP.
   - The company boasts **50M+ HuggingFace downloads**, beats OpenAI on MTEB benchmarks, and is backed by top AI investors from OpenAI, Vercel, Perplexity, Deepmind, and Scale AI.
- **Manus still in Beta gets VEO 3**: Members are questioning why **Manus** is still in Beta, even with features like **Veo 3** and other cool updates.
   - One member reported wasting **2000 credits** due to presentation formatting issues and no refund.
- **Is Manus Pro worth it**: Users are inquiring about the value of a **Pro subscription** to Manus, specifically if the answers are significantly better and worth the cost.
   - Several people reported having problems reaching Manus support.
- **User creates Sci-Fi short film using Manus's VEO 3!**: A user created a five-minute sci-fi short film using **Manus's Veo3 feature** calling it *the most powerful generation function in the world*.
   - Another user said it *looks great*, and intentionally like old school Kung Fu movies.
- **User lost 300 credits to Veo3!**: One user spent **300 credits** on one **Veo3** video and has **38 clips**
   - Another user is asking for **100 credits** after Manus went on loop trying to truncate a file.


  

---


### **LlamaIndex â–· #[blog](https://discord.com/channels/1059199217496772688/1187460979064324127/1381732442154598444)** (5 messages): 

> `Custom Multi-Turn Memory, Real-time Website Summaries, LlamaIndex Agent as MCP Server, Databricks Data + AI Summit, Knowledge Agents to Automate Workflows` 


- ****Custom Multi-Turn Memory** Debuts for Agents**: LlamaIndex introduced a new example for building custom **multi-turn memory implementation**, ideal for agentic workflows needing control and customization, see [this tweet](https://twitter.com/llama_index/status/1932173369858003304).
- ****Instant Summaries** While You Browse**: A project by @itsclelia combines web browsing with AI-generated summaries of websites using LlamaIndex and **Google's Gemini model**, get details [here](https://twitter.com/llama_index/status/1932226292369576018).
- **LlamaIndex Agent Now **MCP Server****: LlamaIndex demoed turning an agent into an MCP server, deploying a custom **FidelityFundExtraction** workflow for extracting structured data from complex multi-fund PDFs, then invoked it from Claude, as reported [here](https://twitter.com/llama_index/status/1932472507577299040).
- **LlamaIndex at **Databricks Summit****: LlamaIndex is present at the Databricks Data + AI Summit at Booth D117 in the AI Pavilion, ready to assist with agentic AI journeys, per [this tweet](https://twitter.com/llama_index/status/1932475369128665497).
- **CEO Talks **Knowledge Agents****: LlamaIndex CEO Jerry Liu hosted a breakout session on Building Knowledge Agents to Automate Document Workflows at the Databricks Data + AI Summit, and will be repeating the session due to demand, details [here](https://twitter.com/llama_index/status/1932495184291627398).


  

---


### **LlamaIndex â–· #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1381897989899161631)** (14 messagesðŸ”¥): 

> `Agent Workflow, Handoff Issues, DirectOutputAgent, Multi-Agent Systems, OpenAI Agents SDK` 


- **Agent Workflow Woes: Handoff Hang-Ups**: A user is experiencing issues with their LlamaIndex-based product recommendation system using an agent workflow where the **plan_agent** sometimes fails to hand off to other agents like **DirectOutputAgent** or **SearchAgent**.
   - Logs indicate that the streaming stops without a clear reason, and the user is seeking assistance to understand why the handoff doesn't occur consistently.
- **RAG Reliance: Prompt Engineering Path**: A user asked about ensuring user queries are exclusively answered by RAG (Retrieval-Augmented Generation) in LlamaIndex, attempting to control this via system context and chat mode settings.
   - Another member suggested that *prompt engineering* is the primary method, and a secondary LLM call could inspect sources to judge the answer's quality.
- **Multi-Agent Mania: LlamaIndex vs OpenAI SDK Showdown**: A user inquired about the capabilities of LlamaIndex compared to the OpenAI Agents SDK for building multi-agent systems, specifically regarding integration with OpenAI and tracing in the OpenAI dashboard.
   - The member clarified that [Arize](https://manus.im/invitation/MLFE09IILLLYO) can be used for tracing with LlamaIndex, although it doesn't directly integrate with OpenAI's tracing tools.


  

---


### **LlamaIndex â–· #[ai-discussion](https://discord.com/channels/1059199217496772688/1100478495295017063/1382047619823697953)** (6 messages): 

> `Open Source Deep Research, Long Context Generation, Local Machine Research, spy-search Github repo` 


- **Open Source Tool for Deep Research**: A member introduced **spy-search**, an open-source tool that supports **Ollama** and enables deep research on a local machine.
   - The tool is designed to generate long reports, exceeding **1000 words**, offering a more comprehensive alternative to research tools with shorter outputs.
- **Spy-Search Generates Long-Context Responses**: **Spy-search** aims to provide long-context responses with the latest information, similar to **Perplexity**, but as an open-source solution.
   - The member invited the community to search for the **spy-search** repository on GitHub for those concerned about opening the direct link.


  

---


### **Cohere â–· #[ðŸ§µ-general-thread](https://discord.com/channels/954421988141711382/954421988783444043/1381755019518414939)** (15 messagesðŸ”¥): 

> `Cohere support channels, Cohere Open Science Community` 


- **Cohere's Quicker Support Channel Launched**: A member announced a new support channel, promising faster assistance using an AI-generated reply bot leveraging Cohere's documentation, available at [<#1381756280716132412>](https://discord.com/channels/967550997985429534/1381756280716132412).
   - The bot, built with **command-a**, focuses on documentation-based queries, directing account and API issues to support@cohere.com; misuse (prompt injection, etc.) will result in an instant ban.
- **Open Science Application Approvals Approaching**: A member inquired about the timeline for acceptance into the Cohere Open Science Community after submitting their application.
   - Another member responded with that *they should let you know some time soon*.


  

---


### **Cohere â–· #[ðŸ“£-announcements](https://discord.com/channels/954421988141711382/996880279224451154/1381776718456426517)** (2 messages): 

> `Cohere North, GameWarden Integration, EnsembleHP Partnership` 


- **Cohere North Integrates with GameWarden Platform**: **Cohere North** now securely integrates with the full **GameWarden** platform through a partnership with **Second Front**, to help service members gain unprecedented effectiveness and speed against an ever-evolving threat landscape, as announced in [this tweet](https://x.com/1vnzh/status/1930298055099613307).
- **Cohere North Partners with EnsembleHP**: **Cohere** is bringing **Cohere North** to healthcare by partnering with **EnsembleHP** to reduce administrative friction and elevate patient experience in hospitals and health systems with their secure AI agents platform, detailed in [this blog post](https://cohere.com/blog/ensemble-partnership).


  

---


### **Cohere â–· #[ðŸ”Œ-api-discussions](https://discord.com/channels/954421988141711382/1168578329423642786/1381754303441666099)** (4 messages): 

> `Open Source Repo for Contributions, API Tier Discussion, Reranking API Latency` 


- **Cohere has Open-Source Repo for Contributions**: Cohere has an open-source repository, the **[Cohere Developer Experience GitHub repository](https://github.com/cohere-ai/cohere-developer-experience/)**, where users can contribute by submitting pull requests to improve the documentation content.
   - The repository's **README** file offers more guidance on contributing; OpenAPI specs and snippets are one-way synced from internal repositories.
- **API Tier Talk Sparks Solutions**: A user inquired if Cohere has API tiers similar to OpenAI, mentioning a **2-second latency** with the reranking API and seeking improvements.
   - While Cohere doesn't offer tiers, there are *other solutions*, and the user was advised to contact carolyn@cohere.com for assistance.


  

---


### **Cohere â–· #[ðŸ‘‹-introduce-yourself](https://discord.com/channels/954421988141711382/1346635816629178410/1381793303812509798)** (3 messages): 

> `Vitalops, Datatune, Open Source Tools, Data Transformations, Natural Language Data Transformation` 


- **Vitalops Co-Founder Joins Community, Shares Datatune**: The co-founder of **Vitalops** introduced themselves, expressing excitement about joining the community.
   - They are developing **Datatune**, an [open source tool](https://github.com/vitalops/datatune) designed for data transformations using plain natural language.
- **Datatune: Transforming Data with Natural Language**: **Datatune**, created by **Vitalops**, is an open-source tool that allows users to perform data transformations using natural language.
   - The co-founder is eager to engage with the community and gather feedback on **Datatune's** development and potential applications.


  

---


### **Cohere â–· #[ðŸ””-ping-settings](https://discord.com/channels/954421988141711382/1346642216541622343/)** (1 messages): 

competent: Moved to <id:customize>
  

---


### **Torchtune â–· #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1381872087224029254)** (15 messagesðŸ”¥): 

> `HuggingFaceModelTokenizer Usage, Muon Performance in torchtune, Tokenizer truncation bugs, Kimi Moonlight paper, Qwen2` 


- **`HuggingFaceModelTokenizer` Intended Usage Debated**: Members discussed the intended usage of `HuggingFaceModelTokenizer`, with concerns raised about the interface differences and how to handle `max_seq_len` for packing, in particular whether to change recipes or the tokenizer itself.
   - One member suggested a solution where the recipes should be changed to take a `max_seq_len` which is then passed through, which is close to the HF pattern and aligned with [this proposal](https://github.com/pytorch/torchtune/pull/2803#discussion_r2138285972).
- **HF Tokenizer Integration Faces Hurdles**: After testing the HF Tokenizer, it was found that loss curves and total tokens don't align with the classic tokenizer, suggesting differing behavior despite minor code changes; the integration will be ready after issues [#2794](https://github.com/pytorch/torchtune/pull/2794) and [#2574](https://github.com/pytorch/torchtune/pull/2574) are addressed.
   - Also the member reported that *pre-packing takes 2-3 times longer*.
- **Tokenizer truncation bugs found**: Bugs were found in truncation while implementing the tokenizer, with related points highlighted in issue [#2792](https://github.com/pytorch/torchtune/pull/2792) with concerns that this can affect performance.
   - Members suggest sticking with the original tokenizers for training for now, awaiting consolidation to the HF ones later.
- **Muon Integration Performance Scrutinized**: The performance benefits of Muon, when integrated into torchtune, are being scrutinized, with one member wanting to see the performance benefits when it is **integrated in torchtune** to justify adding another abstraction, and another member wondering if issue [#2809](https://github.com/pytorch/torchtune/pull/2809) is critical.
   - A member points out that *there's some evidence that muon is more useful for finetuning models that were also pretrained with muon*, referencing the [Kimi Moonlight paper](https://arxiv.org/abs/2502.16982).
- **Qwen2 Issue Check**: A member has to check **Qwen2** to see if the truncation issue exists there also.
   - It was acknowledged that if the original testing didnâ€™t find the difference, it probably doesn't make a big difference but we should fix it either way.


  

---


### **DSPy â–· #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1381760873458044999)** (8 messagesðŸ”¥): 

> `Transfer learning, DSPy documentation, DSPy 3 announcement, Context optimization in DSPy, Dataset building and export tools` 


- **Transfer Learning techniques asked about**: A member inquired about the possibility of transferring post-training learning from one model to another without repeating the learning process, like finetuning or RL.
   - No specific answers were provided in the channel.
- **DSPy Documentation Vanishes**: A user noted that [a documentation file](https://github.com/stanfordnlp/dspy/pull/8094/files#diff-c7d76e3e57f0ce279bc4000266c3ec228863080e4cf891fd61e0ff6fd7575da6) was removed in a recent PR and expressed difficulty finding the same level of parameter documentation elsewhere.
   - Another user shared a link to [AI-generated docs](https://deepwiki.com/stanfordnlp/dspy/4.1-teleprompters-and-optimization#miprov2-multi-prompt-instruction-and-retrieval-optimization) that might be helpful.
- **DSPy's Contextual Prompt Optimizer**: A user asked if DSPy has anything to optimize what context to include in a prompt when a dozen variables are available.
   - They wanted to see which combination leads to good metrics balanced against the token usage of the prompt, but there was no further discussion on the topic.
- **Dataset building and export tools for DSPy**: A member asked about tools to easily build up and export datasets for use with DSPy.
   - They specifically mentioned needing something that allows generating a few dozen synthetic examples and then hand labeling them, but there was no further discussion on the topic.


  

---


### **tinygrad (George Hotz) â–· #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1381724182932361328)** (8 messagesðŸ”¥): 

> `Failing Tests, Bounty Locked Meaning, NCHWCPUGraph / LLVMGraph Refactor` 


- **Tests Failing, Bounty Locked Blocked**: Members reported that the **tests are failing**, which means that a bounty cannot be locked (ready to merge).
   - It was clarified that *bounty locked means basically ready to merge, or serious tested progress toward a goal*, but failing CI will prevent it.
- **Call for AI-Free PRs**: A member requested tasteful PRs, like the one addressing add/mul at [tinygrad/tinygrad#10741](https://github.com/tinygrad/tinygrad/pull/10741), explicitly stating *no AI slop*.
   - It was noted that *add/mul were the easiest ones*.
- **NCHWCPUGraph and LLVMGraph Demand Refactor**: It was suggested that **NCHWCPUGraph / LLVMGraph** really need to be changed to behave like the other graphs.
   - These graphs *shouldn't be rerendering stuff*, and this is related to both **multicore CPU** and the **multi compiler/renderer refactor** where CPU and LLVM should use the same graph since they have the same program.


  

---


### **Nomic.ai (GPT4All) â–· #[general](https://discord.com/channels/1076964370942267462/1090427154141020190/1381780900018131094)** (5 messages): 

> `Nomic Embed Text v1.5, Nomic GPT4All future versions, Python SDK update, GPT4All support for Mistral's Magistral Small` 


- **Nomic Embed Text v1.5 Still Supported**: A user asked if **nomic-embed-text-v1.5** can still be used from Nomic cloud next month and another user confirmed that the model is still supported for self-onboarded inference.
- **Future of Nomic GPT4All**: A user inquired about updates on future versions of **Nomic GPT4All**.
- **Python SDK Update Anticipation**: A user asked if there is an update coming on the **Python SDK**.
- **GPT4All and Mistral's Magistral Small Integration**: A user questioned whether **GPT4All** will support **Mistral's Magistral Small**.


  

---


### **Gorilla LLM (Berkeley Function Calling) â–· #[leaderboard](https://discord.com/channels/1111172801899012102/1214705495974092810/1381914136736174170)** (2 messages): 

> `Leaderboard Updates, GPU Resources` 


- **RunPod Engineer Extends Helping Hand for Leaderboard Revival**: A RunPod DX engineer offered assistance to restart **leaderboard updates**, including providing **GPU resources**.
   - The engineer encouraged direct messages for anyone needing help with resources to get the leaderboard operational again.
- **Community Expresses Gratitude for RunPod's Generosity**: Several members expressed their appreciation for the offer of **GPU resources** from the RunPod engineer.
   - The offer was seen as a significant boost to the community's efforts in maintaining and improving the leaderboard.


  

---


### **Gorilla LLM (Berkeley Function Calling) â–· #[discussion](https://discord.com/channels/1111172801899012102/1111353033352294440/1382020746443886857)** (1 messages): 

> `Agent Marketplace Status` 


- **Agent Marketplace Faces Access Issues**: A member inquired whether the **Agent Marketplace** is still operational, noting difficulties in accessing its repository and webpage.
- **Agent Marketplace Potentially Closed?**: The member also questioned if the project might be temporarily closed due to these access problems.


  

---


### **LLM Agents (Berkeley MOOC) â–· #[hackathon-announcements](https://discord.com/channels/1280234300012494859/1280236929379602493/1381729569580646501)** (1 messages): 

> `Agentic AI Summit, Early Bird Tickets, UC Berkeley, Speaker announcements` 


- **Agentic AI Summit Announced**: The **Agentic AI Summit** will be held at **UC Berkeley** on **August 2, 2025**, building on the popular **LLM Agents MOOC** with **1,500+ in-person attendees** and thousands of virtual participants.
   - The [summit website](https://rdi.berkeley.edu/events/agentic-ai-summit) includes details on how students or indie developers can apply for discount codes.
- **Early Bird Tickets End Soon!**: **Early bird pricing** for the Agentic AI Summit ends on **June 30, 2025**, with student passes at **$25**, startup passes at **$60**, and industry professional passes at **$80**.
   - Tickets can be purchased [here](https://na.eventscloud.com/ereg/index.php?eventid=842399).
- **Speakers Announced for Agentic AI Summit**: Featured speakers at the Agentic AI Summit include **Vinod Khosla** (Khosla Ventures), **Ion Stoica** (Databricks and Anyscale), **Dawn Song** (UC Berkeley), **Sergey Levine** (Physical Intelligence), **Matei Zaharia** (Databricks), **Karthik Narasimhan** (Sierra), **Waseem AlShikh** (Writer), and **Burak Gokturk** (Google Cloud).


  

---


### **LLM Agents (Berkeley MOOC) â–· #[mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/1381977221551489044)** (1 messages): 

> `SP25 Course, Quiz Questions` 


- **SP25 Course Quiz Access Query**: A user inquired about accessing **quiz questions** for the completed **SP25 course** for self-learning purposes, noting the course is currently not in session.
- **Request for SP25 Quiz**: A user has requested the availability of **SP25 course** quiz questions to study independently since the course session has ended.


  

---


### **Codeium (Windsurf) â–· #[announcements](https://discord.com/channels/1027685395649015980/1027688115592237117/1382051236714516560)** (1 messages): 

> `Planning Mode, Windsurf Wave 10, o3 model pricing` 


- **Windsurf Waves into Planning Mode**: Windsurf has released **Planning Mode** as part of Wave 10, featuring a native interface for long-term AI planning with bidirectional updates and synergy between long-term and short-term reasoning models, as detailed in [their blog](https://windsurf.com/blog/windsurf-wave-10-planning-mode) and demonstrated in [this video](https://youtu.be/t3T_re5Q21U?si=0SCf8r-vQo83GerM).
- **Cascade's conversation gets live markdown plan**: Users can toggle Planning Mode via the icon under the prompt box, enabling Cascade to pair every conversation with a live markdown plan of goals & tasks.
   - AI notifications alert users when Cascade updates the plan, facilitating a collaborative planning process.
- **O3 Model Credit Pricing slashed!**: The **o3 model** is now available for just **1x credits** and runs faster within Cascade, enhancing both cost-effectiveness and performance.
   - Planning Mode is accessible on all paid plans without additional charges.

