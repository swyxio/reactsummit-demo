---
id: 38309c71-19ba-4feb-96d7-xxx
title: not much happened today; New email provider for AINews
date: '2025-04-21T05:44:39.731046Z'
description: >-
  **Smol AI** is migrating its AI news email service to **Resend** to improve
  deliverability and enable new features like personalizable AI news and a
  "Hacker News of AI." Recent AI model updates include **OpenAI**'s API-only
  **GPT-4.1**, **Google Gemini 2.5 Flash** reasoning model, **ByteDance
  Seaweed** 7B-param video AI, **Anthropic Claude**'s values system, **Cohere
  Embed 4** multimodal embedding model, and **xAI Grok** updates with Memory and
  Studio features. Discussions also cover agentic workflows for document
  automation and AI coding patterns.
companies:
  - smol-ai
  - resend
  - openai
  - google
  - bytedance
  - anthropic
  - cohere
  - x-ai
models:
  - gpt-4.1
  - gpt-4o
  - gpt-4o-mini
  - gemini-2.5-flash
  - seaweed-7b
  - claude
  - embed-4
  - grok
topics:
  - email-deliverability
  - model-releases
  - reasoning
  - video-generation
  - multimodality
  - embedding-models
  - agentic-workflows
  - document-processing
  - function-calling
  - tool-use
  - ai-coding
people:
  - adcock_brett
  - swyx
  - jerryjliu0
  - alexalbert
  - omarsar0
---



Resend is all we need!

> AI News for 4/18/2025-4/21/2025. We checked 9 subreddits, 449
> https://twitter.com/i/lists/1585430245762441216 Twitters
> https://twitter.com/i/lists/1585430245762441216 and 29 Discords (212 channels,
> and 17339 messages) for you. Estimated reading time saved (at 200wpm): 1365
> minutes. You can now tag @smol_ai https://x.com/smol_ai for AINews
> discussions!


--------------------------------------------------------------------------------

Hello! If you haven't been getting AINews emails for a while but you're reading
this now, it's because we had increasing deliverability issues with our last
provider that affected 1/3 of our subscriber base. We have finally bit the
bullet and begun the migration to Resend https://resend.com/ . This is part of a
broader move to enable the top features you have been asking for, including:

1. personalizable AINews (existing SmolTalk subscribers - fix for Reddit still
ongoing)

2. a new "Hacker News of AI"

There are others but we see no point commenting on them until we ship the
baseline. More updates soon!

If you'd like to help us with this migration, just make sure to add
"news@smol.ai" to your contact list to help us with email domain rep.

Last call for AI Engineer Speaker Applications
https://sessionize.com/ai-engineer-worlds-fair-2025! see you very soon.


--------------------------------------------------------------------------------


AI TWITTER RECAP

Model Releases and Updates

* OpenAI models and Benchmarks: @scaling01
https://twitter.com/scaling01/status/1913955228442833032 shared OpenAI-MRCR
results with Gemini 2.5 Pro, criticizing that @OpenAI
https://twitter.com/OpenAI doesn't include other models in their own
benchmarks. Additionally, @scaling01
https://twitter.com/scaling01/status/1913957941645889583 contrasted what
OpenAI releases used to look like just 1 year ago. @Adcock_brett
https://twitter.com/adcock_brett/status/1913986827817529815 reported OpenAI's
API-only GPT-4.1, 4.1 Mini, and 4.1 Nano for developers, each beating GPT-4o
and 4o mini on dev tasks with up to 1M context windows. They noted that
GPT-4.1 scored 55% on SWE-Bench Verified with prices starting at $2 and $8
per million I/O tokens. @Adcock_brett
https://twitter.com/adcock_brett/status/1913986805520646501 also announced
that OpenAI revealed its smartest reasoning AI models yet with o3 and
o4-mini.

* Google Gemini: @Adcock_brett
https://twitter.com/adcock_brett/status/1913986859148996798 reported that
Google dropped Gemini 2.5 Flash, a reasoning model matching o4-mini in
preview, with up to 24k tokens, and noted its strong performance in
reasoning, STEM, and visual reasoning.

* ByteDance Seaweed: @Adcock_brett
https://twitter.com/adcock_brett/status/1913987039034212834 shared that
ByteDance released Seaweed, a hyper-efficient 7B-param video AI that supports
text-to-video, image-to-video, and audio-driven synthesis, with clips up to
20s. It matches or outperforms larger models like Sora, Kling 1.6, and Veo.

* Anthropic Claude and Values: @AnthropicAI
https://twitter.com/AnthropicAI/status/1914333220067213529 announced that
they have built a system that allows them to see how Claude's values are
expressed and adapted across a huge variety of situations. This involved
studying hundreds of thousands of anonymized conversations.

* Cohere Embed 4: @Adcock_brett
https://twitter.com/adcock_brett/status/1913987067559760340 noted Cohere
released Embed 4, a SOTA multimodal embedding model for building search and
retrieval capable AI apps, with a 128K-token context window, support for 100+
languages, optimization for data from regulated industries, and up to 83%
savings on storage costs.

* Elon Musk's xAI Grok Updates: @Adcock_brett
https://twitter.com/adcock_brett/status/1913986949112631554 noted Elon Musk's
xAI updated Grok with Memory and Studio, with Memory enabling the AI to
remember chats for personalized responses, while Studio creates a new window
to help users collaborate with it on docs and code, and that Studio is pretty
much like ChatGPT Canvas.

* DeepSeek Talk: @swyx https://twitter.com/swyx/status/1913818917765870050 put
out a special call for help to do a DeepSeek talk.

AI Applications and Use Cases

* Agentic Workflows and Automation: @jerryjliu0
https://twitter.com/jerryjliu0/status/1914002780454551851 is releasing slides
for agentic document workflows laying out the architecture for how LLMs can
parse, reason over, and act on PDFs and Excel files, and is excited about
using AI agents to automate knowledge work over documents. @alexalbert__
https://twitter.com/alexalbert__/status/1914333320877584397 shared
Anthropic's learnings about using Claude Code internally, noting the most
effective patterns they've found apply to coding with LLMs generally, with a
link to a blog post. @omarsar0
https://twitter.com/omarsar0/status/1913969681599262768 describes loops,
function calling, tool execution, and the model in building a search agent
from scratch as ~350 lines of code, noting building an agentic system isn't
too hard.

* Gemini 2.5 in Pokemon: @_philschmid
https://twitter.com/_philschmid/status/1914199732013985840 notes Gemini 2.5
Pro continues to make progress in completing PokÃ©mon!

* AI in Healthcare: @Yuchenj_UW
https://twitter.com/Yuchenj_UW/status/1914000352606818419 shared an
experience where ChatGPT correctly diagnosed a medical condition (orthostatic
hypotension) and recommended a treatment (electrolyte water) that doctors had
missed, expressing belief that AI might replace doctors and is feeling
grateful and will consult AI more often. @gdb
https://twitter.com/gdb/status/1914106403574452496 is hearing more stories of
ChatGPT helping people fix longstanding health issues, noting how AI is
already improving people's lives in meaningful ways.

Frameworks and Tooling

* Langchain: @hwchase17
https://twitter.com/hwchase17/status/1914016302261506421 wrote a blog on how
to think about agent frameworks with background info about agents, what is
hard about building them, and flavors of agentic frameworks, and common
questions such as the value of a framework. @jerryjliu0
https://twitter.com/jerryjliu0/status/1914035527608897897 responded to a blog
post about agent frameworks, noting inaccuracies in the checkboxes on
@llama_index, and described the features of llama index including
event-driven workflows, multi-agent abstractions, and support for short-term
and long-term memory. @LangChainAI
https://twitter.com/LangChainAI/status/1913729321463603326 announced a new
Dart package enabling LangGraph agent capabilities in Flutter/Dart apps
across all major platforms, powered by Server-Sent Events for seamless agent
interactions.

* RunwayML Gen-4: @c_valenzuelab
https://twitter.com/c_valenzuelab/status/1913761070725935178 notes they have
always dreamt about a moment in time when they could take a picture or video
of something they see in the street and creatively remix any part of it just
by describing it and that we are finally here.

* Cursor: @Teknium1 https://twitter.com/Teknium1/status/1913737232231612530
thinks Cursor is doing the coolest practical agentic work right now.

AI Research and Techniques

* RL and Reasoning: @iScienceLuvr
https://twitter.com/iScienceLuvr/status/1913783561725100494 suggests there's
more than just math and coding for RL. @_philschmid
https://twitter.com/_philschmid/status/1913869660463702435 discussed research
on integrating tool use in long-form reasoning and how ReTool teaches LLMs to
dynamically interleave reasoning with tool-use (code execution) using
Reinforcement Learning. @TheTuringPost
https://twitter.com/TheTuringPost/status/1913729366976332153 covers hogwild
inference, and how it lets multiple instances of the same LLM run in
parallel.

* Antidistillation Sampling: @TheAITimeline
https://twitter.com/TheAITimeline/status/1914175986100310119 described
antidistillation sampling, a technique that strategically modifies the
model's next-token probability distribution during the generation process to
counter unwanted model distillation.

* Sleep-time Compute: @TheAITimeline
https://twitter.com/TheAITimeline/status/1914175946715779426 summarized
Sleep-time compute, a technique that allows LLMs to perform computations
offline by anticipating user queries, aiming to reduce the high latency and
cost associated with scaling test-time inference.

* BitNet b1.58 2B4T: @TheAITimeline
https://twitter.com/TheAITimeline/status/1914175939698630757 summarized the
BitNet b1.58 2B4T Technical Report, noting it is a native 1-bit LLM with 2
billion parameters trained on 4 trillion tokens, matching the performance of
comparable full-precision LLMs.

* RAG: @TheTuringPost
https://twitter.com/TheTuringPost/status/1913915061388849613 shared a list of
11 new types of RAG with links and more info.

* Diffusion Transformers: @realDanFu
https://twitter.com/realDanFu/status/1914376553745850454 announced Chipmunk
ðŸ¿ï¸- training-free acceleration of diffusion transformers (video, image
generation) with dynamic attention & MLP sparsity that is 3.7x faster for
video generation, 1.6x faster for image generation, and that kernels are
written in TK âš¡ï¸ðŸ±.

Broader AI Discussions and Concerns

* Definition of AGI: @karpathy
https://twitter.com/karpathy/status/1913741942221144430 states that the
goalpost movement in his tl is in the reverse direction recently, with LLMs
solving prompt puzzles and influencers hyperventilating about AGI, so that
they are sticking with the original OpenAI definition, and arenâ€™t sure what
people mean by the term anymore.

* Open Source and Ethics: @teortaxesTex
https://twitter.com/teortaxesTex/status/1913755206292013250 writes that even
if the actual CCP then continues building some Two Agents One PASTA Factory,
the gap won't be as big as between LLaMA 4.1 and â€œUSG AGIâ€.

* AI replacing jobs: @Teknium1
https://twitter.com/Teknium1/status/1914079672071331865 wonders what the next
actual job that AI will actually replace is.

Humor and Memes

* Easter Eggs: @AravSrinivas
https://twitter.com/AravSrinivas/status/1913815909040275888 shared some good
looking Easter eggs.

* Vibes: @jxmnop https://twitter.com/jxmnop/status/1914079978800517600 said
this tweet is a direct response to the appearance of the phrase â€œvibe
bloggingâ€ on my timeline.

* Lights: @typedfemale
https://twitter.com/typedfemale/status/1913717083978056093 asked why do
performance engineers love putting up lights at christmas? Because they love
rooflining!

* @abacaj https://twitter.com/abacaj/status/1913758272982495287 posted "itâ€™s
true ðŸ˜‚".


--------------------------------------------------------------------------------


AI REDDIT RECAP


/R/LOCALLLAMA RECAP




1. NEW MODEL AND BENCHMARK RELEASES (GLM-4 32B, ORPHEUS, TTS, INSTANTCHARACTER)

* GLM-4 32B is mind blowing
https://www.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/
(Score: 306, Comments: 105
https://www.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/):
The post benchmarks GLM-4 32B Q8 (quantized, 8-bit) run locally via llama.cpp
PR #12957 https://github.com/ggml-org/llama.cpp/pull/12957 against Gemini 2.5
Flash and other ~32B and larger open-source models, reporting markedly
superior performance in code-generation and UI/visualization tasks. The
author highlights zero-shot outputs where GLM-4-32B produced complete,
lengthy, and highly detailed single-file implementations (e.g., 630+ lines
for neural nets visualization with UI controls and coherent code output)
without truncationâ€”unlike Gemini and others. Inference speed was 22 t/s on 3x
RTX 3090 GPUs at Q8; GLM-4-32B demonstrated solid tool-calling and
integration with Aider/CLI tools, with qualitative outperformance observed
versus Qwen 2.5 coder, QWQ, and Gemini, especially in code completeness and
UI fidelity (see code/demo comparisons
https://reddit.com/link/1k4god7/video/ylcl9s4ri7we1/player). Top commenters
corroborate the benchmark, claiming GLM-4-32B outperforms Qwen 2.5 coder/QWQ
and expressing interest in broader deployment; one requests comparison with
QwQ-32B specifically. The technical debate centers on code-quality, feature
set, and readiness for integration into inference tools like LM Studio.

* Multiple commenters compare GLM-4 32B favorably to Qwen 2.5 Coder and QwQ
models, highlighting its superior performance in coding-related tasks and
general usage. One user suggests direct testing at https://chat.z.ai/
https://chat.z.ai/ for firsthand evaluation.

* One technical point raised is the use of "broken" GGUFs (model files)
requiring specific command-line options for usability. The commenter
advises waiting for the final merged version of GLM-4 32B for enhanced
stability and compatibility, which would enable broader experimentation
across different workflows and platforms.

* Interest is expressed in using GLM-4 32B with LM Studio, indicating a
demand for ecosystem support and integration for easier deployment and
accessibility.

* nsfw orpheus early v1
https://www.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/
(Score: 332, Comments: 66
https://www.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/):
The post announces the early preview release of an NSFW-focused TTS voice
model, mOrpheus_3B-1Base, available on Hugging Face (v1
https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview, v2
preview
https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview-v1-8600).
The model currently supports "common sounds" and generalizes well, albeit
with only one voice in the preview checkpoints. The author notes considerable
technical work on data cleaning and pipeline preparation to enable clean
output, with the early models exhibiting convincing moans, laughter, and
buildup to "sultry content." One technical question from commenters raises
the challenge of synthesizing complex emotional expressions (crying, angry
screaming) and asks about the future ability of the model to handle such
nuance, reflecting a broader technical gap in TTS emotional expressiveness.

* MrAlienOverLord discusses the significant challenge of creating a reliable
data pipeline for NSFW voice synthesis, emphasizing that cleaning and
structuring data for emotionally rich TTS output was a major hurdle but
ultimately enabled the generation of nuanced behaviors (e.g., moaning,
laughing, and sultry buildup).

* ffgg333 inquires about the model's capacity for emotional
diversityâ€”specifically whether it can believably produce complex emotions
like crying or angry screamingâ€”and whether future iterations will tackle
these harder-to-synthesize expressions. They also request information on
supported emotional tags and seek access to demos on platforms like
HuggingFace, highlighting practical interest in both technical capabilities
and user interface.

* BlipOnNobodysRadar requests usage instructions, indicating a typical need
for accessible implementation guides or documentation for running
early-stage, community-released TTS models.

* Hunyuan open-sourced InstantCharacter - image generator with
character-preserving capabilities from input image
https://www.reddit.com/gallery/1k43htm (Score: 132, Comments: 6
https://www.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/):
Tencent has open-sourced InstantCharacterâ€”a tuning-free, one-shot image
generation framework for character-preserving synthesis from a single
reference image plus text prompt, aiming to balance consistency, image
quality, and domain flexibility (project https://instantcharacter.github.io/,
code https://github.com/Tencent/InstantCharacter, paper
https://arxiv.org/abs/2504.12395). The method is compatible with the Flux
pipeline, delivering high-fidelity, text-controllable outputs and targets
per-instance generalization without retraining, positioning itself as a
precision competitor to GPT-4o for image synthesis. Example results and
evaluation demos are available on HuggingFace
https://huggingface.co/spaces/InstantX/InstantCharacter. Expert users note
the model achieves moderate clothing fidelity but struggles with facial
identity and body shapes, implying limitations on realism ("no resemblance to
the input face at all"). VRAM requirements are discussed, with estimates
suggesting usage similar to other Flux-based models (~20â€“30GB). The model is
criticized for poor handling of anime-style 2D images, which were not
evidently covered in training.

* Users report that InstantCharacter, run on hardware like A40 via RunPod,
performs similarly to a slightly improved IPAdapter for character
preservation, particularly noting its utility for clothing generation but
significant shortcomings for facial matching and body type retentionâ€”"No
resemblance to the input face at all. Doesn't even take a body type into
account."

* There are technical questions and speculation about the VRAM requirements
of InstantCharacter, with some users estimating resource needs similar to
base 'flux dev' models, in the range of 20-30GB VRAM, though no concrete
figures are provided in the current discussion.

* Performance appears to vary by training data: one user notes poor
capability for anime-style 2D images, suggesting the lack of training on
such data impacts generalization, particularly if non-photorealistic genres
are important use cases.

* A new TTS model capable of generating ultra-realistic dialogue
https://github.com/nari-labs/dia (Score: 140, Comments: 40
https://www.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/):
A new TTS model has been released, reportedly capable of producing
ultra-realistic dialogue with significant expressive detail, as demonstrated
in linked samples https://voca.ro/1oFebhjnkimo and emphasis via audio prompts
https://voca.ro/1fQ6XXCOkiBI. Technical questions focus on whether the
released weights correspond to the demoed model (possible distinction between
smaller released and unreleased larger models), supported languages, emotion
steering, voice cloning, prosody control (pauses, phonemization), and
corpus/training regimen specifics. Commenters express skepticism about
whether public weights match the showcased results and note limited
documentation on functionalities (languages, emotion control, phoneme
support, etc.), indicating that the README lacks important model capabilities
and configuration details. [External Link Summary] Dia
https://github.com/nari-labs/dia is a 1.6B parameter text-to-speech (TTS)
model developed by Nari Labs that generates ultra-realistic multi-speaker
dialogues directly from transcripts in a single pass. It supports audio
conditioning for controlling emotion and tone, as well as synthesis of
nonverbal sounds (e.g., laughter, coughing), and provides research-ready
pretrained weights and code with a PyTorch backend (CUDA 12.6, pytorch 2.0+),
requiring ~10GB VRAM for inference; quantized and CPU versions are planned.
Dia is inspired by SoundStorm and Descript Audio Codec, with results and
comparisons hosted on Hugging Face, and is released under the Apache 2.0
License, with clear restrictions against misuse for identity theft,
deception, or illegality.

* Commenters discuss the model's demo samples, noting significant increases
in audio realism when using the 'audio prompt' feature; the quality with
this setting reportedly surpasses standard outputs, showcasing substantial
advancements in expressive and nuanced synthesis (see link with audio
prompt https://voca.ro/1fQ6XXCOkiBI).

* Technical questions are raised regarding the model's capabilities: support
for multiple languages, degree and method of emotion steering, voice
cloning procedures, insertion of pauses and phonemization capability, and
details on required training data duration, implying the current
documentation lacks key implementation and feature information.

* Users compare output speed and naturalness, with one requesting options to
slow down generated speech due to a rapid delivery reminiscent of legacy
TTS systems (e.g., MicroMachines commercial), suggesting that prosody and
pacing controls are critical for achieving ultra-realistic dialogue.


2. HARDWARE AND VRAM CONSIDERATIONS FOR RUNNING LLMS

* 24GB Arc GPU might still be on the way - less expensive alternative for a
3090/4090/7900XTX to run LLMs?
https://videocardz.com/newz/sparkle-confirms-arc-battlemage-gpu-with-24gb-memory-slated-for-may-june
(Score: 191, Comments: 77
https://www.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/):
Rumors suggest Intel may release a 24GB Arc GPU, positioning it as a less
expensive consumer-grade alternative for running LLMs (large language
models), compared to high-end GPUs like the RTX 3090, 4090, or 7900XTX.
Technical discussion points to strong Intel driver support, ongoing community
integration with IPEX-LLM, and competitive VRAM, though there is no CUDA
support and memory bandwidth is estimated to be about half of an RTX 3090,
potentially aligning it more with an RTX 4060 in compute capability but with
superior memory capacity. Commenters note the lack of CUDA as a significant
limitation for LLM and ML workloads, though Vulkan and increased VRAM make it
promising for non-CUDA-based applications. The card could compete with
mid-range NVIDIA offerings for memory-constrained tasks, but bandwidth
bottlenecks and actual performance parity are debated. [External Link
Summary] The article reports that Sparkle has officially confirmed an Intel
Arc â€œBattlemageâ€ GPU equipped with 24GB of memory. This high-capacity
graphics card is slated for release in the Mayâ€“June timeframe. The
announcement highlights a significant memory bump over current Arc models,
indicating competitive positioning for demanding workloads or next-gen
gaming. Read more
https://videocardz.com/newz/sparkle-confirms-arc-battlemage-gpu-with-24gb-memory-slated-for-may-june

* Commenters note that a 24GB Arc GPU could provide a less expensive
alternative for running large language models (LLMs), given its large VRAM
capacity and integration efforts like IPEX-LLM, but the lack of CUDA
support poses substantial compatibility limitations for many deep learning
frameworks.

* Performance-wise, the anticipated Arc GPU is compared to an RTX 4060, but
with half the memory bandwidth of the RTX 3090. While the high VRAM is
attractive for LLM workloads, there are concerns that bandwidth and general
performance would lag behind higher-end Nvidia cards (e.g., 3090/4090) and
even some upcoming mid-range cards (e.g., RTX 5060 Ti 16GB).

* Recent updates highlight conflicting messaging from board partners (e.g.,
Sparkle Taiwan vs. Sparkle China) regarding the actual release and
existence of the 24GB Arc GPU, reflecting ongoing uncertainty, which may
affect planning for developers or researchers considering non-Nvidia
hardware for LLM workloads.

* What's the best models available today to run on systems with 8 GB / 16 GB /
24 GB / 48 GB / 72 GB / 96 GB of VRAM today?
https://www.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/
(Score: 171, Comments: 101
https://www.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/):
The post requests up-to-date recommendations for the best local LLMs suitable
for various VRAM capacities ranging from 8GB to 96GB, specifically focusing
on practical deployment constraints. A detailed table in the comments
suggests specific models for each VRAM range, e.g., Gemma 3 4B (8GB), Llama
3.1 8B (12GB), Gemma 3 27B/Qwen 2.5 32B (32GB), up to Command A 111B and
Mistral Large (96GB), assuming use of 4-bit quantization for both weights and
KV-cache with up to 48,000 token context. The post has been periodically
updated with new model releases, such as QwQ 32B and Mistral Large, to
reflect the fast-changing LLM ecosystem. One technically relevant comment
challenges the framing, asking 'Best for what?', emphasizing that optimal
choice depends on performance tradeoffs (e.g., speed, accuracy, specific task
domain, etc.). Another meta-comment notes how frequently this question is
asked, suggesting the need for regularly updated sticky guidance.

* A table of model-VRAM pairings suggests optimal choices for various VRAM
amounts, with notable examples including Gemma 3 4B for 8GB, Llama 3.1 8B
for 12GB, and Command A 111B or Mistral Large for 96GB VRAM, all with
configuration details like 48k token context and 4-bit quantization for
both weights and KV cache. This highlights not only raw VRAM requirements
but also practical quantization techniques that influence feasibility and
performance.

* Experiments with Gemma 3 12B QAT (quantization-aware training)
quantizations show that even with only 12 GB VRAM, the model can run
acceptably by offloading some layers to CPU, though with reduced speed.
While not matching top cloud LLMs in perceived output quality, local models
like this provide competitive results and illuminate hardware compromises
for edge deployments.

* On systems with 8GB VRAM, user experimentation finds models in the
effective 9-13GB parameter range if quantized appropriately, such as Reka
Flash 3 (Q3) for reasoning and Gemma 3 12B (Q4) for multimodal
applications. Detailed TPS (tokens per second) and offload ratios are
discussed for practical real-world throughput, noting that some large
models (e.g., QWQ 32B or Mistral Small 3.1) are functionally usable but may
be frustratingly slow at this VRAM tier.

* Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)
https://v.redd.it/8hsjp4q1w3we1 (Score: 154, Comments: 15
https://www.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/):
The post demonstrates using KoboldCpp's "noscript mode" through Internet
Explorer 6 (released in 2001) by accessing the web UI over a network,
leveraging browser emulation (via oldweb.today http://oldweb.today) or a VM.
The KoboldCpp Windows binary is not compatible with systems that can only run
IE6; only the web interface is accessed remotely from modern hardware running
the model. A top comment clarifies technical feasibility: noscript mode is
broadly compatible with browsers from the last 30 years since it avoids
modern scripting. Additionally, there is mention of someone running an
extremely small (but largely impractical) language model on Pentium 3
hardware, illustrating the lower bound of LLM deployment. [External Link
Summary] This Reddit thread discusses running KoboldCpp, a local large
language model (LLM) UI, in "noscript" mode to support legacy browsers such
as Internet Explorer 6 by disabling JavaScript. While the KoboldCpp Windows
binary cannot execute directly on such old systems, its web interface is
accessible from them via network. The noscript mode is valuable for
retro-computing enthusiasts, those needing terminal browser support, or users
concerned about JavaScript security, with the feature mainly added for fun
and niche use cases by the developers. Original thread here.
https://v.redd.it/8hsjp4q1w3we1

* One commenter clarifies that the true technical constraint here is the
frontend/browser: while Internet Explorer 6 (released in 2001) is featured,
the UI in 'noscript' mode works on almost any browser from the past 30
years. The backend (KoboldCpp) runs on modern hardware, with old browsers
connecting over the network; actual KoboldCpp binaries cannot run directly
on legacy systems due to significant hardware and OS limitations.

* A user recalls a technical experiment where a very minimal language model
was run on Pentium III (P3) hardware. Although the model was tiny and
functionally limited, the demonstration highlights the constraints and
possibilities of running language models on vintage hardware, emphasizing
the orders-of-magnitude difference in resource requirements compared to
contemporary LLMs.


3. OPEN SOURCE AI BUSINESS MODELS AND COMMUNITY SPECULATION

* Why are so many companies putting so much investment into free open source
AI?
https://www.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/
(Score: 164, Comments: 130
https://www.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/):
The post questions the business rationale behind heavy corporate investment
into free/open-source AI, given proliferating alternatives that undercut
commercial subscription models (citing OpenAI, Google, Llama.cpp, Unsloth,
Mergekit). It highlights that firms like OpenAI provide generous free-tier
access, further diminishing apparent revenue prospects, and wonders about
ultimate strategic goals if no distinct performance lead emerges. Technical
comments stress crowd-sourced innovation and rapid iteration as a key
motivator: releasing open source models accelerates improvement and ecosystem
building (e.g., major downstream contributions like Llama.cpp have made
quantifiable cost savings industry-wide). Additionally, companies gain
significant indirect value via publicity, research feedback, and ecosystem
dominance. Monetization remains elusive for most, with GPU rental services
being a notable exception thus far. Commentary underlines that much open AI
investment is about collective progress and moat destruction, with many
startups and tech leaders seeing open strategy as a necessary response to
proprietary moves (e.g., OpenAI). The community's cumulative research has
broadly accelerated state of the art and redistributed value, but the
long-term profitability question is unresolved, with speculation that direct
profits are distant for all but infra providers.

* Open source AI investment enables large-scale crowdsourcing for testing,
development, and rapid innovation, as seen in projects like Llama.cpp,
Unsloth, and Mergekit. The open release of models allows external
researchers and enthusiasts to provide free feedback, discover
optimizations, and share findings via open repositories and
papersâ€”generating massive aggregate cost savings and accelerating progress
for those companies.

* Meta's shift post-Llama weights leak demonstrates a deliberate
commoditization strategy: by open-sourcing strong models, they raise the
ecosystem 'floor,' drive widespread Llama compatibility, and benefit from
global research focused on their architecture. This approach is less about
having the absolute best proprietary model and more about entrenching their
ecosystem as a default, drawing clear parallels to OpenAI's API lock-in and
Anthropic's MCP. Nation-state initiatives (e.g., Mistral in Europe, Falcon
in Abu Dhabi) further diversify the motivations, often focused on regional
technological independence.

* Monetization is still elusive for most open-source model providers;
companies offering GPU rental services are currently the main profit
makers. Many subscriptions (even at large companies like OpenAI) have
historically operated at a loss or are subsidized, sometimes using prompt
data for further training or employing a strategy of undercutting prices to
build market dominance, with plans for monetization post-competition
shakeout.

* Donâ€™t Trust This Woman â€” She Keeps Lying
https://www.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/
(Score: 140, Comments: 40
https://www.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/):
The post centers on Bindu Reddy, CEO of Abacus.AI http://Abacus.AI and
sponsor of LiveBench, allegedly spreading unverified rumors about major
open-source LLM release timelines (notably for Qwen and Deepseek models).
Screenshots show Qwen's official team publicly denying claims about imminent
releases, attributed to Reddy on social media. Conflicting release
information is described as speculative and rapidly corrected by developers
themselves, highlighting a persistent cycle of misinformation. Top commenters
assert this behavior damages credibility in the open-source community,
pointing out that repeated unsubstantiated leaks are motivated by attention
rather than fact. They advocate ignoring such sources unless credible
evidence or direct confirmation from model developers exists.

* The top comment provides detailed allegations that Bindu Reddy, CEO of
Abacus.AI http://Abacus.AI and sponsor of LiveBench, regularly announces
false release dates for major open-source AI models (specifically citing
"R2" and "Qwen 3") without evidence. The commenter notes a repeating
pattern: Reddy is contradicted by official model developers, deletes the
inaccurate posts, and faces no repercussions, potentially spreading
misinformation regarding the timelines and availability of significant LLM
releases.




OTHER AI SUBREDDIT RECAP

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI,
> /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo


1. OPENAI O3 RELEASE: COMMUNITY BENCHMARKS AND HALLUCINATION ISSUES

* OpenAI's o3 AI model scores lower on a benchmark than the company initially
implied | TechCrunch
https://techcrunch.com/2025/04/20/openais-o3-ai-model-scores-lower-on-a-benchmark-than-the-company-initially-implied/
(Score: 147, Comments: 25
https://www.reddit.com/r/OpenAI/comments/1k41jso/openais_o3_ai_model_scores_lower_on_a_benchmark/):
The post discusses discrepancies in benchmark results for OpenAI's 'o3' AI
model on the FrontierMath benchmark. The original headline claims the public
o3 model scored lower than OpenAI implied, but top commenters note the public
release marginally outperforms earlier claims (improving from 8-9% to
approximately 10%), while the much-publicized 25% result was achieved under
extremely high test-time compute (~$3000/prompt per ARC AGI), not on the
standard model. Issues around result reproducibility, benchmark subsets
(frontiermath-2024-11-26 vs frontiermath-2025-02-28-private), and internal
versus public scaffolding are highlighted as confounding factors. Commenters
emphasize skepticism towards company benchmarks, stressing the need for
independent, third-party benchmarks for accurate model assessment. One notes
recent model releases seem underwhelming in real-world use.

* The public release of OpenAI's o3 model scores around 10% on a certain
benchmark, which is slightly higher than the previously announced 8-9%. The
much-publicized 25% score applied only under extremely high compute
conditions (e.g., estimates of ~$3000 per prompt), making that result
unrepresentative of actual user experience in the public version.

* A critical point raised is the limited reliability of internal benchmarks
from model developers like OpenAI; independent third party benchmarks are
recommended, especially those evaluating across a broad set of real-world
scenarios, for a more accurate measurement of the model's true performance.

* There's a technical usability critique comparing OpenAI's project/file
limitations to Claude's; specifically, OpenAI limits by file count (e.g.,
20 small files), rather than token count, which constrains users who
organize projects modularly even if the total data is minimal compared to
Claude's more generous limits.

* Shocked at how much o3 is hallucinating.
https://www.reddit.com/r/OpenAI/comments/1k4a9jj/shocked_at_how_much_o3_is_hallucinating/
(Score: 138, Comments: 54
https://www.reddit.com/r/OpenAI/comments/1k4a9jj/shocked_at_how_much_o3_is_hallucinating/):
A user reports a significant increase in hallucination rates in GPT-4o
(referred to as o3) compared to previous versions, particularly when the
model is tasked with complex queries such as genealogy research involving
sparse and ambiguous historical records. The model fabricated
plausible-sounding but completely false citations and biographical details,
only retracting them after persistent questioning. Commenters reference
internal data (OpenAI systems card) indicating o3 has a 30% hallucination
rate on the personQA benchmark, double that of GPT-3.5 (o1 at 15%),
suggesting improved capabilities come at the cost of higher confident
fabrication, potentially due to insufficient reinforcement against
hallucination during post-training. Top comments highlight expert concern
that o3 is a 'Baron Munchausen'â€”more capable but more prone to elaborate
falsehoods. Debate includes whether previous GPT models performed better on
such tasks, and speculation that the RLHF phase does not sufficiently
penalize plausible-sounding hallucinations.

* A commenter cited the OpenAI system card, stating that GPT-4o ("o3") has a
hallucination rate of 30% on the PersonQA benchmark, double that of GPT-4
("o1") at 15%, meaning o3 is more accurate overall but also substantially
more likely to hallucinate. This suggests that hallucinations are not
sufficiently penalized in o3's post-training phase, possibly by design or
oversight.

* Multiple users noted experiential issues with o3, including a specific
instance in which the model invented new content while editing text and
then insisted the fabricated content originated from the userâ€™s documents.
This highlights not only increased hallucination frequency but also a
strong confidence in false outputs, making detection and correction
cumbersome for end users.

* There is speculation among technically proficient users that OpenAI may
have released unfinished or insufficiently validated models, as o3 is
repeatedly reported as underperforming in both output length and
hallucination control compared to earlier versions. The consensus in
rigorous use environments is that these regression issues are widespread
and significant.

* o3 is Brilliant... and Unusable
https://www.reddit.com/r/OpenAI/comments/1k4bfy6/o3_is_brilliant_and_unusable/
(Score: 597, Comments: 159
https://www.reddit.com/r/OpenAI/comments/1k4bfy6/o3_is_brilliant_and_unusable/):
The post discusses the o3 model, which shows remarkable promises in
specialized domains like nutraceutical development, chemistry, and biology by
generating novel and creative solutions. However, the poster highlights o3's
significant hallucination rate, where plausible-sounding yet inaccurate
information is common â€” a recognized issue substantiated by OpenAI's own
reporting (system card PDF
https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf),
which lists o3's PersonQA hallucination rate at 0.33 (vs. o1's 0.16), even as
its accuracy is only modestly higher (0.53 for o3, 0.47 for o1). This echoes
broader concerns about RLHF (Reinforcement Learning from Human Feedback)
tuning pushing models toward confident, logical, but sometimes incorrect
synthesis. Commenters emphasize that this creative overreach is a novel
QA/process issue, diverging from the anticipated AI trajectory; o3's lateral
reasoning yields impressive but unreliable content, necessitating human-like
quality assurance but with a distinct fault heuristic. The proliferation of
convincingly wrong output poses risks in automated knowledge work, with some
users sharing anecdotes of AI fabricating plausible yet fictional academic
content, highlighting the practical impact of model hallucinations.

* OpenAI's internal testing highlights the trade-off in o3: it achieves
higher accuracy (0.53 on PersonQA) than o1 (0.47), but at the cost of
doubling the hallucination rate (0.33 for o3 vs. 0.16 for o1), as detailed
in their system card
https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf.
This raises questions about whether raw capability improvements outweigh
reliability in applied settings.

* Discussion suggests that the increased creativity and lateral thinking in
o3 results in more frequent and convincing hallucinations, comparable to
how an expertâ€”when encouraged to be more conversationalâ€”may start making
confident but incorrect off-the-cuff statements. This aligns with
observations in other user-facing variants, where increased conversational
ability often trades off with truthfulness or factual grounding.

* A key implementation note is that o3-based models used for Deep Research do
not exhibit the same hallucination problem, indicating that the issue may
stem from the post-training process optimized for chatbot use (e.g., cost
and engagement tuning), rather than the base model itself. This points to
the importance of post-training alignment and deployment context in model
behavior.


2. SKYREELS-V2 AND LTX 0.9.6: ADVANCES IN OPEN-SOURCE VIDEO GENERATION

* I tried Skyreels-v2 to generate a 30-second video, and the outcome was
stunning! The main subject stayed consistent and without any distortion
throughout. What an incredible achievement! Kudos to the team!
https://v.redd.it/nfyhj0xyx4we1 (Score: 210, Comments: 51
https://www.reddit.com/r/StableDiffusion/comments/1k47784/i_tried_skyreelsv2_to_generate_a_30second_video/):
The user reports that Skyreels-v2, running on a 1xA100 GPU, successfully
generated a 30-second video with high consistency and no subject distortion.
Notably, Skyreels-v2 produces video at 24fps, an improvement over competing
models Wan and Vace (which run at 16fps), resulting in smoother motion and
reduced artifacting, especially during rapid movement. Commenters express
hope for rapid integration into other platforms (e.g., Kijai), highlighting
community interest in broader adoption due to these technical improvements.
[External Link Summary] A user reported generating a 30-second video using
Skyreels-v2 on a single NVIDIA A100 GPU, noting the main subject remained
stable and free of distortions. The discussion highlights that Skyreels-v2
renders at 24fps, providing smoother motion than previous models (like Wan
and Vace, which output at 16fps), reducing common video generation artifacts
such as limb or face disintegration. The post and comments indicate that such
results are contingent on access to high-end hardware, though models may be
quantized in the future for broader local deployment. Original post
https://v.redd.it/nfyhj0xyx4we1

* One commenter notes that Skyreels V2 generates videos at 24fps (compared to
16fps for competing models like Wan and Vace), resulting in more fluid
motion and reducing visible artifacts such as limbs and faces
'disintegrating' during fast movements, which directly improves output
realism and temporal coherence.

* There is technical interest in hardware and performance specifics: a user
asks how long it took to generate a 30s video and which GPU was used,
pointing to performance expectations with different accelerators (noting
the OP reportedly used a 1xA100).

* Another user inquires about which specific Skyreels v2 model was used to
achieve good character consistency, natural motion, and lighting effects,
indicating the presence of multiple model variants and a technical focus on
reproducibility and deployment choices.

* SkyReels-V2 I2V is really amazing. The prompt following, image detail, and
dynamic performance are all impressive! https://v.redd.it/jsudhyhiu5we1
(Score: 190, Comments: 91
https://www.reddit.com/r/StableDiffusion/comments/1k49qn9/skyreelsv2_i2v_is_really_amazing_the_prompt/):
The post describes strong empirical performance of the open-source
SkyReels-v2 image-to-video (I2V) model, highlighting its prompt adherence,
image detail, and motion smoothness compared to proprietary alternatives like
Sora, Kling, and Wan. Community commentary includes a direct link to Kijai's
quantized 14B-540P version (HuggingFace model card
https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels), confirming
distribution and practical reproducibility. A notable technical claim in the
comments states SkyReels-v2 uses Wan 2.1 as a base, suggesting potential
architectural or training dependencies; discussion also includes skepticism
about the organic nature of the praise. [External Link Summary] SkyReels-V2
is an open-source image-to-video (I2V) model praised for its strong prompt
adherence, high image detail, and smooth video generation, positioning it
competitively with leading models like Wan, Sora, and Kling. Multiple model
sizes are available (1.3B, 5B, 14B parameters), including quantized and FP8
versions for reduced VRAM usage, with community-reported successful runs on
top-tier GPUs (e.g., RTX 4090, A100). Integrations exist via ComfyUI and
WanVideo wrappers, and the latest release and resources are available on
GitHub https://github.com/SkyworkAI/SkyReels-V2 and HuggingFace
https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels.

* The quantized 14B-540P version of SkyReels V2 I2V was uploaded by Kijai,
making it accessible on HuggingFace (link
https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels).
Quantization typically reduces VRAM requirements, facilitating
experimentation for users with more modest hardware.

* A commenter highlights that SkyReels V2 is powered by Wan 2.1, indicating
that the underlying Image2Video (I2V) capabilities and model quality are
linked directly to recent advances in the Wan model family.

* SkyReels V2's full model size is reportedly 48GB, making local execution
impractical for most users without substantial GPU resources. This
significant size suggests higher capacity for detail and prompt-following
but also means it's best accessed via cloud or remote services.

* LTX .0.9.6 is really something! Super Impressed.
https://v.redd.it/xyf1swixq7we1 (Score: 101, Comments: 30
https://www.reddit.com/r/StableDiffusion/comments/1k4hea9/ltx_096_is_really_something_super_impressed/):
Post expresses strong positive reaction to LTX .0.9.6, a release likely
referring to an engine, framework, or tool dubbed "LTX." However, there are
no technical details (e.g., benchmarks, feature lists, or architectural
changes) provided. The only technical engagement is a comment indicating
someone considers their test result on 0.9.6 to be good, without specifying
metrics or context. Comments exhibit a divide: one strongly disagrees ('No.
Just no.'), while another supports the original impression with confirmation
of good results in their own testing. No deep technical debate or breakdown
is present. [External Link Summary] The Reddit post on r/StableDiffusion
discusses user impressions of LTX 0.9.6, a new distilled video generation
model targeting high efficiency and rapid output, reportedly capable of
generating satisfactory results in seconds. While the reception is mixedâ€”with
some praising its speed and output quality, and others critiquing specific
artifactsâ€”community commentary indicates improvement over prior versions and
broad compatibility across lower-end GPUs. Full post: LTX .0.9.6 is really
something! Super Impressed. https://v.redd.it/xyf1swixq7we1

* User xyzdist briefly mentions that their experiences testing LTX 0.9.6 have
led them to consider the results as 'good', implying improvements or
stability in this release as compared to prior versions. However, they do
not provide detailed benchmarks, quantitative metrics, or describe specific
features being evaluated.


3. MAGI-1 AND FRAMEPACK: NEW VIDEO MODEL LAUNCHES AND OPEN-SOURCE PERFORMANCE

* New open source autoregressive video model: MAGI-1 (
https://v.redd.it/8h3us8t1z7we1https://huggingface.co/sand-ai/MAGI-1
https://huggingface.co/sand-ai/MAGI-1) https://v.redd.it/8h3us8t1z7we1
(Score: 291, Comments: 66
https://www.reddit.com/r/StableDiffusion/comments/1k4ik0z/new_open_source_autoregressive_video_model_magi1/):
The open-source MAGI-1 autoregressive video model has been released on
HuggingFace (sand-ai/MAGI-1 https://huggingface.co/sand-ai/MAGI-1). The
current largest variant (24B parameters) requires 8x NVIDIA H100 GPUs for
inference, while an upcoming 4.5B parameter variant will be able to run on a
single RTX 4090. The model is capable of generating video natively at high
resolution (1440x2568px). Discussion highlights the substantial hardware
requirements for the 24B variant, with some users joking about the
impracticality of running such a large model locally and awaiting the more
accessible 4.5B variant. [External Link Summary] MAGI-1 is a newly released
open-source autoregressive video generation model, available at
https://huggingface.co/sand-ai/MAGI-1 https://huggingface.co/sand-ai/MAGI-1.
The flagship 24B parameter variant requires substantial compute (8x NVIDIA
H100 GPUs), but a smaller 4.5B parameter version, which can run on a single
RTX 4090, is planned for release. The model reportedly generates
native-resolution video at 1440x2568px, and supports quantized FP8/Q4 modes
to address memory requirements.

* The MAGI-1 24B parameter variant reportedly requires 8x H100 GPUs to run,
but a smaller 4.5B parameter version will be released that can operate on a
single RTX 4090. Native video output resolution for the demo is
1440x2568px, which is notable given the high performance demands for such
video generation.

* One comment notes technical details about quantization: the full precision
(FP8) model weighs in at 26GB, but with quantization to Q4, this can be
reduced to about 14GB. The use of blockswap techniques is mentioned as a
potential approach to further manage memory requirements for local runs of
the model.

* MAGI-1: Autoregressive Diffusion Video Model. https://v.redd.it/dxj6443u88we1
(Score: 152, Comments: 35
https://www.reddit.com/r/StableDiffusion/comments/1k4jz8t/magi1_autoregressive_diffusion_video_model/):
The MAGI-1 model is presented as the first autoregressive diffusion video
model with open-sourced code and weights, providing notable advancements in
infinite temporal extension and second-level control over video generation.
Pretrained weights for multiple model sizes (4.5B and 24B params), including
quantized and distilled variants, are available on HuggingFace
(https://huggingface.co/sand-ai/MAGI-1 https://huggingface.co/sand-ai/MAGI-1)
and require substantial hardware (e.g., dual H1008 for 24B, RTX 40901 for
4.5B). Technical details and benchmarks can be found in the linked tech
report https://github.com/SandAI-org/MAGI-1 and model cards. Discussion
centers on practical issues: the largest models require high-end hardware,
limiting accessibility for most users, and there are questions regarding
censorship or filtering within the open-source release, but no confirmation
in the documentation. [External Link Summary] MAGI-1 is a fully open-source,
autoregressive diffusion video generation model offering state-of-the-art
quality and precise one-second temporal control. Pretrained weights are
provided for multiple variant sizes (24B and 4.5B parameters), with hardware
recommendations indicating MAGI-1-24B targeting H100/H800 (multi-GPU) setups
and the 4.5B model suitable for single RTX 4090 GPUs. The model demonstrates
strong benchmark performance, supports infinite video extension, and comes
with accessible model zoo resources on Hugging Face:
https://huggingface.co/sand-ai/MAGI-1 https://huggingface.co/sand-ai/MAGI-1.

* Several pre-trained weights for MAGI-1 are available on HuggingFace,
including the 24B and 4.5B models, as well as distilled and quantized
versions. The recommended hardware varies: the 24B model (and distill)
requires H100/H800 GPUs (8x for base/distill, 4x for quantized), while the
4.5B model runs on a single RTX 4090. Notably, the quantized 24B-distill
version can also run on 8x RTX 4090s. Model zoo details and weights here.
https://huggingface.co/sand-ai/MAGI-1

* Initial user-generated image-to-video (i2v) tests indicate that MAGI-1
provides lower-quality results versus existing solutions like Kling 1.6/2,
especially at large resolutions (e.g., 2580x1408); outputs may appear
upscaled, with issues such as morphing hands, uncanny faces, and abnormal
human motionâ€”especially for rapid movement. The issues could stem from both
the model and input image quality. Direct comparisons with models like LTX,
WAN, Framepack, or Hunyan are limited due to hardware access constraints.

* I still can't believe FramePack lets me generate videos with just 6GB VRAM.
https://v.redd.it/nac1agdih4we1 (Score: 106, Comments: 50
https://www.reddit.com/r/StableDiffusion/comments/1k45ycn/i_still_cant_believe_framepack_lets_me_generate/):
The post highlights that FramePack can generate short videos (6 seconds) on a
modest RTX 3060 Mobile with only 6GB VRAM, requiring ~60 minutes per video
and using default settings. The user expresses that, despite the long
runtime, FramePack's low VRAM requirement is motivating for trying more
robust models (e.g., full img2vid) on cloud services like Runpod. No model
architecture, optimization specifics, or quality metrics were provided. The
top comment critiques misleading marketing around VRAM requirements by
contextualizing the tradeoff: low VRAM support comes at the cost of extremely
slow generation times (e.g., 60 mins for 6 seconds). Others joke about even
lower VRAM thresholds, indirectly questioning performance and usability at
those limits. [External Link Summary] A Reddit user demonstrates using
FramePack, a video generation tool leveraging Stable Diffusion, on an RTX
3060 Mobile GPU with only 6GB VRAM. The user was able to generate a 6-second,
30fps video (150 frames) in 60 minutes using default settings, highlighting
FramePack's ability to perform video generation on low VRAM consumer
hardware, albeit with significant processing time. This underscores recent
algorithmic improvements allowing resource-restricted devices to handle tasks
previously requiring much higher hardware specifications. Source: Reddit post
https://v.redd.it/nac1agdih4we1

* A commenter highlights that while FramePack enables video generation on
GPUs with only 6GB VRAM, the process can be extremely slow, pointing out
that it can take around 60 minutes just to generate a single 6-second
video. This suggests a significant trade-off between accessibility and
speed for those with lower-end hardware.

* Further technical inquiry is raised about the actual frame rate and total
number of frames output by FramePack for a 6-second video, implying that
performance and resource demand is tightly linked to these generation
parameters.




--------------------------------------------------------------------------------


AI DISCORD RECAP

> A summary of Summaries of Summaries by Gemini 2.5 Pro Exp

Theme 1: Model Mania & Performance Showdowns

* Gemma 3 Gets Quantized Redemption Arc
https://developers.googleblog.com/en/gemma-3-quantized-aware-trained: Google
released Gemma 3 with Quantization Aware Training
https://developers.googleblog.com/en/gemma-3-quantized-aware-trained
targeting consumer GPUs, addressing feedback from a prior announcement
https://x.com/JeffDean/status/1908608454216028222 and potentially redeeming
the model in the eyes of some developers. Members in Latent Space noted this
signifies a positive response to community input.

* DeepSeek-R2 & O3 Pro Hype Trains Collide
https://discord.com/channels/1340554757349179412/1340554757827461211/1362865327657975998:
Anticipation is high in LMArena for O3 Pro and DeepSeek-R2, with speculation
that DeepSeek-R2 might rival O3 performance at lower costs, as one user
exclaimed r2 is absolutely near o3well more than near but you'll see.
Meanwhile, observations suggest the performance gap between O4 Mini and O3 is
narrowing.

* GPT-4.5 Bests Rivals in Creative Writing, But Claude Clings On
https://discord.com/channels/1179035537009545276/1179035537529643040/1362867910002999418:
In Unsloth AI discussions, GPT 4.5 was generally favored over Claude and Grok
for creative writing, though one high-volume Claude user ($30k/month) found
3.7 slightly better with specific prompting. Comparisons also placed Gemini
2.5 Pro close to OpenAI's o3 and o4-mini high in OpenAI channels, citing
Gemini's longer thinking time on complex tasks.

Theme 2: Tooling Trials & Triumphs

* Aider Gets Gemini-Friendly Edit Format, But Architect Mode Needs Flag Finesse
https://discord.com/channels/1131200896827654144/1131200896827654149/1362865287074156616:
Aider introduced the udiff-simple edit format in its main branch to improve
compatibility with Gemini 2.5 Pro, although users noted the --architect flag
must be placed at the end of the command to work correctly. Issues with go
mod tidy not running properly in architect mode were also reported, requiring
manual /run commands.

* Unsloth Users Dodge OOM with Colab GGUF Trick
https://discord.com/channels/1179035537009545276/1179777624986357780/1362870393517375718:
Users experiencing Out of Memory errors saving finetuned models to GGUF in
Unsloth AI found a workaround by pushing the Lora adapter to HF, then using a
Google Colab T4 instance for the save. This bypasses VRAM spikes caused by
16-bit decompression during save_pretrained_merged, needing ~60GB disk space
for Llama3.1.

* HuggingFace Spaces Suffer Stage Fright, Users Report Build Bugs
https://discord.com/channels/879548962464493619/879548962464493622/1362879343046430861:
Multiple users across HuggingFace channels reported Hugging Face Spaces
getting stuck in the building state with 401 Unauthorized errors, suggesting
an infrastructure issue. A potential fix mentioned involves duplicating the
space https://huggingface.co/docs/hub/spaces, with ongoing discussions
tracked here
https://discuss.huggingface.co/t/my-space-suddenly-went-offline-the-cpu-cannot-restart/151121/10
and in various discord threads.

Theme 3: Hardware Headaches & High Performance

* Nvidia Blackwell Specs Surface While Drivers Melt Temps
https://discord.com/channels/1189498204333543425/1189498205101109300/1362921019546796113:
GPU MODE members hunted for Nvidia H200/B200/B300 specs, pointing to the
Blackwell architecture docs
https://resources.nvidia.com/en-us-blackwell-architecture, while LM Studio
users warned the new Nvidia driver (576.02) causes incorrect temperature
reporting, discussed on Reddit
https://www.reddit.com/r/ZephyrusG14/comments/1k27vuv/do_not_update_to_nvidia_driver_57602/.

* Gelid Pads Keep VRAM Cool Under Pressure
https://discord.com/channels/1179035537009545276/1179039861576056922/1362980479132762133:
An Unsloth AI member shared success using Gelid Extreme thermal pads on VRAM,
keeping temps below 75C. They noted others use thermal putty with mixed
results but expressed personal preference against it.

* AMD FP8 Scaling Factors Clarified for MatMul Mayhem
https://discord.com/channels/1189498204333543425/1359640791525490768/1362871203068379218:
In GPU MODE's AMD competition channel, it was clarified that AMD's FP8 matrix
multiplication uses per-matrix scaling factors (a_scale, b_scale) with shapes
[m, k // 128] and [n // 128, k // 128], not per-element scalars. Matrix
dimensions m and n must be divisible by 64, while k needs divisibility by
128.

Theme 4: Protocols & Integration Patterns

* Minions March to Save Aider Users Dough
https://discord.com/channels/1131200896827654144/1131200896827654149/1362865287074156616:
The Minions protocol https://github.com/HazyResearch/minions, enabling small
local models to collaborate with cloud giants, gained attention in the Aider
discord as a potential cost-saver. This approach aims to reduce token usage
by handling initial steps locally before engaging SOTA models.

* MCP Filesystem Boosts Cursor Project Sharing
https://discord.com/channels/1312302100125843476/1312302100125843479/1362879424164401352:
Members in the MCP discord are using the MCP filesystem server
https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem with
Cursor IDE to share multiple projects as a single context source. This offers
a more reliable method than dragging/dropping files for managing larger
project contexts.

* LlamaIndex Flexes Integration Muscles with Google Cloud & ZapGit
https://discord.com/channels/1059199217496772688/1187460979064324127/1362876950451982468:
LlamaIndex showcased integrations with Google Cloud databases for multi-step
knowledge agents https://t.co/fGpgPbGTLO and powered ZapGit for managing
@github issues via natural language https://t.co/qkp50i2SOc using MCP
(@zapier servers). Jerry Liu also lectured on evolving from RAG to agents
with multi-step reasoning https://t.co/t3MA2y5356.

Theme 5: Ecosystem Buzz & Benchmark Battles

* Deepseek Report Sparks Data Theft Deja Vu
https://discord.com/channels/1131200896827654144/1268910919057149974/1362866612209713162:
A report alleging Deepseek data misuse and CCP ties
https://selectcommitteeontheccp.house.gov/media/reports/deepseek-unmasked-
stirred discussion in the Aider channel, with one member sarcastically noting
'an AI company stealing data ... this is unheard of ...'. This fueled broader
conversations about data practices and geopolitical concerns in the AI
industry.

* EleutherAI Discord Gains Unexpected Fame
https://discord.com/channels/729741769192767510/729741769738158194/1362930940300492963:
Members reported that Deepseek and GPT models are recommending the EleutherAI
Discord server, leading to speculation about its growing influence.
EleutherAI was also listed alongside Meta, Mistral, and Hugging Face as part
of 'The Open Federation' promoting 'Freedom, creativity, decentralization'.

* Pass@k Metric Slammed as Vendor Shill Tactic
https://discord.com/channels/1216353675241590815/1293438210097025085/1363503010105528490:
In the Torchtune channel, a member criticized the pass@k benchmark metric
https://x.com/0xcodys/status/1901965450503725325?s=46&t=b1X88nwMsmZgHkmMFkiG3g
as bullshit invented by compute vendors to sell more compute, especially
absurd variants like pass@100 on multiple-choice benchmarks like GPQA. A
related paper exploring this is available at arxiv.org/abs/2504.13837
https://www.arxiv.org/abs/2504.13837.






--------------------------------------------------------------------------------

You are receiving this email because you signed up via our AINews site.

Want to change how you receive these emails?
You can unsubscribe from this list {{{RESEND_UNSUBSCRIBE_URL}}}.


